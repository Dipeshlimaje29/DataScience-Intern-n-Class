{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a7534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install selenium\n",
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79d2c30",
   "metadata": {},
   "source": [
    "## Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d528db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbeb363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e41e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c901a10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the link naukri.com\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08e307",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entering data analyst jobs\n",
    "search_job=driver.find_element(By.CLASS_NAME,'suggestor-input')\n",
    "search_job.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7f4ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entering location\n",
    "search_loc=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8f786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# click search button\n",
    "search_btn=driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720de293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 jobs titles\n",
    "job_titles=[]\n",
    "titles_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in titles_tags[0:10]:\n",
    "    job_titles.append(i.text)\n",
    "len(job_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a8375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 jobs location\n",
    "job_locations=[]\n",
    "loc_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in loc_tags[0:10]:\n",
    "    job_locations.append(i.text)\n",
    "len(job_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3691373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 company name\n",
    "company_names=[]\n",
    "comp_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in comp_tags[0:10]:\n",
    "    company_names.append(i.text)\n",
    "len(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52933b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 Experience required\n",
    "experience_req=[]\n",
    "exp_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in exp_tags[0:10]:\n",
    "    experience_req.append(i.text)\n",
    "len(experience_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ced158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating dataframe \n",
    "\n",
    "DF=pd.DataFrame({'Job Title':job_titles,'Job Location':job_locations,'Company Name':company_names,'Experience Required':experience_req})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610d08f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afe19f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1d2e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0350863",
   "metadata": {},
   "source": [
    "## Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data. This task will be done in following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1b6e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6fd1a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f068de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d291531",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the link naukri.com\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf00b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entering data Scientist jobs\n",
    "search_job=driver.find_element(By.CLASS_NAME,'suggestor-input')\n",
    "search_job.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9250359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entering location\n",
    "search_loc=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66716b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# click search button\n",
    "search_btn=driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896edec5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 jobs titles\n",
    "job_titles=[]\n",
    "titles_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in titles_tags[0:10]:\n",
    "    job_titles.append(i.text)\n",
    "len(job_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6374959c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 jobs location\n",
    "job_locations=[]\n",
    "loc_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in loc_tags[0:10]:\n",
    "    job_locations.append(i.text)\n",
    "len(job_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1878f7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 company name\n",
    "company_names=[]\n",
    "comp_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in comp_tags[0:10]:\n",
    "    company_names.append(i.text)\n",
    "len(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb75998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 Experience required\n",
    "experience_req=[]\n",
    "exp_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in exp_tags[0:10]:\n",
    "    experience_req.append(i.text)\n",
    "len(experience_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00e91c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "\n",
    "DF=pd.DataFrame({'Job Title':job_titles,'Job Location':job_locations,'Company Name':company_names,'Experience Required':experience_req})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deeb750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ba3a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdba756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d6035a8",
   "metadata": {},
   "source": [
    "## Q3: In this question you have to scrape data using the filters available on the webpage as shown below: You have to use the location and salary filter. You have to scrape data for “DataScientist” designation for first 10 job results. You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847baba9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6550a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac270b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d82f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the link naukri.com\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfe550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entering data Scientist jobs\n",
    "search_job=driver.find_element(By.CLASS_NAME,'suggestor-input')\n",
    "search_job.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d90806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entering location\n",
    "search_loc=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e52f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# click search button\n",
    "search_btn=driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de6ffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filtering by salary 3 to 6\n",
    "\n",
    "sal_fil=driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/i')\n",
    "sal_fil.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0dab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filtering by location delhi NCR\n",
    "\n",
    "loc_fil=driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/section[1]/div[2]/div[13]/div[2]/div[4]/label/i')\n",
    "loc_fil.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39f4da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 jobs titles\n",
    "job_titles=[]\n",
    "titles_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in titles_tags[0:10]:\n",
    "    job_titles.append(i.text)\n",
    "len(job_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e86d39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 jobs location\n",
    "job_locations=[]\n",
    "loc_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in loc_tags[0:10]:\n",
    "    job_locations.append(i.text)\n",
    "len(job_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11435449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 company name\n",
    "company_names=[]\n",
    "comp_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in comp_tags[0:10]:\n",
    "    company_names.append(i.text)\n",
    "len(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba7464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 Experience required\n",
    "experience_req=[]\n",
    "exp_tags=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "for i in exp_tags[0:10]:\n",
    "    experience_req.append(i.text)\n",
    "len(experience_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34134e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "\n",
    "DF=pd.DataFrame({'Job Title':job_titles,'Job Location':job_locations,'Company Name':company_names,'Experience Required':experience_req})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c62352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05061541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce9c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19ee01e6",
   "metadata": {},
   "source": [
    "## Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Price discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5677f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd7760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11992adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffccda6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the link flipkart.com\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5002fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entering sunglasses\n",
    "search_job=driver.find_element(By.CLASS_NAME,'_3704LK')\n",
    "search_job.send_keys('sunglasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04966931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# click search button\n",
    "search_btn=driver.find_element(By.CLASS_NAME,'L0Z3Pu')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c75c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 100 Brand Names\n",
    "Brand_names=[]\n",
    "#start=0\n",
    "#end=3\n",
    "for page in range(0,3):\n",
    "    brand=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in brand:\n",
    "        Brand_names.append(i.text)\n",
    "\n",
    " #scrapping first 100 discription\n",
    "Product_dist=[]\n",
    "#start=0\n",
    "#end=3\n",
    "for page in range(0,3):\n",
    "    prod=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in prod:\n",
    "        Product_dist.append(i.text)\n",
    "    prod=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa _2-ICcC\"]')    \n",
    "    for j in prod:\n",
    "        Product_dist.append(i.text)\n",
    "\n",
    "#scrapping first 100 price tag        \n",
    "Price_tag=[]\n",
    "#start=0\n",
    "#end=3\n",
    "for page in range(0,3):\n",
    "    price=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in price:\n",
    "        Price_tag.append(i.text)\n",
    "\n",
    "#scrapping first 100 discounts        \n",
    "Product_dis=[]\n",
    "#start=0\n",
    "#end=3\n",
    "for page in range(0,3):\n",
    "    dis=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "    for i in dis:\n",
    "        Product_dis.append(i.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb540c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "df=pd.DataFrame({'Brand Names':Brand_names,'Product Description':Product_dist,'Price':Price_tag,'Discount':Product_dis})\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada83a98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858a2a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "949868d9",
   "metadata": {},
   "source": [
    "## Q5) Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace. When you will open the above link you will reach to the below shown webpage . As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45b3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9d580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ebedb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db45a93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the link flipkart.com\n",
    "url='https://www.flipkart.com/apple-iphone-11-black-64-gb/p/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART&q=iphone+11&store=tyy%2F4io&srno=s_1_1&otracker=search&otracker1=search&fm=organic&iid=92b7f6cf-c935-4b73-aae0-59fcb624aa9d.MOBFWQ6BXGJCEYNY.SEARCH&ppt=hp&ppn=homepage&ssid=j0p299ip1c0000001667488872326&qH=f6cdfdaa9f3c23f3'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e0299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reviewall button\n",
    "rev_but=driver.find_element(By.XPATH,'//div[@class=\"_3UAT2v _16PBlm\"]')\n",
    "rev_but.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a6e34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 20 Product rating because next button is different\n",
    "rating_tag=[]\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start,end):\n",
    "    rat=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in rat:\n",
    "        rating_tag.append(i.text)\n",
    "        \n",
    "        \n",
    "# scrapping first 20 Product summary because next button is different        \n",
    "review_summ=[]\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start,end):\n",
    "    rev=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in rev:\n",
    "        review_summ.append(i.text)\n",
    "        \n",
    "# scrapping first 20 Product comment because next button is different\n",
    "comments=[]\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start,end):\n",
    "    com=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in com:\n",
    "        comments.append(i.text)\n",
    "\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()                     \n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e5ffd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first 20 products\n",
    "len(review_summ),len(comments),len(rating_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb0086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#click on next button\n",
    "next_button=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "next_button.click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f24ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# scrapping next 80 product rating\n",
    "rating_tag=[]\n",
    "start=3\n",
    "end=13\n",
    "for page in range(start,end):\n",
    "    rat=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in rat:\n",
    "        rating_tag.append(i.text)\n",
    "        \n",
    "## scrapping next 80 Product summary       \n",
    "review_summ=[]\n",
    "start=3\n",
    "end=13\n",
    "for page in range(start,end):\n",
    "    rev=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in rev:\n",
    "        review_summ.append(i.text)\n",
    "        \n",
    "# scrapping next 80 Product comment\n",
    "comments=[]\n",
    "start=3\n",
    "end=13\n",
    "for page in range(start,end):\n",
    "    com=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in com:\n",
    "        comments.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "    next_button.click()\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6e715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#next 80 products total 100 producs\n",
    "len(review_summ),len(comments),len(rating_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc1816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "df=pd.DataFrame({'Ratings':rating_tag,'Review summary':review_summ,'Full review':comments})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d923e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6aad4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "375b8670",
   "metadata": {},
   "source": [
    "## Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field. You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171f47b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6efca83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c561878d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f866b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the link flipkart.com\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5449d83f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entering sunglasses\n",
    "search_job=driver.find_element(By.CLASS_NAME,'_3704LK')\n",
    "search_job.send_keys('sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11265c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# click search button\n",
    "search_btn=driver.find_element(By.CLASS_NAME,'L0Z3Pu')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c96e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 100 Brand Names\n",
    "Brand_names=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    brand=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in brand:\n",
    "        Brand_names.append(i.text)   \n",
    "\n",
    "# scrapping first 100 shoe discription\n",
    "Product_dist=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    prod=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in prod:\n",
    "        Product_dist.append(i.text)\n",
    "    prod=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa _2-ICcC\"]')    \n",
    "    for j in prod:\n",
    "        Product_dist.append(i.text)\n",
    "\n",
    "\n",
    "# scrapping first 100 shoe price\n",
    "shoe_price=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    shoe=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in shoe:\n",
    "        shoe_price.append(i.text)\n",
    "\n",
    "\n",
    "# scrapping first 100 shoe discount\n",
    "discount_tag=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    dis=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]')\n",
    "    for i in dis:\n",
    "        discount_tag.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499362d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "df=pd.DataFrame({'Brand':Brand_names,'Product Description':Product_dist,'Price':shoe_price,'Discounts':discount_tag})\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435f6cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca43b5",
   "metadata": {},
   "source": [
    "# Q7: Go to the link - https://www.myntra.com/shoes\n",
    "### Set second Price filter and Color filter to “Black”, as shown in the below image.And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1a1a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23bc02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333fd36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde96a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the link \n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134a04c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filtering by price\n",
    "\n",
    "sal_fil=driver.find_element(By.XPATH,'/html/body/div[2]/div/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div')\n",
    "sal_fil.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d300deb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# filtering by black color\n",
    "\n",
    "sal_fil=driver.find_element(By.XPATH,'/html/body/div[2]/div/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div')\n",
    "sal_fil.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed221e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping 100 product names\n",
    "Brand_names=[]\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start,end):\n",
    "    rat=driver.find_elements(By.XPATH,'//h3[@class=\"product-brand\"]')\n",
    "    for i in rat:\n",
    "        Brand_names.append(i.text)\n",
    "        \n",
    "## scrapping 100 product summary       \n",
    "discription=[]\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start,end):\n",
    "    rev=driver.find_elements(By.XPATH,'//h4[@class=\"product-product\"]')\n",
    "    for i in rev:\n",
    "        discription.append(i.text)\n",
    "        \n",
    "# scrapping 100 product price\n",
    "price=[]\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start,end):\n",
    "    com=driver.find_elements(By.XPATH,'//div[@class=\"product-price\"]')\n",
    "    for i in com:\n",
    "        price.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//li[@class=\"pagination-next\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9810d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "df=pd.DataFrame({'Brand':Brand_names,'Product Description':discription,'Price':price})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83a741",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4a963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec8f9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6be7f03",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e578980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d822c5a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53110a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59b9e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the link\n",
    "url='https://www.amazon.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b789f34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# entering sunglasses\n",
    "search_job=driver.find_element(By.XPATH,'(//input[@class=\"nav-input nav-progressive-attribute\"])[1]')\n",
    "search_job.send_keys('Laptop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77687a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# click search button\n",
    "search_btn=driver.find_element(By.XPATH,'(//input[@class=\"nav-input nav-progressive-attribute\"])[2]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ea86c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter\n",
    "filter_btn=driver.find_element(By.XPATH,'(//i[@class=\"a-icon a-icon-checkbox\"])[59]')\n",
    "filter_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045ac95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 laptop names\n",
    "lap_name=[]\n",
    "\n",
    "for page in range(0,1):\n",
    "    laptop=driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "    for i in laptop[0:10]:\n",
    "        lap_name.append(i.text)\n",
    "\n",
    "        \n",
    "        \n",
    "len(lap_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08152360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lap_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30fac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# scrapping first 10 laptop price\n",
    "lap_price=[]\n",
    "#start=0\n",
    "#end=1\n",
    "for page in range(0,1):\n",
    "    laptop=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "    for i in laptop[0:10]:\n",
    "        lap_price.append(i.text)\n",
    "\n",
    "        \n",
    "len(lap_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32d43e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lap_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68420c0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping first 10 laptop ratings\n",
    "Rating=[]\n",
    "titles_rat=driver.find_elements(By.XPATH,\"//div[@class='a-row a-size-small']/span\")\n",
    "\n",
    "for i in titles_rat:\n",
    "\n",
    "    Rating.append(i.get_attribute(\"aria-label\"))\n",
    "\n",
    "rating=[]\n",
    "\n",
    "for i in range(0,len(Rating))[0:20]:\n",
    "\n",
    "    if i == 0 or  i/2 == i//2:\n",
    "\n",
    "        rating.append(Rating[i][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a99fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7be81d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating dataframe \n",
    "df=pd.DataFrame({'Laptop Name':lap_name,'Laptop Price':lap_price,'Ratings':rating})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a45353",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477c51e",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1.First get the webpage https://www.azquotes.com/\n",
    "2. Click on Top Quotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c73bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f0401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746f0a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6191c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the link \n",
    "url='https://www.azquotes.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7aa38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter by top quotes\n",
    "filter_btn=driver.find_element(By.XPATH,'//a[@href=\"/top_quotes.html\"]')\n",
    "filter_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ece219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping 1000 qoute\n",
    "title_tag=[]\n",
    "start=0\n",
    "end=10\n",
    "for page in range(start,end):\n",
    "    rat=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "    for i in rat:\n",
    "        title_tag.append(i.text)\n",
    "        \n",
    "## scrapping 1000 author     \n",
    "author_summ=[]\n",
    "start=0\n",
    "end=10\n",
    "for page in range(start,end):\n",
    "    rev=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "    for i in rev:\n",
    "        author_summ.append(i.text)\n",
    "        \n",
    "# scrapping 1000 type of qoute\n",
    "tag=[]\n",
    "start=0\n",
    "end=10\n",
    "for page in range(start,end):\n",
    "    com=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "    for i in com:\n",
    "        tag.append(i.text)\n",
    "\n",
    "# next button        \n",
    "start=0\n",
    "end=9\n",
    "for page in range(start,end):    \n",
    "    next_button=driver.find_element(By.XPATH,'(//li[@class=\"next\"])[2]')\n",
    "    next_button.click()\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903b605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking scrapped data total 1000\n",
    "len(title_tag),len(author_summ),len(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985bfc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#createing dataframe\n",
    "df=pd.DataFrame({'Quote':title_tag,'Author':author_summ,'Type Of Quote':tag})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00decd7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2532a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12e1aeb",
   "metadata": {},
   "source": [
    "# Q10: Write s python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc28c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50042d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6e112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3bc07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the link \n",
    "url='https://www.jagranjosh.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973d84b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter\n",
    "filter_btn=driver.find_element(By.XPATH,'(//a[@href=\"/general-knowledge?ref=nav_dd\"])[2]')\n",
    "filter_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d34727d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter\n",
    "filter_btn=driver.find_element(By.XPATH,'//a[@href=\"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\"]')\n",
    "filter_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57af442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#name\n",
    "Name=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"table-box\"]/table/tbody/tr/td[2]/p')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    Name.append(name.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d7a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed62591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#born-dead\n",
    "bo_de=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"table-box\"]/table/tbody/tr/td[3]/p')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    bo_de.append(name.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eef714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bo_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7965f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Term of office\n",
    "term=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"table-box\"]/table/tbody/tr/td[4]/p')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    term.append(name.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f71f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e705671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remark\n",
    "remark=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"table-box\"]/table/tbody/tr/td[5]/p')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    remark.append(name.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465b4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91c36b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "df=pd.DataFrame({'Name':Name,'Born-Dead':bo_de,'Remarks':remark})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a9140",
   "metadata": {},
   "source": [
    "# Q11: Write s python program to display list of 50 Most expensive cars in the world (i.e. Company name, Model name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to click on the List option from Dropdown menu on left side.\n",
    "3. Then click on 50 most expensive cars in the world..\n",
    "4. Then scrap the mentioned data and make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebaf05c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad4917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90882a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81094e34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the link \n",
    "url='https://www.motor1.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc5969b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter \n",
    "filter_btn=driver.find_element(By.XPATH,'//div[@class=\"m1-hamburger-button\"]')\n",
    "filter_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f04057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter \n",
    "filter_btn=driver.find_element(By.XPATH,'(//a[@href=\"/features/category/lists/\"])[2]')\n",
    "filter_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4fda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter\n",
    "filter_btn=driver.find_element(By.XPATH,'(//a[@href=\"/features/308149/most-expensive-new-cars-ever/\"])[2]')\n",
    "filter_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b855f72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrapping 50 company name with price\n",
    "car_price=[]\n",
    "comp_tags=driver.find_elements(By.XPATH,'/html/body/div[2]/div[7]/div[2]/div[1]/div[2]/div[1]/ul')\n",
    "for i in comp_tags[0:50]:\n",
    "    car_price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc84a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('List of all Car Company with Price :',*car_price, sep='\\n\\n' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
