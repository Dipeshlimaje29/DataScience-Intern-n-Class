{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a64fe3cb",
   "metadata": {},
   "source": [
    "# 1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "* You need to find following details:\n",
    "    *  A) Rank\n",
    "    *  B) Name\n",
    "    *  C) Artist\n",
    "    *  D) Upload date\n",
    "    *  E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e1b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import re\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df6d447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')\n",
    "\n",
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139a18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the link\n",
    "url='https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb135a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.',\n",
       " '2.',\n",
       " '3.',\n",
       " '4.',\n",
       " '5.',\n",
       " '6.',\n",
       " '7.',\n",
       " '8.',\n",
       " '9.',\n",
       " '10.',\n",
       " '11.',\n",
       " '12.',\n",
       " '13.',\n",
       " '14.',\n",
       " '15.',\n",
       " '16.',\n",
       " '17.',\n",
       " '18.',\n",
       " '19.',\n",
       " '20.',\n",
       " '21.',\n",
       " '22.',\n",
       " '23.',\n",
       " '24.',\n",
       " '25.',\n",
       " '26.',\n",
       " '27.',\n",
       " '28.',\n",
       " '29.',\n",
       " '30.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping Sr no\n",
    "sr=[]\n",
    "names = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]//tr/td[1]')\n",
    "\n",
    "for name in names[:30]:\n",
    "\n",
    "    sr.append(name.text)\n",
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a2eca18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Baby Shark Dance\"[4]',\n",
       " '\"Despacito\"[7]',\n",
       " '\"Johny Johny Yes Papa\"[14]',\n",
       " '\"Shape of You\"[15]',\n",
       " '\"Bath Song\"[17]',\n",
       " '\"See You Again\"[18]',\n",
       " '\"Phonics Song with Two Words\"[23]',\n",
       " '\"Uptown Funk\"[24]',\n",
       " '\"Learning Colors – Colorful Eggs on a Farm\"[25]',\n",
       " '\"Wheels on the Bus\"[26]',\n",
       " '\"Gangnam Style\"[27]',\n",
       " '\"Masha and the Bear – Recipe for Disaster\"[32]',\n",
       " '\"Dame Tu Cosita\"[33]',\n",
       " '\"Sugar\"[34]',\n",
       " '\"Roar\"[35]',\n",
       " '\"Counting Stars\"[36]',\n",
       " '\"Axel F\"[37]',\n",
       " '\"Sorry\"[38]',\n",
       " '\"Thinking Out Loud\"[39]',\n",
       " '\"Baa Baa Black Sheep\"[40]',\n",
       " '\"Dark Horse\"[41]',\n",
       " '\"Faded\"[42]',\n",
       " '\"Girls Like You\"[43]',\n",
       " '\"Let Her Go\"[44]',\n",
       " '\"Waka Waka (This Time for Africa)\"[45]',\n",
       " '\"Perfect\"[46]',\n",
       " '\"Bailando\"[47]',\n",
       " '\"Lean On\"[48]',\n",
       " '\"Humpty the train on a fruits ride\"[49]',\n",
       " '\"Shake It Off\"[50]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Video Name\n",
    "video=[]\n",
    "names = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]//tr/td[2]')\n",
    "\n",
    "for name in names[:30]:\n",
    "\n",
    "    video.append(name.text)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed8593b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Pinkfong Baby Shark - Kids' Songs & Stories\",\n",
       " 'Luis Fonsi',\n",
       " 'LooLoo Kids',\n",
       " 'Ed Sheeran',\n",
       " 'Cocomelon – Nursery Rhymes',\n",
       " 'Wiz Khalifa',\n",
       " 'ChuChu TV',\n",
       " 'Mark Ronson',\n",
       " 'Miroshka TV',\n",
       " 'Cocomelon – Nursery Rhymes',\n",
       " 'Psy',\n",
       " 'Get Movies',\n",
       " 'El Chombo',\n",
       " 'Maroon 5',\n",
       " 'Katy Perry',\n",
       " 'OneRepublic',\n",
       " 'Crazy Frog',\n",
       " 'Justin Bieber',\n",
       " 'Ed Sheeran',\n",
       " 'Cocomelon – Nursery Rhymes',\n",
       " 'Katy Perry',\n",
       " 'Alan Walker',\n",
       " 'Maroon 5',\n",
       " 'Passenger',\n",
       " 'Shakira',\n",
       " 'Ed Sheeran',\n",
       " 'Enrique Iglesias',\n",
       " 'Major Lazer',\n",
       " 'Kiddiestv Hindi – Nursery Rhymes & Kids Songs',\n",
       " 'Taylor Swift']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Video Artist\n",
    "uploader=[]\n",
    "names = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]//tr/td[3]')\n",
    "\n",
    "for name in names[:30]:\n",
    "\n",
    "    uploader.append(name.text)\n",
    "uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ff1f24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11.74',\n",
       " '8.01',\n",
       " '6.53',\n",
       " '5.85',\n",
       " '5.77',\n",
       " '5.70',\n",
       " '5.02',\n",
       " '4.76',\n",
       " '4.73',\n",
       " '4.63',\n",
       " '4.61',\n",
       " '4.51',\n",
       " '4.14',\n",
       " '3.78',\n",
       " '3.69',\n",
       " '3.68',\n",
       " '3.61',\n",
       " '3.61',\n",
       " '3.52',\n",
       " '3.43',\n",
       " '3.40',\n",
       " '3.37',\n",
       " '3.35',\n",
       " '3.34',\n",
       " '3.31',\n",
       " '3.30',\n",
       " '3.30',\n",
       " '3.29',\n",
       " '3.23',\n",
       " '3.23']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Video Views\n",
    "views=[]\n",
    "names = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]//tr/td[4]')\n",
    "\n",
    "for name in names[:30]:\n",
    "\n",
    "    views.append(name.text)\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ccbf070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['June 17, 2016',\n",
       " 'January 12, 2017',\n",
       " 'October 8, 2016',\n",
       " 'January 30, 2017',\n",
       " 'May 2, 2018',\n",
       " 'April 6, 2015',\n",
       " 'March 6, 2014',\n",
       " 'November 19, 2014',\n",
       " 'February 27, 2018',\n",
       " 'May 24, 2018',\n",
       " 'July 15, 2012',\n",
       " 'January 31, 2012',\n",
       " 'April 5, 2018',\n",
       " 'January 14, 2015',\n",
       " 'September 5, 2013',\n",
       " 'May 31, 2013',\n",
       " 'June 16, 2009',\n",
       " 'October 22, 2015',\n",
       " 'October 7, 2014',\n",
       " 'June 25, 2018',\n",
       " 'February 20, 2014',\n",
       " 'December 3, 2015',\n",
       " 'May 31, 2018',\n",
       " 'July 25, 2012',\n",
       " 'June 4, 2010',\n",
       " 'November 9, 2017',\n",
       " 'April 11, 2014',\n",
       " 'March 22, 2015',\n",
       " 'January 26, 2018',\n",
       " 'August 18, 2014']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Video Date\n",
    "up_date=[]\n",
    "names = driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]//tr/td[5]')\n",
    "\n",
    "for name in names[:30]:\n",
    "\n",
    "    up_date.append(name.text)\n",
    "up_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7248d0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[4]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>11.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[7]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[14]</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Shape of You\"[15]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>5.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Bath Song\"[17]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[18]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[23]</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Uptown Funk\"[24]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>4.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[25]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Wheels on the Bus\"[26]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>4.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[27]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[32]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[33]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Sugar\"[34]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Roar\"[35]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Counting Stars\"[36]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Axel F\"[37]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Sorry\"[38]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Thinking Out Loud\"[39]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[40]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Dark Horse\"[41]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Faded\"[42]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Girls Like You\"[43]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Let Her Go\"[44]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[45]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Perfect\"[46]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Bailando\"[47]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Lean On\"[48]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[49]</td>\n",
       "      <td>Kiddiestv Hindi – Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Shake It Off\"[50]</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>August 18, 2014</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[4]   \n",
       "1    2.                                   \"Despacito\"[7]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[14]   \n",
       "3    4.                               \"Shape of You\"[15]   \n",
       "4    5.                                  \"Bath Song\"[17]   \n",
       "5    6.                              \"See You Again\"[18]   \n",
       "6    7.                \"Phonics Song with Two Words\"[23]   \n",
       "7    8.                                \"Uptown Funk\"[24]   \n",
       "8    9.  \"Learning Colors – Colorful Eggs on a Farm\"[25]   \n",
       "9   10.                          \"Wheels on the Bus\"[26]   \n",
       "10  11.                              \"Gangnam Style\"[27]   \n",
       "11  12.   \"Masha and the Bear – Recipe for Disaster\"[32]   \n",
       "12  13.                             \"Dame Tu Cosita\"[33]   \n",
       "13  14.                                      \"Sugar\"[34]   \n",
       "14  15.                                       \"Roar\"[35]   \n",
       "15  16.                             \"Counting Stars\"[36]   \n",
       "16  17.                                     \"Axel F\"[37]   \n",
       "17  18.                                      \"Sorry\"[38]   \n",
       "18  19.                          \"Thinking Out Loud\"[39]   \n",
       "19  20.                        \"Baa Baa Black Sheep\"[40]   \n",
       "20  21.                                 \"Dark Horse\"[41]   \n",
       "21  22.                                      \"Faded\"[42]   \n",
       "22  23.                             \"Girls Like You\"[43]   \n",
       "23  24.                                 \"Let Her Go\"[44]   \n",
       "24  25.           \"Waka Waka (This Time for Africa)\"[45]   \n",
       "25  26.                                    \"Perfect\"[46]   \n",
       "26  27.                                   \"Bailando\"[47]   \n",
       "27  28.                                    \"Lean On\"[48]   \n",
       "28  29.          \"Humpty the train on a fruits ride\"[49]   \n",
       "29  30.                               \"Shake It Off\"[50]   \n",
       "\n",
       "                                           Artist        Upload Date  Views  \n",
       "0     Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016  11.74  \n",
       "1                                      Luis Fonsi   January 12, 2017   8.01  \n",
       "2                                     LooLoo Kids    October 8, 2016   6.53  \n",
       "3                                      Ed Sheeran   January 30, 2017   5.85  \n",
       "4                      Cocomelon – Nursery Rhymes        May 2, 2018   5.77  \n",
       "5                                     Wiz Khalifa      April 6, 2015   5.70  \n",
       "6                                       ChuChu TV      March 6, 2014   5.02  \n",
       "7                                     Mark Ronson  November 19, 2014   4.76  \n",
       "8                                     Miroshka TV  February 27, 2018   4.73  \n",
       "9                      Cocomelon – Nursery Rhymes       May 24, 2018   4.63  \n",
       "10                                            Psy      July 15, 2012   4.61  \n",
       "11                                     Get Movies   January 31, 2012   4.51  \n",
       "12                                      El Chombo      April 5, 2018   4.14  \n",
       "13                                       Maroon 5   January 14, 2015   3.78  \n",
       "14                                     Katy Perry  September 5, 2013   3.69  \n",
       "15                                    OneRepublic       May 31, 2013   3.68  \n",
       "16                                     Crazy Frog      June 16, 2009   3.61  \n",
       "17                                  Justin Bieber   October 22, 2015   3.61  \n",
       "18                                     Ed Sheeran    October 7, 2014   3.52  \n",
       "19                     Cocomelon – Nursery Rhymes      June 25, 2018   3.43  \n",
       "20                                     Katy Perry  February 20, 2014   3.40  \n",
       "21                                    Alan Walker   December 3, 2015   3.37  \n",
       "22                                       Maroon 5       May 31, 2018   3.35  \n",
       "23                                      Passenger      July 25, 2012   3.34  \n",
       "24                                        Shakira       June 4, 2010   3.31  \n",
       "25                                     Ed Sheeran   November 9, 2017   3.30  \n",
       "26                               Enrique Iglesias     April 11, 2014   3.30  \n",
       "27                                    Major Lazer     March 22, 2015   3.29  \n",
       "28  Kiddiestv Hindi – Nursery Rhymes & Kids Songs   January 26, 2018   3.23  \n",
       "29                                   Taylor Swift    August 18, 2014   3.23  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe \n",
    "\n",
    "DF=pd.DataFrame({'Rank':sr,'Name':video,'Artist':uploader,'Upload Date':up_date,'Views':views})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aab5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e94c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "796454cc",
   "metadata": {},
   "source": [
    "# 2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "* You need to find following details:\n",
    "    *  A) Match title (I.e. 1st ODI)\n",
    "    *  B) Series\n",
    "    *  C) Place\n",
    "    *  D) Date\n",
    "    *  E) Time\n",
    "* Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "496b1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import re\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a328093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')\n",
    "\n",
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b69b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the link\n",
    "url='https://www.bcci.tv/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fe17d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/nav/div[1]/div[2]/ul[1]/li[2]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "594c1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div/div/div/div[2]/div[2]/div/div[1]/div/div[1]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64a5af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div/div/div/div[2]/div[2]/div/div[1]/div/div[2]/div[2]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32c2b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div/div/div/div[2]/div[2]/div/div[2]/div/div[1]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0df690d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div/div/div/div[2]/div[2]/div/div[2]/div/div[2]/div[3]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "853e3fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bangladesh', 'vs', 'India'],\n",
       " ['Bangladesh', 'vs', 'India'],\n",
       " ['Bangladesh', 'vs', 'India']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping ODI matches\n",
    "views=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"fixture-card-mid d-flex align-items-center justify-content-between\"]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    views.append(name.text.split('\\n'))\n",
    "views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef03c9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8187cc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INDIA TOUR OF BANGLADESH ODI SERIES 2022-23',\n",
       " 'INDIA TOUR OF BANGLADESH ODI SERIES 2022-23',\n",
       " 'INDIA TOUR OF BANGLADESH ODI SERIES 2022-23']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping ODI matches Dates\n",
    "up_date=[]\n",
    "names = driver.find_elements(By.XPATH,'//h5[@class=\"fix-text\"][2]/span')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    up_date.append(name.text)\n",
    "up_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9fc6115b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(up_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f76bf2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1st ODI - Shere Bangla National Stadium, Mirpur, Dhaka',\n",
       " '2nd ODI - Shere Bangla National Stadium, Mirpur, Dhaka',\n",
       " '3rd ODI - Zahur Ahmed Chowdhury Stadium, Chattogram']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping ODI matches Places\n",
    "place=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"fix-place ng-binding ng-scope\"]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    place.append(name.text)\n",
    "place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4bb2b0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4 DEC 2022'], ['7 DEC 2022'], ['10 DEC 2022']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping ODI matches Date\n",
    "date=[]\n",
    "names = driver.find_elements(By.XPATH,'//h5[@class=\"ng-binding\"]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    date.append(name.text.split('\\n'))\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0484bf65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "901af64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['11:30 AM IST'], ['11:30 AM IST'], ['11:30 AM IST']]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping ODI matches time\n",
    "time=[]\n",
    "names = driver.find_elements(By.XPATH,'//h5[@class=\"text-right ng-binding\"]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    time.append(name.text.split('\\n'))\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d83e250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Bangladesh, vs, India]</td>\n",
       "      <td>INDIA TOUR OF BANGLADESH ODI SERIES 2022-23</td>\n",
       "      <td>1st ODI - Shere Bangla National Stadium, Mirpu...</td>\n",
       "      <td>[4 DEC 2022]</td>\n",
       "      <td>[11:30 AM IST]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Bangladesh, vs, India]</td>\n",
       "      <td>INDIA TOUR OF BANGLADESH ODI SERIES 2022-23</td>\n",
       "      <td>2nd ODI - Shere Bangla National Stadium, Mirpu...</td>\n",
       "      <td>[7 DEC 2022]</td>\n",
       "      <td>[11:30 AM IST]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Bangladesh, vs, India]</td>\n",
       "      <td>INDIA TOUR OF BANGLADESH ODI SERIES 2022-23</td>\n",
       "      <td>3rd ODI - Zahur Ahmed Chowdhury Stadium, Chatt...</td>\n",
       "      <td>[10 DEC 2022]</td>\n",
       "      <td>[11:30 AM IST]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Match Title                                       Series  \\\n",
       "0  [Bangladesh, vs, India]  INDIA TOUR OF BANGLADESH ODI SERIES 2022-23   \n",
       "1  [Bangladesh, vs, India]  INDIA TOUR OF BANGLADESH ODI SERIES 2022-23   \n",
       "2  [Bangladesh, vs, India]  INDIA TOUR OF BANGLADESH ODI SERIES 2022-23   \n",
       "\n",
       "                                               Place           Date  \\\n",
       "0  1st ODI - Shere Bangla National Stadium, Mirpu...   [4 DEC 2022]   \n",
       "1  2nd ODI - Shere Bangla National Stadium, Mirpu...   [7 DEC 2022]   \n",
       "2  3rd ODI - Zahur Ahmed Chowdhury Stadium, Chatt...  [10 DEC 2022]   \n",
       "\n",
       "             Time  \n",
       "0  [11:30 AM IST]  \n",
       "1  [11:30 AM IST]  \n",
       "2  [11:30 AM IST]  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe \n",
    "\n",
    "DF=pd.DataFrame({'Match Title':views,'Series':up_date,'Place':place,'Date':date,'Time':time})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3a28bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92757d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8341be6e",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of selenium exception from guru99.com.\n",
    "Url = https://www.guru99.com/\n",
    "* You need to find following details:\n",
    "    *  A) Name\n",
    "    *  B) Description\n",
    "* Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fc400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import re\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67c5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')\n",
    "\n",
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de4dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the link\n",
    "url='https://www.guru99.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea075c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[2]/div[1]/div/ul[1]/li[3]/a')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f3d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div/main/div/article/div/div/table[5]/tbody/tr[34]/td[1]/a/strong')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c70a3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. ElementNotVisibleException',\n",
       " '2. ElementNotSelectableException',\n",
       " '3. NoSuchElementException',\n",
       " '4. NoSuchFrameException',\n",
       " '5. NoAlertPresentException',\n",
       " '6. NoSuchWindowException',\n",
       " '7. StaleElementReferenceException',\n",
       " '8. SessionNotFoundException',\n",
       " '9. TimeoutException',\n",
       " '10. WebDriverException',\n",
       " '11. ConnectionClosedException',\n",
       " '12. ElementClickInterceptedException',\n",
       " '13. ElementNotInteractableException',\n",
       " '14. ErrorInResponseException',\n",
       " '15. ErrorHandler.UnknownServerException',\n",
       " '16. ImeActivationFailedException',\n",
       " '17. ImeNotAvailableException',\n",
       " '18. InsecureCertificateException',\n",
       " '19. InvalidArgumentException',\n",
       " '20. InvalidCookieDomainException',\n",
       " '21. InvalidCoordinatesException',\n",
       " '22. InvalidElementStateException',\n",
       " '23. InvalidSessionIdException',\n",
       " '24. InvalidSwitchToTargetException',\n",
       " '25. JavascriptException',\n",
       " '26. JsonException',\n",
       " '27. NoSuchAttributeException',\n",
       " '28. MoveTargetOutOfBoundsException',\n",
       " '29. NoSuchContextException',\n",
       " '30. NoSuchCookieException',\n",
       " '31. NotFoundException',\n",
       " '32. RemoteDriverServerException',\n",
       " '33. ScreenshotException',\n",
       " '34. SessionNotCreatedException',\n",
       " '35. UnableToSetCookieException',\n",
       " '36. UnexpectedTagNameException',\n",
       " '37. UnhandledAlertException',\n",
       " '38. UnexpectedAlertPresentException',\n",
       " '39. UnknownMethodException',\n",
       " '40. UnreachableBrowserException',\n",
       " '41. UnsupportedCommandException']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Exception Names\n",
    "title=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"entry-content single-content\"]/p')\n",
    "\n",
    "for name in names[:41]:\n",
    "\n",
    "    title.append(name.text.split(',')[0].split(':')[0])\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc019e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' This type of Selenium exception occurs when an existing element in DOM has a feature set as hidden.',\n",
       " ' This Selenium exception occurs when an element is presented in the DOM, but you can be able to select. Therefore, it is not possible to interact.',\n",
       " ' This Exception occurs if an element could not be found.',\n",
       " ' This Exception occurs if the frame target to be switched to does not exist.',\n",
       " ' This Exception occurs when you switch to no presented alert.',\n",
       " ' This Exception occurs if the window target to be switch does not exist.',\n",
       " ' This Selenium exception occurs happens when the web element is detached from the current DOM.',\n",
       " ' The WebDriver is acting after you quit the browser.',\n",
       " ' Thrown when there is not enough time for a command to be completed. For Example, the element searched wasn’t found in the specified time.',\n",
       " ' This Exception takes place when the WebDriver is acting right after you close the browser.',\n",
       " ' This type of Exception takes place when there is a disconnection in the driver.',\n",
       " ' The command may not be completed as the element receiving the events is concealing the element which was requested clicked.',\n",
       " ' This Selenium exception is thrown when any element is presented in the DOM. However, it is impossible to interact with such an element.',\n",
       " ' This happens while interacting with the Firefox extension or the remote driver server.',\n",
       " ' Exception is used as a placeholder in case if the server returns an error without a stack trace.',\n",
       " ' This expectation will occur when IME engine activation has failed.',\n",
       " ' It takes place when IME support is unavailable.',\n",
       " ' Navigation made the user agent to hit a certificate warning. This can cause by an invalid or expired TLS certificate.',\n",
       " ' It occurs when an argument does not belong to the expected type.',\n",
       " ' This happens when you try to add a cookie under a different domain instead of current URL.',\n",
       " ' This type of Exception matches an interacting operation that is not valid.',\n",
       " ' It occurs when command can’t be finished when the element is invalid.',\n",
       " ' This Exception took place when the given session ID is not included in the list of active sessions. It means the session does not exist or is inactive either.',\n",
       " ' This occurs when the frame or window target to be switched does not exist.',\n",
       " ' This issue occurs while executing JavaScript given by the user.',\n",
       " ' It occurs when you afford to get the session when the session is not created.',\n",
       " ' This kind of Exception occurs when the attribute of an element could not be found.',\n",
       " ' It takes place if the target provided to the ActionChains move() methodology is not valid. For Example, out of the document.',\n",
       " ' ContextAware does mobile device testing.',\n",
       " ' This Exception occurs when no cookie matching with the given pathname found for all the associated cookies of the currently browsing document.',\n",
       " ' This Exception is a subclass of WebDriverException. This will occur when an element on the DOM does not exist.',\n",
       " ' This Selenium exception is thrown when the server is not responding because of the problem that the capabilities described are not proper.',\n",
       " ' It is not possible to capture a screen.',\n",
       " ' It happens when a new session could not be successfully created.',\n",
       " ' This occurs if a driver is unable to set a cookie.',\n",
       " ' Happens if a support class did not get a web element as expected.',\n",
       " ' This expectation occurs when there is an alert, but WebDriver is not able to perform Alert operation.',\n",
       " ' It occurs when there is the appearance of an unexpected alert.',\n",
       " ' This Exception happens when the requested command matches with a known URL but and not matching with a methodology for a specific URL.',\n",
       " ' This Exception occurs only when the browser is not able to be opened or crashed because of some reason.',\n",
       " ' This occurs when remote WebDriver doesn’t send valid commands as expected.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Exception Description\n",
    "titles=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"entry-content single-content\"]/p')\n",
    "\n",
    "for name in names[:41]:\n",
    "\n",
    "    titles.append(name.text.split(':')[1])\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7f39dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. ElementNotVisibleException</td>\n",
       "      <td>This type of Selenium exception occurs when a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. ElementNotSelectableException</td>\n",
       "      <td>This Selenium exception occurs when an elemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3. NoSuchElementException</td>\n",
       "      <td>This Exception occurs if an element could not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4. NoSuchFrameException</td>\n",
       "      <td>This Exception occurs if the frame target to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. NoAlertPresentException</td>\n",
       "      <td>This Exception occurs when you switch to no p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6. NoSuchWindowException</td>\n",
       "      <td>This Exception occurs if the window target to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7. StaleElementReferenceException</td>\n",
       "      <td>This Selenium exception occurs happens when t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8. SessionNotFoundException</td>\n",
       "      <td>The WebDriver is acting after you quit the br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9. TimeoutException</td>\n",
       "      <td>Thrown when there is not enough time for a co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10. WebDriverException</td>\n",
       "      <td>This Exception takes place when the WebDriver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11. ConnectionClosedException</td>\n",
       "      <td>This type of Exception takes place when there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12. ElementClickInterceptedException</td>\n",
       "      <td>The command may not be completed as the eleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13. ElementNotInteractableException</td>\n",
       "      <td>This Selenium exception is thrown when any el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14. ErrorInResponseException</td>\n",
       "      <td>This happens while interacting with the Firef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15. ErrorHandler.UnknownServerException</td>\n",
       "      <td>Exception is used as a placeholder in case if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16. ImeActivationFailedException</td>\n",
       "      <td>This expectation will occur when IME engine a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17. ImeNotAvailableException</td>\n",
       "      <td>It takes place when IME support is unavailable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18. InsecureCertificateException</td>\n",
       "      <td>Navigation made the user agent to hit a certi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19. InvalidArgumentException</td>\n",
       "      <td>It occurs when an argument does not belong to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20. InvalidCookieDomainException</td>\n",
       "      <td>This happens when you try to add a cookie und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21. InvalidCoordinatesException</td>\n",
       "      <td>This type of Exception matches an interacting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22. InvalidElementStateException</td>\n",
       "      <td>It occurs when command can’t be finished when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23. InvalidSessionIdException</td>\n",
       "      <td>This Exception took place when the given sess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24. InvalidSwitchToTargetException</td>\n",
       "      <td>This occurs when the frame or window target t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25. JavascriptException</td>\n",
       "      <td>This issue occurs while executing JavaScript ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26. JsonException</td>\n",
       "      <td>It occurs when you afford to get the session ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27. NoSuchAttributeException</td>\n",
       "      <td>This kind of Exception occurs when the attrib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28. MoveTargetOutOfBoundsException</td>\n",
       "      <td>It takes place if the target provided to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29. NoSuchContextException</td>\n",
       "      <td>ContextAware does mobile device testing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30. NoSuchCookieException</td>\n",
       "      <td>This Exception occurs when no cookie matching...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31. NotFoundException</td>\n",
       "      <td>This Exception is a subclass of WebDriverExce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32. RemoteDriverServerException</td>\n",
       "      <td>This Selenium exception is thrown when the se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33. ScreenshotException</td>\n",
       "      <td>It is not possible to capture a screen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34. SessionNotCreatedException</td>\n",
       "      <td>It happens when a new session could not be su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35. UnableToSetCookieException</td>\n",
       "      <td>This occurs if a driver is unable to set a co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36. UnexpectedTagNameException</td>\n",
       "      <td>Happens if a support class did not get a web ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37. UnhandledAlertException</td>\n",
       "      <td>This expectation occurs when there is an aler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38. UnexpectedAlertPresentException</td>\n",
       "      <td>It occurs when there is the appearance of an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39. UnknownMethodException</td>\n",
       "      <td>This Exception happens when the requested com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40. UnreachableBrowserException</td>\n",
       "      <td>This Exception occurs only when the browser i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41. UnsupportedCommandException</td>\n",
       "      <td>This occurs when remote WebDriver doesn’t sen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       NAME  \\\n",
       "0             1. ElementNotVisibleException   \n",
       "1          2. ElementNotSelectableException   \n",
       "2                 3. NoSuchElementException   \n",
       "3                   4. NoSuchFrameException   \n",
       "4                5. NoAlertPresentException   \n",
       "5                  6. NoSuchWindowException   \n",
       "6         7. StaleElementReferenceException   \n",
       "7               8. SessionNotFoundException   \n",
       "8                       9. TimeoutException   \n",
       "9                    10. WebDriverException   \n",
       "10            11. ConnectionClosedException   \n",
       "11     12. ElementClickInterceptedException   \n",
       "12      13. ElementNotInteractableException   \n",
       "13             14. ErrorInResponseException   \n",
       "14  15. ErrorHandler.UnknownServerException   \n",
       "15         16. ImeActivationFailedException   \n",
       "16             17. ImeNotAvailableException   \n",
       "17         18. InsecureCertificateException   \n",
       "18             19. InvalidArgumentException   \n",
       "19         20. InvalidCookieDomainException   \n",
       "20          21. InvalidCoordinatesException   \n",
       "21         22. InvalidElementStateException   \n",
       "22            23. InvalidSessionIdException   \n",
       "23       24. InvalidSwitchToTargetException   \n",
       "24                  25. JavascriptException   \n",
       "25                        26. JsonException   \n",
       "26             27. NoSuchAttributeException   \n",
       "27       28. MoveTargetOutOfBoundsException   \n",
       "28               29. NoSuchContextException   \n",
       "29                30. NoSuchCookieException   \n",
       "30                    31. NotFoundException   \n",
       "31          32. RemoteDriverServerException   \n",
       "32                  33. ScreenshotException   \n",
       "33           34. SessionNotCreatedException   \n",
       "34           35. UnableToSetCookieException   \n",
       "35           36. UnexpectedTagNameException   \n",
       "36              37. UnhandledAlertException   \n",
       "37      38. UnexpectedAlertPresentException   \n",
       "38               39. UnknownMethodException   \n",
       "39          40. UnreachableBrowserException   \n",
       "40          41. UnsupportedCommandException   \n",
       "\n",
       "                                          DESCRIPTION  \n",
       "0    This type of Selenium exception occurs when a...  \n",
       "1    This Selenium exception occurs when an elemen...  \n",
       "2    This Exception occurs if an element could not...  \n",
       "3    This Exception occurs if the frame target to ...  \n",
       "4    This Exception occurs when you switch to no p...  \n",
       "5    This Exception occurs if the window target to...  \n",
       "6    This Selenium exception occurs happens when t...  \n",
       "7    The WebDriver is acting after you quit the br...  \n",
       "8    Thrown when there is not enough time for a co...  \n",
       "9    This Exception takes place when the WebDriver...  \n",
       "10   This type of Exception takes place when there...  \n",
       "11   The command may not be completed as the eleme...  \n",
       "12   This Selenium exception is thrown when any el...  \n",
       "13   This happens while interacting with the Firef...  \n",
       "14   Exception is used as a placeholder in case if...  \n",
       "15   This expectation will occur when IME engine a...  \n",
       "16    It takes place when IME support is unavailable.  \n",
       "17   Navigation made the user agent to hit a certi...  \n",
       "18   It occurs when an argument does not belong to...  \n",
       "19   This happens when you try to add a cookie und...  \n",
       "20   This type of Exception matches an interacting...  \n",
       "21   It occurs when command can’t be finished when...  \n",
       "22   This Exception took place when the given sess...  \n",
       "23   This occurs when the frame or window target t...  \n",
       "24   This issue occurs while executing JavaScript ...  \n",
       "25   It occurs when you afford to get the session ...  \n",
       "26   This kind of Exception occurs when the attrib...  \n",
       "27   It takes place if the target provided to the ...  \n",
       "28           ContextAware does mobile device testing.  \n",
       "29   This Exception occurs when no cookie matching...  \n",
       "30   This Exception is a subclass of WebDriverExce...  \n",
       "31   This Selenium exception is thrown when the se...  \n",
       "32            It is not possible to capture a screen.  \n",
       "33   It happens when a new session could not be su...  \n",
       "34   This occurs if a driver is unable to set a co...  \n",
       "35   Happens if a support class did not get a web ...  \n",
       "36   This expectation occurs when there is an aler...  \n",
       "37   It occurs when there is the appearance of an ...  \n",
       "38   This Exception happens when the requested com...  \n",
       "39   This Exception occurs only when the browser i...  \n",
       "40   This occurs when remote WebDriver doesn’t sen...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe \n",
    "\n",
    "DF=pd.DataFrame({'NAME':title,'DESCRIPTION':titles})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "636fa5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c0363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b7559db",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "* You have to find following details:\n",
    "    *  A) Rank\n",
    "    *  B) State\n",
    "    *  C) GSDP(18-19)- at current prices\n",
    "    *  D) GSDP(19-20)- at current prices\n",
    "    *  E) Share(18-19)\n",
    "    *  F) GDP($ billion)\n",
    "* Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9350d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import re\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "09ae6f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')\n",
    "\n",
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b230536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the link\n",
    "url='http://statisticstimes.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8f923b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[2]/div[1]/div[2]/div[2]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f0803bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[2]/div[1]/div[2]/div[2]/div/a[3]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6d328837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34a9527d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping Sr NO\n",
    "number=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"dataTables_wrapper\"][1]/table//tbody/tr/td[1]')\n",
    "\n",
    "for name in names[:33]:\n",
    "\n",
    "    number.append(name.text)\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ecc28719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Maharashtra',\n",
       " 'Tamil Nadu',\n",
       " 'Uttar Pradesh',\n",
       " 'Gujarat',\n",
       " 'Karnataka',\n",
       " 'West Bengal',\n",
       " 'Rajasthan',\n",
       " 'Andhra Pradesh',\n",
       " 'Telangana',\n",
       " 'Madhya Pradesh',\n",
       " 'Kerala',\n",
       " 'Delhi',\n",
       " 'Haryana',\n",
       " 'Bihar',\n",
       " 'Punjab',\n",
       " 'Odisha',\n",
       " 'Assam',\n",
       " 'Chhattisgarh',\n",
       " 'Jharkhand',\n",
       " 'Uttarakhand',\n",
       " 'Jammu & Kashmir',\n",
       " 'Himachal Pradesh',\n",
       " 'Goa',\n",
       " 'Tripura',\n",
       " 'Chandigarh',\n",
       " 'Puducherry',\n",
       " 'Meghalaya',\n",
       " 'Sikkim',\n",
       " 'Manipur',\n",
       " 'Nagaland',\n",
       " 'Arunachal Pradesh',\n",
       " 'Mizoram',\n",
       " 'Andaman & Nicobar Islands']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping States\n",
    "state=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"dataTables_wrapper\"][1]/table//tbody/tr/td[2]')\n",
    "\n",
    "for name in names[:33]:\n",
    "\n",
    "    state.append(name.text)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "90eadaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '1,845,853',\n",
       " '1,687,818',\n",
       " '-',\n",
       " '1,631,977',\n",
       " '1,253,832',\n",
       " '1,020,989',\n",
       " '972,782',\n",
       " '969,604',\n",
       " '906,672',\n",
       " '-',\n",
       " '856,112',\n",
       " '831,610',\n",
       " '611,804',\n",
       " '574,760',\n",
       " '521,275',\n",
       " '-',\n",
       " '329,180',\n",
       " '328,598',\n",
       " '-',\n",
       " '-',\n",
       " '165,472',\n",
       " '80,449',\n",
       " '55,984',\n",
       " '-',\n",
       " '38,253',\n",
       " '36,572',\n",
       " '32,496',\n",
       " '31,790',\n",
       " '-',\n",
       " '-',\n",
       " '26,503',\n",
       " '-']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping GSDP 18-19\n",
    "GSDP1819=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"dataTables_wrapper\"][1]/table//tbody/tr/td[3]')\n",
    "\n",
    "for name in names[:33]:\n",
    "\n",
    "    GSDP1819.append(name.text)\n",
    "GSDP1819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "551113a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GSDP1819)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fba5156b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2,632,792',\n",
       " '1,630,208',\n",
       " '1,584,764',\n",
       " '1,502,899',\n",
       " '1,493,127',\n",
       " '1,089,898',\n",
       " '942,586',\n",
       " '862,957',\n",
       " '861,031',\n",
       " '809,592',\n",
       " '781,653',\n",
       " '774,870',\n",
       " '734,163',\n",
       " '530,363',\n",
       " '526,376',\n",
       " '487,805',\n",
       " '315,881',\n",
       " '304,063',\n",
       " '297,204',\n",
       " '245,895',\n",
       " '155,956',\n",
       " '153,845',\n",
       " '73,170',\n",
       " '49,845',\n",
       " '42,114',\n",
       " '34,433',\n",
       " '33,481',\n",
       " '28,723',\n",
       " '27,870',\n",
       " '27,283',\n",
       " '24,603',\n",
       " '22,287',\n",
       " '-']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping GSDP 19-20\n",
    "GSDP1920=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"dataTables_wrapper\"][1]/table//tbody/tr/td[4]')\n",
    "\n",
    "for name in names[:33]:\n",
    "\n",
    "    GSDP1920.append(name.text)\n",
    "GSDP1920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4fbd740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13.94%',\n",
       " '8.63%',\n",
       " '8.39%',\n",
       " '7.96%',\n",
       " '7.91%',\n",
       " '5.77%',\n",
       " '4.99%',\n",
       " '4.57%',\n",
       " '4.56%',\n",
       " '4.29%',\n",
       " '4.14%',\n",
       " '4.10%',\n",
       " '3.89%',\n",
       " '2.81%',\n",
       " '2.79%',\n",
       " '2.58%',\n",
       " '1.67%',\n",
       " '1.61%',\n",
       " '1.57%',\n",
       " '1.30%',\n",
       " '0.83%',\n",
       " '0.81%',\n",
       " '0.39%',\n",
       " '0.26%',\n",
       " '0.22%',\n",
       " '0.18%',\n",
       " '0.18%',\n",
       " '0.15%',\n",
       " '0.15%',\n",
       " '0.14%',\n",
       " '0.13%',\n",
       " '0.12%',\n",
       " '-']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Share 18-19\n",
    "share1819=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"dataTables_wrapper\"][1]/table//tbody/tr/td[5]')\n",
    "\n",
    "for name in names[:33]:\n",
    "\n",
    "    share1819.append(name.text)\n",
    "share1819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1b1d0824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['399.921',\n",
       " '247.629',\n",
       " '240.726',\n",
       " '228.290',\n",
       " '226.806',\n",
       " '165.556',\n",
       " '143.179',\n",
       " '131.083',\n",
       " '130.791',\n",
       " '122.977',\n",
       " '118.733',\n",
       " '117.703',\n",
       " '111.519',\n",
       " '80.562',\n",
       " '79.957',\n",
       " '74.098',\n",
       " '47.982',\n",
       " '46.187',\n",
       " '45.145',\n",
       " '37.351',\n",
       " '23.690',\n",
       " '23.369',\n",
       " '11.115',\n",
       " '7.571',\n",
       " '6.397',\n",
       " '5.230',\n",
       " '5.086',\n",
       " '4.363',\n",
       " '4.233',\n",
       " '4.144',\n",
       " '3.737',\n",
       " '3.385',\n",
       " '-']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping GDP\n",
    "GDP=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"dataTables_wrapper\"][1]/table//tbody/tr/td[6]')\n",
    "\n",
    "for name in names[:33]:\n",
    "\n",
    "    GDP.append(name.text)\n",
    "GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eea20608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP(18-19)</th>\n",
       "      <th>GSDP(19-20)</th>\n",
       "      <th>Share(18-19)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>942,586</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>972,782</td>\n",
       "      <td>862,957</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>969,604</td>\n",
       "      <td>861,031</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>906,672</td>\n",
       "      <td>809,592</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>-</td>\n",
       "      <td>781,653</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>856,112</td>\n",
       "      <td>774,870</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>831,610</td>\n",
       "      <td>734,163</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>611,804</td>\n",
       "      <td>530,363</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>574,760</td>\n",
       "      <td>526,376</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>521,275</td>\n",
       "      <td>487,805</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-</td>\n",
       "      <td>315,881</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>329,180</td>\n",
       "      <td>304,063</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>328,598</td>\n",
       "      <td>297,204</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>-</td>\n",
       "      <td>245,895</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>-</td>\n",
       "      <td>155,956</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>165,472</td>\n",
       "      <td>153,845</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>80,449</td>\n",
       "      <td>73,170</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>55,984</td>\n",
       "      <td>49,845</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>-</td>\n",
       "      <td>42,114</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>38,253</td>\n",
       "      <td>34,433</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>36,572</td>\n",
       "      <td>33,481</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>32,496</td>\n",
       "      <td>28,723</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>31,790</td>\n",
       "      <td>27,870</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>27,283</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>24,603</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>26,503</td>\n",
       "      <td>22,287</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP(18-19) GSDP(19-20) Share(18-19)  \\\n",
       "0     1                Maharashtra           -   2,632,792       13.94%   \n",
       "1     2                 Tamil Nadu   1,845,853   1,630,208        8.63%   \n",
       "2     3              Uttar Pradesh   1,687,818   1,584,764        8.39%   \n",
       "3     4                    Gujarat           -   1,502,899        7.96%   \n",
       "4     5                  Karnataka   1,631,977   1,493,127        7.91%   \n",
       "5     6                West Bengal   1,253,832   1,089,898        5.77%   \n",
       "6     7                  Rajasthan   1,020,989     942,586        4.99%   \n",
       "7     8             Andhra Pradesh     972,782     862,957        4.57%   \n",
       "8     9                  Telangana     969,604     861,031        4.56%   \n",
       "9    10             Madhya Pradesh     906,672     809,592        4.29%   \n",
       "10   11                     Kerala           -     781,653        4.14%   \n",
       "11   12                      Delhi     856,112     774,870        4.10%   \n",
       "12   13                    Haryana     831,610     734,163        3.89%   \n",
       "13   14                      Bihar     611,804     530,363        2.81%   \n",
       "14   15                     Punjab     574,760     526,376        2.79%   \n",
       "15   16                     Odisha     521,275     487,805        2.58%   \n",
       "16   17                      Assam           -     315,881        1.67%   \n",
       "17   18               Chhattisgarh     329,180     304,063        1.61%   \n",
       "18   19                  Jharkhand     328,598     297,204        1.57%   \n",
       "19   20                Uttarakhand           -     245,895        1.30%   \n",
       "20   21            Jammu & Kashmir           -     155,956        0.83%   \n",
       "21   22           Himachal Pradesh     165,472     153,845        0.81%   \n",
       "22   23                        Goa      80,449      73,170        0.39%   \n",
       "23   24                    Tripura      55,984      49,845        0.26%   \n",
       "24   25                 Chandigarh           -      42,114        0.22%   \n",
       "25   26                 Puducherry      38,253      34,433        0.18%   \n",
       "26   27                  Meghalaya      36,572      33,481        0.18%   \n",
       "27   28                     Sikkim      32,496      28,723        0.15%   \n",
       "28   29                    Manipur      31,790      27,870        0.15%   \n",
       "29   30                   Nagaland           -      27,283        0.14%   \n",
       "30   31          Arunachal Pradesh           -      24,603        0.13%   \n",
       "31   32                    Mizoram      26,503      22,287        0.12%   \n",
       "32   33  Andaman & Nicobar Islands           -           -            -   \n",
       "\n",
       "   GDP($ billion)  \n",
       "0         399.921  \n",
       "1         247.629  \n",
       "2         240.726  \n",
       "3         228.290  \n",
       "4         226.806  \n",
       "5         165.556  \n",
       "6         143.179  \n",
       "7         131.083  \n",
       "8         130.791  \n",
       "9         122.977  \n",
       "10        118.733  \n",
       "11        117.703  \n",
       "12        111.519  \n",
       "13         80.562  \n",
       "14         79.957  \n",
       "15         74.098  \n",
       "16         47.982  \n",
       "17         46.187  \n",
       "18         45.145  \n",
       "19         37.351  \n",
       "20         23.690  \n",
       "21         23.369  \n",
       "22         11.115  \n",
       "23          7.571  \n",
       "24          6.397  \n",
       "25          5.230  \n",
       "26          5.086  \n",
       "27          4.363  \n",
       "28          4.233  \n",
       "29          4.144  \n",
       "30          3.737  \n",
       "31          3.385  \n",
       "32              -  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe \n",
    "\n",
    "DF=pd.DataFrame({'Rank':number,'State':state,'GSDP(18-19)':GSDP1819,'GSDP(19-20)':GSDP1920,'Share(18-19)':share1819,'GDP($ billion)':GDP})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d51b07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd47a642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3362b95",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "* You have to find the following details:\n",
    "    *  A) Repository title\n",
    "    *  B) Repository description\n",
    "    *  C) Contributors count\n",
    "    *  D) Language used\n",
    "* Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4171d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import re\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59cdea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')\n",
    "\n",
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c01d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the link\n",
    "url='https://github.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6211febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[1]/div[5]/div/div/div/div/div/main/div[2]/div/div/turbo-frame/tab-container/div[2]/div[1]/a')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51d43853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[1]/div[5]/main/div[2]/div/div/div[3]/div[1]/div[3]/a')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9373deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URLs of the trending Github\n",
    "product_URL = []\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//h1[@class=\"h3 lh-condensed\"]//a')\n",
    "    for i in url:\n",
    "        product_URL.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e658002b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0f5b094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/invoke-ai/InvokeAI',\n",
       " 'https://github.com/apple/ml-stable-diffusion',\n",
       " 'https://github.com/AleoHQ/snarkOS',\n",
       " 'https://github.com/misskey-dev/misskey',\n",
       " 'https://github.com/Klipper3d/klipper',\n",
       " 'https://github.com/PKUFlyingPig/cs-self-learning',\n",
       " 'https://github.com/danielgross/whatsapp-gpt',\n",
       " 'https://github.com/hehonghui/awesome-english-ebooks',\n",
       " 'https://github.com/Visualize-ML/Book4_Power-of-Matrix',\n",
       " 'https://github.com/thangchung/go-coffeeshop',\n",
       " 'https://github.com/monicahq/monica',\n",
       " 'https://github.com/coolsnowwolf/lede',\n",
       " 'https://github.com/Bogdanp/awesome-advent-of-code',\n",
       " 'https://github.com/palera1n/palera1n',\n",
       " 'https://github.com/t3-oss/create-t3-app',\n",
       " 'https://github.com/maplecool/easytrojan',\n",
       " 'https://github.com/openai/openai-quickstart-node',\n",
       " 'https://github.com/microsoft/PowerToys',\n",
       " 'https://github.com/evrone/go-clean-template',\n",
       " 'https://github.com/openzfs/zfs',\n",
       " 'https://github.com/recloudstream/cloudstream',\n",
       " 'https://github.com/fspoettel/advent-of-code-rust',\n",
       " 'https://github.com/getify/You-Dont-Know-JS',\n",
       " 'https://github.com/openai/openai-quickstart-python',\n",
       " 'https://github.com/linebender/xilem']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1aea6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of the trending Github Description\n",
    "disc=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/div[1]/div[5]/div/main/turbo-frame/div/div/div/div[3]/div[2]/div/div[1]/div/p')\n",
    "        disc.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        disc.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4beb741a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This version of Stable Diffusion features a slick WebGUI, an interactive command-line script that combines text2img and img2img functionality in a \"dream bot\" style interface, and multiple features and other enhancements. For more info, see the website link below.',\n",
       " 'Stable Diffusion with Core ML on Apple Silicon',\n",
       " 'A Decentralized Operating System for ZK Applications',\n",
       " '🌎 An interplanetary microblogging platform 🚀',\n",
       " 'Klipper is a 3d-printer firmware',\n",
       " '计算机自学指南',\n",
       " '-',\n",
       " '经济学人(含音频)、纽约客、卫报、连线、大西洋月刊等英语杂志免费下载,支持epub、mobi、pdf格式, 每周更新',\n",
       " 'Book_4_《矩阵力量》 | 鸢尾花书：从加减乘除到机器学习；本册有，584幅图，81个代码文件，其中18个Streamlit App；状态：清华社五审五校中；Github稿件基本稳定，欢迎提意见，会及时修改',\n",
       " '☕ A practical event-driven microservices demo built with Golang. Nomad, Consul Connect, Vault, and Terraform for deployment',\n",
       " 'Personal CRM. Remember everything about your friends, family and business relationships.',\n",
       " \"Lean's LEDE source\",\n",
       " 'A collection of awesome resources related to the yearly Advent of Code challenge.',\n",
       " 'iOS 15.0-15.7.1 (semi-)tethered checkm8 \"jailbreak\"',\n",
       " 'The best way to start a full-stack, typesafe Next.js app',\n",
       " '世界上最简单的Trojan部署脚本，仅需一行命令即可搭建一台代理服务器',\n",
       " 'Node.js example app from the OpenAI API quickstart tutorial',\n",
       " 'Windows system utilities to maximize productivity',\n",
       " 'Clean Architecture template for Golang services',\n",
       " 'OpenZFS on Linux and FreeBSD',\n",
       " 'Android app for streaming and downloading Movies, TV-Series and Anime.',\n",
       " '🎄Starter template for solving Advent of Code in Rust.',\n",
       " 'A book series on JavaScript. @YDKJS on twitter.',\n",
       " 'Python example app from the OpenAI API quickstart tutorial',\n",
       " 'An experimental Rust native UI framework']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "35c68f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a44bbb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of the trending Github Titles\n",
    "title=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/div[1]/div[5]/div/main/div/div[1]/div/div/span[1]/a')\n",
    "        title.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        title.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d67ceaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "47ca02b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['invoke-ai',\n",
       " 'apple',\n",
       " 'AleoHQ',\n",
       " 'misskey-dev',\n",
       " 'Klipper3d',\n",
       " 'PKUFlyingPig',\n",
       " 'danielgross',\n",
       " 'hehonghui',\n",
       " 'Visualize-ML',\n",
       " 'thangchung',\n",
       " 'monicahq',\n",
       " 'coolsnowwolf',\n",
       " 'Bogdanp',\n",
       " 'palera1n',\n",
       " 't3-oss',\n",
       " 'maplecool',\n",
       " 'openai',\n",
       " 'microsoft',\n",
       " 'evrone',\n",
       " 'openzfs',\n",
       " 'recloudstream',\n",
       " 'fspoettel',\n",
       " 'getify',\n",
       " 'openai',\n",
       " 'linebender']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c1765c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of the trending Github Contributors count\n",
    "con=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/div[1]/div[5]/div/main/turbo-frame/div/div/div/div[3]/div[2]/div/div[4]/div/h2/a/span')\n",
    "        con.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        con.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "da8993f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b00152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '-',\n",
       " '44',\n",
       " '',\n",
       " '',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '1',\n",
       " '1',\n",
       " '938',\n",
       " '-',\n",
       " '134',\n",
       " '-',\n",
       " '-',\n",
       " '323',\n",
       " '12',\n",
       " '524',\n",
       " '-',\n",
       " '3',\n",
       " '-',\n",
       " '-',\n",
       " '7']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f68918dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of the trending Github Language\n",
    "Lan=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/div[1]/div[5]/div/main/turbo-frame/div/div/div/div[3]/div[2]/div/div[5]/div/ul/li/a/span[1]')\n",
    "        Lan.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        Lan.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "193d4b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Lan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6ed20f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " 'Python',\n",
       " 'Rust',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " 'Go',\n",
       " 'CSS',\n",
       " 'Python',\n",
       " 'Go',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " 'TypeScript',\n",
       " '-',\n",
       " '-',\n",
       " 'C#',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " 'CSS',\n",
       " '-']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42317a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository title</th>\n",
       "      <th>Repository description</th>\n",
       "      <th>Contributors count</th>\n",
       "      <th>Language used</th>\n",
       "      <th>Repository URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>invoke-ai</td>\n",
       "      <td>This version of Stable Diffusion features a sl...</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/invoke-ai/InvokeAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple</td>\n",
       "      <td>Stable Diffusion with Core ML on Apple Silicon</td>\n",
       "      <td>-</td>\n",
       "      <td>Python</td>\n",
       "      <td>https://github.com/apple/ml-stable-diffusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AleoHQ</td>\n",
       "      <td>A Decentralized Operating System for ZK Applic...</td>\n",
       "      <td>44</td>\n",
       "      <td>Rust</td>\n",
       "      <td>https://github.com/AleoHQ/snarkOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>misskey-dev</td>\n",
       "      <td>🌎 An interplanetary microblogging platform 🚀</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/misskey-dev/misskey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Klipper3d</td>\n",
       "      <td>Klipper is a 3d-printer firmware</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/Klipper3d/klipper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PKUFlyingPig</td>\n",
       "      <td>计算机自学指南</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/PKUFlyingPig/cs-self-learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>danielgross</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Go</td>\n",
       "      <td>https://github.com/danielgross/whatsapp-gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hehonghui</td>\n",
       "      <td>经济学人(含音频)、纽约客、卫报、连线、大西洋月刊等英语杂志免费下载,支持epub、mobi...</td>\n",
       "      <td>-</td>\n",
       "      <td>CSS</td>\n",
       "      <td>https://github.com/hehonghui/awesome-english-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Visualize-ML</td>\n",
       "      <td>Book_4_《矩阵力量》 | 鸢尾花书：从加减乘除到机器学习；本册有，584幅图，81个代...</td>\n",
       "      <td>-</td>\n",
       "      <td>Python</td>\n",
       "      <td>https://github.com/Visualize-ML/Book4_Power-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thangchung</td>\n",
       "      <td>☕ A practical event-driven microservices demo ...</td>\n",
       "      <td>-</td>\n",
       "      <td>Go</td>\n",
       "      <td>https://github.com/thangchung/go-coffeeshop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>monicahq</td>\n",
       "      <td>Personal CRM. Remember everything about your f...</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/monicahq/monica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>coolsnowwolf</td>\n",
       "      <td>Lean's LEDE source</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/coolsnowwolf/lede</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bogdanp</td>\n",
       "      <td>A collection of awesome resources related to t...</td>\n",
       "      <td>938</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/Bogdanp/awesome-advent-of-code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>palera1n</td>\n",
       "      <td>iOS 15.0-15.7.1 (semi-)tethered checkm8 \"jailb...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/palera1n/palera1n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>t3-oss</td>\n",
       "      <td>The best way to start a full-stack, typesafe N...</td>\n",
       "      <td>134</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>https://github.com/t3-oss/create-t3-app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>maplecool</td>\n",
       "      <td>世界上最简单的Trojan部署脚本，仅需一行命令即可搭建一台代理服务器</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/maplecool/easytrojan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>openai</td>\n",
       "      <td>Node.js example app from the OpenAI API quicks...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/openai/openai-quickstart-node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>microsoft</td>\n",
       "      <td>Windows system utilities to maximize productivity</td>\n",
       "      <td>323</td>\n",
       "      <td>C#</td>\n",
       "      <td>https://github.com/microsoft/PowerToys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>evrone</td>\n",
       "      <td>Clean Architecture template for Golang services</td>\n",
       "      <td>12</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/evrone/go-clean-template</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>openzfs</td>\n",
       "      <td>OpenZFS on Linux and FreeBSD</td>\n",
       "      <td>524</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/openzfs/zfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>recloudstream</td>\n",
       "      <td>Android app for streaming and downloading Movi...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/recloudstream/cloudstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>fspoettel</td>\n",
       "      <td>🎄Starter template for solving Advent of Code i...</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/fspoettel/advent-of-code-rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>getify</td>\n",
       "      <td>A book series on JavaScript. @YDKJS on twitter.</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/getify/You-Dont-Know-JS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>openai</td>\n",
       "      <td>Python example app from the OpenAI API quickst...</td>\n",
       "      <td>-</td>\n",
       "      <td>CSS</td>\n",
       "      <td>https://github.com/openai/openai-quickstart-py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>linebender</td>\n",
       "      <td>An experimental Rust native UI framework</td>\n",
       "      <td>7</td>\n",
       "      <td>-</td>\n",
       "      <td>https://github.com/linebender/xilem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Repository title                             Repository description  \\\n",
       "0         invoke-ai  This version of Stable Diffusion features a sl...   \n",
       "1             apple     Stable Diffusion with Core ML on Apple Silicon   \n",
       "2            AleoHQ  A Decentralized Operating System for ZK Applic...   \n",
       "3       misskey-dev       🌎 An interplanetary microblogging platform 🚀   \n",
       "4         Klipper3d                   Klipper is a 3d-printer firmware   \n",
       "5      PKUFlyingPig                                            计算机自学指南   \n",
       "6       danielgross                                                  -   \n",
       "7         hehonghui  经济学人(含音频)、纽约客、卫报、连线、大西洋月刊等英语杂志免费下载,支持epub、mobi...   \n",
       "8      Visualize-ML  Book_4_《矩阵力量》 | 鸢尾花书：从加减乘除到机器学习；本册有，584幅图，81个代...   \n",
       "9        thangchung  ☕ A practical event-driven microservices demo ...   \n",
       "10         monicahq  Personal CRM. Remember everything about your f...   \n",
       "11     coolsnowwolf                                 Lean's LEDE source   \n",
       "12          Bogdanp  A collection of awesome resources related to t...   \n",
       "13         palera1n  iOS 15.0-15.7.1 (semi-)tethered checkm8 \"jailb...   \n",
       "14           t3-oss  The best way to start a full-stack, typesafe N...   \n",
       "15        maplecool                世界上最简单的Trojan部署脚本，仅需一行命令即可搭建一台代理服务器   \n",
       "16           openai  Node.js example app from the OpenAI API quicks...   \n",
       "17        microsoft  Windows system utilities to maximize productivity   \n",
       "18           evrone    Clean Architecture template for Golang services   \n",
       "19          openzfs                       OpenZFS on Linux and FreeBSD   \n",
       "20    recloudstream  Android app for streaming and downloading Movi...   \n",
       "21        fspoettel  🎄Starter template for solving Advent of Code i...   \n",
       "22           getify    A book series on JavaScript. @YDKJS on twitter.   \n",
       "23           openai  Python example app from the OpenAI API quickst...   \n",
       "24       linebender           An experimental Rust native UI framework   \n",
       "\n",
       "   Contributors count Language used  \\\n",
       "0                   1             -   \n",
       "1                   -        Python   \n",
       "2                  44          Rust   \n",
       "3                                 -   \n",
       "4                                 -   \n",
       "5                   -             -   \n",
       "6                   -            Go   \n",
       "7                   -           CSS   \n",
       "8                   -        Python   \n",
       "9                   -            Go   \n",
       "10                  1             -   \n",
       "11                  1             -   \n",
       "12                938             -   \n",
       "13                  -             -   \n",
       "14                134    TypeScript   \n",
       "15                  -             -   \n",
       "16                  -             -   \n",
       "17                323            C#   \n",
       "18                 12             -   \n",
       "19                524             -   \n",
       "20                  -             -   \n",
       "21                  3             -   \n",
       "22                  -             -   \n",
       "23                  -           CSS   \n",
       "24                  7             -   \n",
       "\n",
       "                                       Repository URL  \n",
       "0               https://github.com/invoke-ai/InvokeAI  \n",
       "1        https://github.com/apple/ml-stable-diffusion  \n",
       "2                   https://github.com/AleoHQ/snarkOS  \n",
       "3              https://github.com/misskey-dev/misskey  \n",
       "4                https://github.com/Klipper3d/klipper  \n",
       "5    https://github.com/PKUFlyingPig/cs-self-learning  \n",
       "6         https://github.com/danielgross/whatsapp-gpt  \n",
       "7   https://github.com/hehonghui/awesome-english-e...  \n",
       "8   https://github.com/Visualize-ML/Book4_Power-of...  \n",
       "9         https://github.com/thangchung/go-coffeeshop  \n",
       "10                 https://github.com/monicahq/monica  \n",
       "11               https://github.com/coolsnowwolf/lede  \n",
       "12  https://github.com/Bogdanp/awesome-advent-of-code  \n",
       "13               https://github.com/palera1n/palera1n  \n",
       "14            https://github.com/t3-oss/create-t3-app  \n",
       "15            https://github.com/maplecool/easytrojan  \n",
       "16   https://github.com/openai/openai-quickstart-node  \n",
       "17             https://github.com/microsoft/PowerToys  \n",
       "18        https://github.com/evrone/go-clean-template  \n",
       "19                     https://github.com/openzfs/zfs  \n",
       "20       https://github.com/recloudstream/cloudstream  \n",
       "21   https://github.com/fspoettel/advent-of-code-rust  \n",
       "22         https://github.com/getify/You-Dont-Know-JS  \n",
       "23  https://github.com/openai/openai-quickstart-py...  \n",
       "24                https://github.com/linebender/xilem  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe \n",
    "\n",
    "DF=pd.DataFrame({'Repository title':title,'Repository description':disc,'Contributors count':con,'Language used':Lan,'Repository URL':product_URL})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d94ef2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1584d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44fe211d",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/\n",
    "* You have to find the following details:\n",
    "    *  A) Song name\n",
    "    *  B) Artist name\n",
    "    *  C) Last week rank\n",
    "    *  D) Peak rank\n",
    "    *  E) Weeks on board\n",
    "* Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fe9f8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import re\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "816e3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')\n",
    "\n",
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a9de1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the link\n",
    "url='https://www.billboard.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e772a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click button\n",
    "search_btn=driver.find_element(By.XPATH,'/html/body/div[3]/main/div[2]/div[1]/div[1]/div[1]/div[2]/div/div[2]/a[1]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1f841277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Midnights',\n",
       " 'Her Loss',\n",
       " 'Un Verano Sin Ti',\n",
       " \"It's Only Me\",\n",
       " 'Dangerous: The Double Album',\n",
       " 'The Highlights',\n",
       " 'Thriller',\n",
       " \"Harry's House\",\n",
       " \"Jupiter's Diary: 7 Day Theory\",\n",
       " 'Christmas',\n",
       " 'Diamonds',\n",
       " 'American Heartbreak',\n",
       " 'Divisive',\n",
       " 'Feed Tha Streets III',\n",
       " 'The Family',\n",
       " 'Renaissance',\n",
       " 'A Charlie Brown Christmas (Soundtrack)',\n",
       " 'The Christmas Song',\n",
       " 'Merry Christmas',\n",
       " 'Folklore',\n",
       " \"Red (Taylor's Version)\",\n",
       " 'Sour',\n",
       " 'Black Panther: Wakanda Forever: Music From And Inspired By',\n",
       " 'Gemini Rights',\n",
       " 'Only Built For Infinity Links',\n",
       " 'I Never Liked You',\n",
       " 'Diamonds & Rhinestones: The Greatest Hits Collection',\n",
       " 'Leave The Light On',\n",
       " 'Lover',\n",
       " \"Get Rollin'\",\n",
       " \"Growin' Up\",\n",
       " 'Rumours',\n",
       " 'Beautiful Mind',\n",
       " 'Curtain Call: The Hits',\n",
       " 'Certified Lover Boy',\n",
       " 'Anyways, Life’s Great...',\n",
       " 'Me / And / Dad',\n",
       " 'The Best Of Pentatonix Christmas',\n",
       " 'Greatest Hits',\n",
       " 'good kid, m.A.A.d city',\n",
       " '1989',\n",
       " 'Evermore',\n",
       " 'Ultimate Christmas',\n",
       " 'Smithereens',\n",
       " 'My Turn',\n",
       " 'Christmas Classics',\n",
       " \"Hollywood's Bleeding\",\n",
       " 'Traveller',\n",
       " \"This One's For You\",\n",
       " 'Goodbye & Good Riddance',\n",
       " 'Mr. Morale & The Big Steppers',\n",
       " 'The Andy Williams Christmas Album',\n",
       " 'Curtain Call 2',\n",
       " 'Chronicle The 20 Greatest Hits',\n",
       " '30',\n",
       " 'Ctrl',\n",
       " 'If I Know Me',\n",
       " 'Indigo',\n",
       " '7220',\n",
       " 'Rudolph The Red-Nosed Reindeer',\n",
       " 'What You See Is What You Get',\n",
       " 'Fine Line',\n",
       " 'Planet Her',\n",
       " \"Rockin' Around The Christmas Tree: The Decca Christmas Recordings\",\n",
       " \"Takin' It Back\",\n",
       " '=',\n",
       " 'reputation',\n",
       " 'The Melodic Blue',\n",
       " \"Fearless (Taylor's Version)\",\n",
       " 'Proof',\n",
       " 'A Christmas Gift For You From Phil Spector',\n",
       " 'Shoot For The Stars Aim For The Moon',\n",
       " 'Only The Strong Survive',\n",
       " 'Saturno',\n",
       " 'Sonder',\n",
       " 'Happier Than Ever',\n",
       " 'YHLQMDLG',\n",
       " 'This Is What ____ Feels Like (Vol.1-4)',\n",
       " 'Twelve Carat Toothache',\n",
       " 'Honestly, Nevermind',\n",
       " 'DAMN.',\n",
       " 'Who Is Nardo Wick?',\n",
       " 'When We All Fall Asleep, Where Do We Go?',\n",
       " 'Legends Never Die',\n",
       " 'Starting Over',\n",
       " 'Take Care',\n",
       " 'The Best Of Bobby Helms',\n",
       " 'The Black Parade',\n",
       " 'Different Man',\n",
       " 'Encanto (Highlights)',\n",
       " 'Wrapped In Red',\n",
       " 'Wasteland',\n",
       " 'Scorpion',\n",
       " 'Greatest Hits',\n",
       " '2014 Forest Hills Drive',\n",
       " 'Doo-Wops & Hooligans',\n",
       " 'Hot Rocks 1964-1971',\n",
       " 'The Original: Gene Autry Sings Rudolph The Red Nosed Reindeer & Other...',\n",
       " 'Future Nostalgia',\n",
       " 'The Hits 2']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping top 100 Song Names\n",
    "songname=[]\n",
    "names = driver.find_elements(By.XPATH,'//ul[@class=\"lrv-a-unstyle-list lrv-u-flex lrv-u-height-100p lrv-u-flex-direction-column@mobile-max\"]')\n",
    "\n",
    "for name in names[:100]:\n",
    "\n",
    "    songname.append(name.text.split('\\n')[0])\n",
    "songname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8c2954b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taylor Swift',\n",
       " 'Drake & 21 Savage',\n",
       " 'Bad Bunny',\n",
       " 'Lil Baby',\n",
       " 'Morgan Wallen',\n",
       " 'The Weeknd',\n",
       " 'Michael Jackson',\n",
       " 'Harry Styles',\n",
       " 'Rod Wave',\n",
       " 'Michael Buble',\n",
       " 'Elton John',\n",
       " 'Zach Bryan',\n",
       " 'Disturbed',\n",
       " 'Roddy Ricch',\n",
       " 'BrockHampton',\n",
       " 'Beyonce',\n",
       " 'Vince Guaraldi Trio',\n",
       " 'Nat King Cole',\n",
       " 'Mariah Carey',\n",
       " 'Taylor Swift',\n",
       " 'Taylor Swift',\n",
       " 'Olivia Rodrigo',\n",
       " 'Soundtrack',\n",
       " 'Steve Lacy',\n",
       " 'Quavo & Takeoff',\n",
       " 'Future',\n",
       " 'Dolly Parton',\n",
       " 'Bailey Zimmerman',\n",
       " 'Taylor Swift',\n",
       " 'Nickelback',\n",
       " 'Luke Combs',\n",
       " 'Fleetwood Mac',\n",
       " 'Rod Wave',\n",
       " 'Eminem',\n",
       " 'Drake',\n",
       " 'GloRilla',\n",
       " 'Billy Strings',\n",
       " 'Pentatonix',\n",
       " 'Queen',\n",
       " 'Kendrick Lamar',\n",
       " 'Taylor Swift',\n",
       " 'Taylor Swift',\n",
       " 'Frank Sinatra',\n",
       " 'Joji',\n",
       " 'Lil Baby',\n",
       " 'Bing Crosby',\n",
       " 'Post Malone',\n",
       " 'Chris Stapleton',\n",
       " 'Luke Combs',\n",
       " 'Juice WRLD',\n",
       " 'Kendrick Lamar',\n",
       " 'Andy Williams',\n",
       " 'Eminem',\n",
       " 'Creedence Clearwater Revival',\n",
       " 'Adele',\n",
       " 'SZA',\n",
       " 'Morgan Wallen',\n",
       " 'Chris Brown',\n",
       " 'Lil Durk',\n",
       " 'Burl Ives',\n",
       " 'Luke Combs',\n",
       " 'Harry Styles',\n",
       " 'Doja Cat',\n",
       " 'Brenda Lee',\n",
       " 'Meghan Trainor',\n",
       " 'Ed Sheeran',\n",
       " 'Taylor Swift',\n",
       " 'Baby Keem',\n",
       " 'Taylor Swift',\n",
       " 'BTS',\n",
       " 'Various Artists',\n",
       " 'Pop Smoke',\n",
       " 'Bruce Springsteen',\n",
       " 'Rauw Alejandro',\n",
       " 'Dermot Kennedy',\n",
       " 'Billie Eilish',\n",
       " 'Bad Bunny',\n",
       " 'JVKE',\n",
       " 'Post Malone',\n",
       " 'Drake',\n",
       " 'Kendrick Lamar',\n",
       " 'Nardo Wick',\n",
       " 'Billie Eilish',\n",
       " 'Juice WRLD',\n",
       " 'Chris Stapleton',\n",
       " 'Drake',\n",
       " 'Bobby Helms',\n",
       " 'My Chemical Romance',\n",
       " 'Kane Brown',\n",
       " 'Soundtrack',\n",
       " 'Kelly Clarkson',\n",
       " 'Brent Faiyaz',\n",
       " 'Drake',\n",
       " '2Pac',\n",
       " 'J. Cole',\n",
       " 'Bruno Mars',\n",
       " 'The Rolling Stones',\n",
       " 'Gene Autry',\n",
       " 'Dua Lipa',\n",
       " 'Prince']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping top 100 Song Artist\n",
    "artist=[]\n",
    "names = driver.find_elements(By.XPATH,'//ul[@class=\"lrv-a-unstyle-list lrv-u-flex lrv-u-height-100p lrv-u-flex-direction-column@mobile-max\"]')\n",
    "\n",
    "for name in names[:100]:\n",
    "\n",
    "    artist.append(name.text.split('\\n')[1])\n",
    "artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7f84fd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '6',\n",
       " '7',\n",
       " '115',\n",
       " '9',\n",
       " '-',\n",
       " '19',\n",
       " '17',\n",
       " '13',\n",
       " '-',\n",
       " '-',\n",
       " '-',\n",
       " '15',\n",
       " '43',\n",
       " '77',\n",
       " '40',\n",
       " '20',\n",
       " '24',\n",
       " '28',\n",
       " '12',\n",
       " '14',\n",
       " '16',\n",
       " '23',\n",
       " '-',\n",
       " '22',\n",
       " '30',\n",
       " '-',\n",
       " '27',\n",
       " '36',\n",
       " '21',\n",
       " '29',\n",
       " '26',\n",
       " '11',\n",
       " '-',\n",
       " '88',\n",
       " '38',\n",
       " '32',\n",
       " '35',\n",
       " '44',\n",
       " '-',\n",
       " '18',\n",
       " '31',\n",
       " '111',\n",
       " '33',\n",
       " '62',\n",
       " '37',\n",
       " '34',\n",
       " '45',\n",
       " '113',\n",
       " '42',\n",
       " '60',\n",
       " '131',\n",
       " '39',\n",
       " '48',\n",
       " '47',\n",
       " '46',\n",
       " '154',\n",
       " '49',\n",
       " '72',\n",
       " '51',\n",
       " '165',\n",
       " '58',\n",
       " '63',\n",
       " '61',\n",
       " '41',\n",
       " '79',\n",
       " '69',\n",
       " '108',\n",
       " '54',\n",
       " '8',\n",
       " '25',\n",
       " '-',\n",
       " '84',\n",
       " '65',\n",
       " '75',\n",
       " '56',\n",
       " '52',\n",
       " '64',\n",
       " '57',\n",
       " '87',\n",
       " '68',\n",
       " '71',\n",
       " '66',\n",
       " '-',\n",
       " '85',\n",
       " '67',\n",
       " '106',\n",
       " '-',\n",
       " '70',\n",
       " '76',\n",
       " '80',\n",
       " '73',\n",
       " '109',\n",
       " '53',\n",
       " '-',\n",
       " '78',\n",
       " '112']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping top 100 Song lastweek Rank\n",
    "lastweek=[]\n",
    "names = driver.find_elements(By.XPATH,'//ul[@class=\"lrv-a-unstyle-list lrv-u-flex lrv-u-height-100p lrv-u-flex-direction-column@mobile-max\"]')\n",
    "\n",
    "for name in names[:100]:\n",
    "\n",
    "    lastweek.append(name.text.split('\\n')[2])\n",
    "lastweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9dbe8302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '9',\n",
       " '1',\n",
       " '7',\n",
       " '5',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '1',\n",
       " '6',\n",
       " '6',\n",
       " '3',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '12',\n",
       " '7',\n",
       " '7',\n",
       " '1',\n",
       " '27',\n",
       " '9',\n",
       " '1',\n",
       " '30',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '11',\n",
       " '37',\n",
       " '7',\n",
       " '8',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '12',\n",
       " '5',\n",
       " '1',\n",
       " '18',\n",
       " '1',\n",
       " '1',\n",
       " '4',\n",
       " '4',\n",
       " '1',\n",
       " '14',\n",
       " '6',\n",
       " '18',\n",
       " '1',\n",
       " '3',\n",
       " '10',\n",
       " '1',\n",
       " '1',\n",
       " '16',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '17',\n",
       " '16',\n",
       " '1',\n",
       " '1',\n",
       " '5',\n",
       " '1',\n",
       " '1',\n",
       " '10',\n",
       " '1',\n",
       " '8',\n",
       " '25',\n",
       " '75',\n",
       " '1',\n",
       " '2',\n",
       " '56',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '16',\n",
       " '1',\n",
       " '1',\n",
       " '3',\n",
       " '1',\n",
       " '27',\n",
       " '2',\n",
       " '5',\n",
       " '19',\n",
       " '3',\n",
       " '2',\n",
       " '1',\n",
       " '3',\n",
       " '1',\n",
       " '3',\n",
       " '4',\n",
       " '98',\n",
       " '3',\n",
       " '54']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping top 100 Song peakweek Rank\n",
    "peak=[]\n",
    "names = driver.find_elements(By.XPATH,'//ul[@class=\"lrv-a-unstyle-list lrv-u-flex lrv-u-height-100p lrv-u-flex-direction-column@mobile-max\"]')\n",
    "\n",
    "for name in names[:100]:\n",
    "\n",
    "    peak.append(name.text.split('\\n')[3])\n",
    "peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "92f357f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5',\n",
       " '3',\n",
       " '29',\n",
       " '6',\n",
       " '98',\n",
       " '93',\n",
       " '549',\n",
       " '27',\n",
       " '1',\n",
       " '103',\n",
       " '263',\n",
       " '27',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '17',\n",
       " '102',\n",
       " '68',\n",
       " '111',\n",
       " '122',\n",
       " '54',\n",
       " '79',\n",
       " '3',\n",
       " '19',\n",
       " '7',\n",
       " '30',\n",
       " '1',\n",
       " '6',\n",
       " '170',\n",
       " '1',\n",
       " '22',\n",
       " '504',\n",
       " '15',\n",
       " '607',\n",
       " '64',\n",
       " '2',\n",
       " '1',\n",
       " '29',\n",
       " '518',\n",
       " '526',\n",
       " '415',\n",
       " '102',\n",
       " '33',\n",
       " '3',\n",
       " '143',\n",
       " '45',\n",
       " '168',\n",
       " '376',\n",
       " '286',\n",
       " '236',\n",
       " '28',\n",
       " '23',\n",
       " '16',\n",
       " '617',\n",
       " '53',\n",
       " '285',\n",
       " '217',\n",
       " '134',\n",
       " '37',\n",
       " '51',\n",
       " '159',\n",
       " '154',\n",
       " '74',\n",
       " '21',\n",
       " '5',\n",
       " '56',\n",
       " '218',\n",
       " '63',\n",
       " '72',\n",
       " '24',\n",
       " '26',\n",
       " '125',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '69',\n",
       " '143',\n",
       " '9',\n",
       " '25',\n",
       " '23',\n",
       " '293',\n",
       " '51',\n",
       " '191',\n",
       " '124',\n",
       " '106',\n",
       " '508',\n",
       " '14',\n",
       " '114',\n",
       " '11',\n",
       " '4',\n",
       " '58',\n",
       " '20',\n",
       " '230',\n",
       " '450',\n",
       " '416',\n",
       " '599',\n",
       " '398',\n",
       " '1',\n",
       " '138',\n",
       " '22']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping top 100 Song Weeks on board\n",
    "names = driver.find_elements(By.XPATH,'//ul[@class=\"lrv-a-unstyle-list lrv-u-flex lrv-u-height-100p lrv-u-flex-direction-column@mobile-max\"]')\n",
    "\n",
    "for name in names[:100]:\n",
    "\n",
    "    weeks.append(name.text.split('\\n')[4])\n",
    "weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e28b5143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Midnights</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Her Loss</td>\n",
       "      <td>Drake &amp; 21 Savage</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Un Verano Sin Ti</td>\n",
       "      <td>Bad Bunny</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's Only Me</td>\n",
       "      <td>Lil Baby</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dangerous: The Double Album</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Doo-Wops &amp; Hooligans</td>\n",
       "      <td>Bruno Mars</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Hot Rocks 1964-1971</td>\n",
       "      <td>The Rolling Stones</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The Original: Gene Autry Sings Rudolph The Red...</td>\n",
       "      <td>Gene Autry</td>\n",
       "      <td>-</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Future Nostalgia</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Hits 2</td>\n",
       "      <td>Prince</td>\n",
       "      <td>112</td>\n",
       "      <td>54</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Song Name         Artist Name  \\\n",
       "0                                           Midnights        Taylor Swift   \n",
       "1                                            Her Loss   Drake & 21 Savage   \n",
       "2                                    Un Verano Sin Ti           Bad Bunny   \n",
       "3                                        It's Only Me            Lil Baby   \n",
       "4                         Dangerous: The Double Album       Morgan Wallen   \n",
       "..                                                ...                 ...   \n",
       "95                               Doo-Wops & Hooligans          Bruno Mars   \n",
       "96                                Hot Rocks 1964-1971  The Rolling Stones   \n",
       "97  The Original: Gene Autry Sings Rudolph The Red...          Gene Autry   \n",
       "98                                   Future Nostalgia            Dua Lipa   \n",
       "99                                         The Hits 2              Prince   \n",
       "\n",
       "   Last Week Rank Peak Rank Weeks on board  \n",
       "0               1         1              5  \n",
       "1               2         1              3  \n",
       "2               3         1             29  \n",
       "3               4         1              6  \n",
       "4               6         1             98  \n",
       "..            ...       ...            ...  \n",
       "95            109         3            599  \n",
       "96             53         4            398  \n",
       "97              -        98              1  \n",
       "98             78         3            138  \n",
       "99            112        54             22  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe \n",
    "\n",
    "DF=pd.DataFrame({'Song Name':songname,'Artist Name':artist,'Last Week Rank':lastweek,'Peak Rank':peak,'Weeks on board':weeks})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d1e61be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a7ffbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b26e9c0",
   "metadata": {},
   "source": [
    "# 7. Scrape the details of Data science recruiters from naukri.com. Url = https://www.naukri.com/\n",
    "* You have to find the following details:\n",
    "    *  A) Name\n",
    "    *  B) Designation\n",
    "    *  C) Company\n",
    "    *  D) Skills they hire for\n",
    "    *  E) Location\n",
    "* Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c9480ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import re\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f3919473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')\n",
    "\n",
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0d5e1838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the link naukri.com\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2b59a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering data Scientist jobs\n",
    "search_job=driver.find_element(By.CLASS_NAME,'suggestor-input')\n",
    "search_job.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c979f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering location\n",
    "search_loc=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "01705aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click search button\n",
    "search_btn=driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d06701e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#URLs of the Each jobs titles\n",
    "product_URL = []\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "    for i in url:\n",
    "        product_URL.append(i.get_attribute('href'))\n",
    "len(product_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0155ad3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.naukri.com/job-listings-analystics-modeling-specialist-accenture-solutions-pvt-ltd-kolkata-mumbai-hyderabad-secunderabad-pune-chennai-bangalore-bengaluru-6-to-8-years-301122914065',\n",
       " 'https://www.naukri.com/job-listings-data-scientist-tata-business-hub-limited-new-delhi-hyderabad-secunderabad-pune-ahmedabad-chennai-bangalore-bengaluru-delhi-ncr-mumbai-all-areas-4-to-8-years-101122006226',\n",
       " 'https://www.naukri.com/job-listings-data-science-engineering-manager-paytm-new-delhi-bangalore-bengaluru-3-to-5-years-021222500074',\n",
       " 'https://www.naukri.com/job-listings-data-scientist-mindtree-limited-noida-kolkata-hyderabad-secunderabad-pune-chennai-bangalore-bengaluru-5-to-10-years-081122012068',\n",
       " 'https://www.naukri.com/job-listings-data-scientist-ii-smartpaddle-technology-pvt-ltd-bangalore-bengaluru-mumbai-all-areas-india-3-to-6-years-210722606804',\n",
       " 'https://www.naukri.com/job-listings-python-programming-language-data-science-practitioner-accenture-solutions-pvt-ltd-bangalore-bengaluru-4-to-6-years-301122915332',\n",
       " 'https://www.naukri.com/job-listings-acn-applied-intelligence-c4di-sustainability-09-accenture-solutions-pvt-ltd-bangalore-bengaluru-4-to-6-years-301122913894',\n",
       " 'https://www.naukri.com/job-listings-data-scientist-supply-chain-cargill-india-pvt-ltd-bangalore-bengaluru-3-to-5-years-291122915490',\n",
       " 'https://www.naukri.com/job-listings-data-science-lead-forecasting-cargill-india-pvt-ltd-bangalore-bengaluru-5-to-7-years-291122915436',\n",
       " 'https://www.naukri.com/job-listings-senior-data-scientist-cargill-india-pvt-ltd-bangalore-bengaluru-5-to-10-years-291122915242',\n",
       " 'https://www.naukri.com/job-listings-data-scientist-deutsche-bank-bangalore-bengaluru-10-to-15-years-281122001753',\n",
       " 'https://www.naukri.com/job-listings-lead-data-scientist-caterpillar-india-private-ltd-bangalore-bengaluru-6-to-9-years-011222501078',\n",
       " 'https://www.naukri.com/job-listings-lead-data-scientist-lowe-s-services-india-private-limited-bangalore-bengaluru-9-to-14-years-011222008903',\n",
       " 'https://www.naukri.com/job-listings-artificial-intelligence-computer-vision-engineer-machine-learning-vicara-kolkata-mumbai-hyderabad-secunderabad-pune-chennai-ahmedabad-delhi-ncr-bangalore-bengaluru-1-to-3-years-011222912067',\n",
       " 'https://www.naukri.com/job-listings-junior-data-scientist-hitachi-ltd-bangalore-bengaluru-1-to-4-years-241122501874',\n",
       " 'https://www.naukri.com/job-listings-data-scientist-ibm-india-pvt-limited-bangalore-bengaluru-8-to-12-years-221122905324',\n",
       " 'https://www.naukri.com/job-listings-senior-data-engineer-boston-consulting-group-bangalore-bengaluru-2-to-5-years-251122502753',\n",
       " 'https://www.naukri.com/job-listings-senior-data-engineer-boston-consulting-group-bangalore-bengaluru-4-to-6-years-251122502601',\n",
       " 'https://www.naukri.com/job-listings-data-engineer-boston-consulting-group-bangalore-bengaluru-2-to-4-years-251122502565',\n",
       " 'https://www.naukri.com/job-listings-sr-data-scientist-accolite-software-india-pvt-ltd-hyderabad-secunderabad-bangalore-bengaluru-6-to-8-years-181022502086']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a615f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping Job titles name\n",
    "name=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/div[1]/main/div[2]/div[2]/section[1]/div[1]/div[1]/header/h1')\n",
    "        name.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        name.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e79bfd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b14f746f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Analystics & Modeling Specialist',\n",
       " 'Data Scientist',\n",
       " 'Data Science - Engineering Manager',\n",
       " '-',\n",
       " 'Data Scientist - II',\n",
       " 'Python Programming Language Data Science Practitioner',\n",
       " 'ACN - Applied Intelligence - C4DI - Sustainability - 09',\n",
       " 'Data Scientist - Supply Chain',\n",
       " 'Data Science Lead - Forecasting',\n",
       " 'Senior Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Lead Data Scientist',\n",
       " 'Lead Data Scientist',\n",
       " 'Artificial Intelligence/Computer Vision Engineer - Machine Learning',\n",
       " 'Junior Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Senior Data Engineer',\n",
       " 'Senior Data Engineer',\n",
       " 'Data Engineer',\n",
       " 'Sr. Data Scientist']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1d49024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping company names\n",
    "comp=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/div[1]/main/div[2]/div[2]/section[1]/div[1]/div[1]/div/a[1]')\n",
    "        comp.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        comp.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6f3c32b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7dcb998e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accenture',\n",
       " 'Tata Nexarc',\n",
       " 'Paytm',\n",
       " '-',\n",
       " 'Bizongo',\n",
       " 'Accenture',\n",
       " 'Accenture',\n",
       " 'Cargill',\n",
       " 'Cargill',\n",
       " 'Cargill',\n",
       " 'Deutsche Bank',\n",
       " 'Caterpillar Inc',\n",
       " \"Lowe's\",\n",
       " 'Vicara',\n",
       " 'Hitachi Ltd.',\n",
       " 'IBM',\n",
       " 'Boston Consulting Group',\n",
       " 'Boston Consulting Group',\n",
       " 'Boston Consulting Group',\n",
       " 'Accolite Software India Pvt Ltd']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5efd36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping skill required for job\n",
    "skill=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/div[1]/main/div[2]/div[2]/section[2]/div[4]/div[2]')\n",
    "        skill.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        skill.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "27905de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "62fe068e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BfsiConsultingMachine learningOpen sourcePython',\n",
       " 'data miningMachine LearningSQLPython',\n",
       " 'Computer scienceData analysisMachine learningEngineering ManagerRisk managementData miningBusiness solutionsMonitoringSQLPython',\n",
       " '-',\n",
       " 'Business AnalyticsRisk AnalyticsArtificial IntelligenceCredit RiskData AnalyticsMachine LearningPython',\n",
       " 'ConsultingMachine learningJavascriptData processingPython',\n",
       " 'Business processData managementConsultingOracleBusiness operations',\n",
       " 'Supply Chain',\n",
       " 'Data Science',\n",
       " 'data science',\n",
       " 'Six Sigma Black BeltData AnalyticsSQL',\n",
       " 'deep learningAutomationData analysisMachine learningRegression analysisMATLABAnalyticsSix sigmaSQLPython',\n",
       " 'optimizationpricingretail',\n",
       " 'Python',\n",
       " 'Computer visiondeep learningImage processingNeural networksArtificial IntelligenceMachine learningSignal processingManager TechnologyFinancial servicesPython',\n",
       " 'ConsultingPredictive modelingSPSSForecastingAnalytics',\n",
       " 'advanced analyticssparkAnalyticalConsultingMachine learningPackagingbig dataTechnical supportMonitoring',\n",
       " 'advanced analyticsData analysisTechnology managementAnalyticalConsultingPackagingbig dataMonitoringTechnical support',\n",
       " 'advanced analyticsData analysisAnalyticalConsultingPackagingData analyticsSubject matter expertisebig dataMonitoringTechnical support',\n",
       " 'Machine learningbusiness rulesData analyticsdata visualizationResearchSQLPython']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a6079718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping Job description\n",
    "desc=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/div[1]/main/div[2]/div[2]/section[2]/div[1]')\n",
    "        desc.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        desc.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1fbf97a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ee269ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This role will be a part of Survey Solutions and Analytics team. The Analytics and Modeling Specialist will be working on developing advanced statistical models, machine learning methods and high-end solutions based on various datasets, helping Accenture improve various business outcome indicators.\\n\\nKEY RESPONSIBILITIES (BULLETS)\\nConceptualize, develop, test and implement various advanced Statistical models on business data.\\nIntegrate the outcomes from various analytics techniques to provide insights beyond the obvious, which helps create value for Accenture and its clients.\\nLeverage a combination of multiple approaches for project execution, including internal assets, third-party and open source solutions to create unique methods of delivery.\\nCollaborate with stakeholders, subject matter experts, client teams and other functional teams globally to understand business problems and conceptualize, design and execute solutions using various data sources and advanced analytics approach.',\n",
       " 'Roles and Responsibilities\\n\\nIdentify valuable data sources and automate collection processes\\nUndertake preprocessing of structured and unstructured data\\nAnalyze large amounts of information to discover trends and patterns\\nBuild predictive models and machine-learning algorithms\\nPresent information using data visualization techniques\\nPropose solutions and strategies to business challenges\\nCollaborate with engineering and product development teams\\nProven experience as a Data Scientist\\nExperience in data mining\\nUnderstanding of machine-learning\\nKnowledge of R, SQL and Python\\nExperience using business intelligence tools (e.g. Tableau) and data frameworks (e.g. Hadoop)\\nAnalytical mind and business acumen\\n\\nDesired Candidate Profile\\n\\nProven experience as a Data Scientist\\nExperience in data mining\\nUnderstanding of machine-learning\\nKnowledge of R, SQL and Python\\nExperience using business intelligence tools (e.g. Tableau) and data frameworks (e.g. Hadoop)\\nAnalytical mind and business acumen\\nRequired abilities / competencies:\\nProblem-solving aptitude\\nExcellent communication and presentation skills\\nBTech/BE in Computer Science, Engineering or relevant field\\n\\nExperience\\n\\n4+ years of experience in working in leading analytics companies or analytic teams of product companies/startups',\n",
       " \"- We are looking for passionate and skilled data scientists to help build the next gen lending platform for Paytm that will allow access to credit for more than 500M Indian consumers and over 21M merchants.\\n- The ideal candidate must be adept at using large data sets to find opportunities for different lending products and developing/validating robust models that can be deployed at scale.\\n- Must have strong experience using a variety of data mining/data analysis methods and effective application of different ML techniques, and a proven ability to drive business results with their data-based insights.\\n- They must be comfortable working with a wide range of stakeholders and functional teams.\\n\\nRequirements:\\n- Own model governance processes for a suite of lending data science models\\n- Refine monitoring and validation framework for machine learning models; conduct ongoing monitoring/validation of various scores and provide feedback to model owners\\n- Review new models from an overall model risk perspective, including methodology, performance, stability, explainability, etc\\n- Bring model risk management expertise to define approach within a dynamic environment\\n- Assess the effectiveness and accuracy of new data sources\\n- Collaborate with engineering and product development teams\\n- Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions.\\n\\nPreferred Qualification : Bachelor's/Master's Degree in Computer Science or equivalent\\n\\nSkills that will help you succeed in this role:\\n-Background in Statistics, Mathematics and Computer Science\\n- 10+ yearspractical experience with machine learning, feature engineering and model deployment\\n- Experience in credit risk domain, particularly in a model risk management role\\n- Strong programming skills in Python and Spark\\n- Knowledge and experience in statistical, data mining and ML techniques: GLM/Regression, Random Forest, Boosting, Neural Networks etc\\n- Worked with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, SQL, etc.\\n- Ability to explain and present analyses and machine learning concepts to a broad technical audience\\n- Willingness to drive projects to completion with minimal guidance.\",\n",
       " '-',\n",
       " 'Roles and Responsibilities\\nWhat you will work on:\\n- Build business analytics solutions to serve various product lines within Bizongo\\n- Work with varied data ranging from e-commerce data, financial data, user data, structured and unstructured documents, sensor data\\n- Work and drive cutting-edge new initiatives like AI and blockchain\\n\\nDesired Candidate Profile\\nYou’ll be a great fit for Bizongo if you:\\n- Have experience with working and deploying state-of-the-art analytics solutions aligned with business goals and designed to drive business impact\\n- Have experience working in Data Analytics & ML\\n- Can write high-quality, robust, maintainable and production-quality code\\n- Are comfortable with the open-end nature of business analytics problems and problems which don’t have perfect solutions\\n- Are familiar with deploying analytics solutions on the server-side and creating robust deployments to be used by end-user at scale\\n- Good coding skills in Python and familiarity with Java, the know-how of different ML & AI libraries in Python\\nExtra brownie points:\\n- Have worked on text mining & NLP\\n- Have worked with neural networks and similar AI implementations\\n- Have worked with financial data and financial risk evaluation\\n- Have a good command of DSA',\n",
       " 'Key Responsibilities : DM-369- Understanding of machine learning algorithms2 Experience in working with graph databases and document DBs3 Hands on experience in writing complex SQL queries4 Hands on experience in building machine learning solutions in Python6 Basic understanding of statistical distributions and probability theory7 Proficiency in general python programming8 Experience in data visualization using opensource libraries in python, javascript and dashboarding tools like superset or powerBI\\nTechnical Experience : Experience in DevOps practices for machine learning Experience with building KPIs for customer, product and market insights Experience in Spark based data processing and machine learning Experience in building and deploying solutions on AWS Hands on experience with sagemaker pipelines Hands on experience with airflow\\nProfessional Attributes : oDefines success in terms of the entire team oAction oriented and has exceptional attitude and work ethic oStrong Desire to learn and improve technical and interpersonal skills',\n",
       " 'We are looking for a highly motivated and passionate Data Science Consultant to join Accenture’s Global Centre for Data and Insights. The vision of the Centre is to combine the power of Machine (data & AI) with Human (Knowledge Experts) to solve some of the most complex problems in the industry. As a key member of the team, your role will focus on providing world class insights to leaders and stakeholders across different Functions in Banking.\\nWhat you will focus on?\\n? Conduct analysis to address critical business challenges faced by client teams working in different markets across the banking value chain to recommend the best data and insights powered solutions\\n? Gathering business requirements to create high level business solution framework covering aspects of banking customer journey, policy & regulatory requirements, data driven business process design & Technology Architecture\\n? Interface with client teams along with inputs from internal research and data insights teams to develop solutions for identified opportunities across banking LOBs\\n? Build analytical use cases in AI and Machine Learning across different banking functions including Operations, Treasury and Finance, Risk assessment, Compliance and Regulatory, Fraud, IT and Technology, Product design and Marketing, AI driven Customer Experience\\n? Ability to work with large data sets and present findings / insights to key stakeholders; Data management using databases like SQL, Oracle, S3 buckets and likes of the same\\n? Use data driven processes; generate actionable insights and recommend client strategy by interpreting ML model outputs, propensity scores to help prioritize recommendations for clients\\n? Build client deliverables with experience in storyboarding and client problem solving\\n? Enhance the business solutions with inputs from internal research and data insights teams for identified opportunities across banking LOBs',\n",
       " \"The Supply Chain Data Scientist will work collaboratively in multidisciplinary teams to define, develop, deploy and sustain complex supply chain digital solutions at scale. In this role, you will deploy predictive and optimization models for digital supply chain products that help deliver significant value to our customers, businesses and functions.\\n\\n\\nKey Accountabilities\\n• Design, implement, and deploy forecasting solutions on C3.ai the platform.\\n• Continuously seek out best practices and develop skills.\\n• Work on multidisciplinary teams of data engineers, software engineers, data scientists, and business subject matter experts to deliver projects on time and within budgets.\\n• Translate ambiguous business problems into project charters clearly identifying technical risks and project scope.\\n• Take ownership for the entire data science workflow including well defined project scoping, exploratory data analysis, model development, deployment and monitoring.\\n• Independently solve moderately complex issues with minimal supervision, while escalating more complex issues to appropriate staff.\\n• Other duties as assigned\\n\\n\\nQualifications\\nMINIMUM QUALIFICATIONS\\n• Bachelor’s degree in data science, computer science, math, engineering or related field or equivalent experience,\\n• Confirmed ability to present to non technical audiences\\n• Strong mathematical background (linear algebra, calculus, probability and statistics).\\n• Experience with JavaScript and prototyping languages such as Python and R.\\n• Applied Machine Learning experience (regression analysis, time series, probabilistic models, supervised, classification and unsupervised learning).\\n• Minimum of two years of related work experience, Other minimum qualifications may apply\\nPREFERRED QUALIFICATIONS\\n• Master's degree or PhD in data science, computer science, math, engineering or related field\\n• Strong data visualization skills\\n• Strong SQL skills\\n• Experience working with continuous integration and delivery (CI/CD) pipelines\\n• Professional experience applying predictive models for Supply Chain solutions.\\n• Experience with timeseries forecasting (such as ARIMA, Prophet, DeepAR, etc.)\\n• Experience in software development environment and code management or versioning\",\n",
       " \"Job Purpose and Impact\\nIntelligent Supply Chain Forecasting Lead will work collaboratively in multidisciplinary teams to define, develop, deploy and sustain complex supply chain digital solutions at scale. In this role, you will deploy predictive, prescriptive models at scale for digital supply chain products that help deliver significant value to our customers, businesses and functions.\\n\\n\\nKey Accountabilities\\nOwn and design statistical model, evaluate differentiating features and deploy forecasting solutions keeping eye on bringing business value for intelligent supply chain applications.\\nWork on multidisciplinary teams of product owners, data engineers, software engineers, data scientists, and business subject-matter experts to deploy complex large-scale projects on time.\\nTake ownership for the entire supply chain data science workflow including well defined, highly complex project scoping, exploratory data analysis, model development, deployment and monitoring.\\nProvide expert thought leadership in your space and work with limited direction, using additional research and interpretation to identify issues or problems. You may provide direction to supporting team members and be a strategic contributor.\\nContinuously seek out best practices and develop skills.\\nOther duties as assigned.\\n\\n\\nQualifications\\nMINIMUM QUALIFICATIONS\\n• Bachelor’s degree in Computer science, Statistics, Math OR related field OR Equivalent Experience.\\n• Applied Machine Learning experience (regression analysis, time series, probabilistic models, supervised classification and unsupervised learning).\\n• Strong mathematical background (linear algebra, calculus, probability and statistics). • Experience with JavaScript and prototyping languages such as Python and R.\\n• Minimum of Six years of related work experience.\\nPREFERRED QUALIFICATIONS\\n• Master's degree or PhD in Computer science / Engineering / Math or Statistics\\n• Professional experience applying predictive, prescriptive models for Supply Chain solutions.\\n• Experience with timeseries forecasting such as ARIMA, Prophet, DeepAR, etc.\",\n",
       " 'Job Purpose and Impact\\nIntelligent Supply Chain Senior Data Scientist will work collaboratively in multidisciplinary teams to define, develop and deploy complex supply chain digital solutions at scale. In this role, you will deploy predictive, prescriptive and optimization models at scale for digital supply chain products that help deliver significant value to our customers, businesses and functions.\\n\\n\\nKey Accountabilities\\nDesign, implement, and deploy forecasting, predictive solutions\\nContinuously seek out best practices and develop skills.\\nContribute to design, and implementation of new features, and deploy predictive models and solutions for intelligent supply chain products at scale.\\nWork and consult on multidisciplinary teams of data engineers, software engineers, data scientists, and business subject-matter experts to deliver highly complex large-scale projects on time.\\nTake ownership and work with limited supervision for the entire supply chain data science workflow including well defined, highly complex project scoping, exploratory data analysis, model development, deployment and monitoring.\\nProvide expert thought leadership in the area of business analytics and work with limited direction, using additional research and interpretation to identify issues or problems.\\nYou may provide direction to supporting team members and other duties as assigned\\n\\n\\nMINIMUM QUALIFICATIONS\\n• Master’s degree in Management, Computer science, Statistics, Data Science OR related field OR Equivalent Experience.\\n• Applied Machine Learning experience (regression analysis, time series, probabilistic models, supervised classification and unsupervised learning).\\n• Strong mathematical background (linear algebra, calculus, probability and statistics).\\n• Strong coding skills in Python (or similar object-oriented programming language).\\n• Minimum of 4 years of related work experience.\\nPREFERRED QUALIFICATIONS\\n• Five or more years’ experience in Supply chain\\n• PhD in Computer science, Artificial Intelligence, Optimization, Operational Research, or Math OR related field.\\n• Professional experience applying predictive models for Supply Chain solutions.\\n• Experience with timeseries forecasting such as ARIMA, Prophet, DeepAR, etc.',\n",
       " 'Job Title: Data Scientist / Data Analyst - AVP\\nLocation: Bangalore\\n\\nRole Description\\n\\nThe Data Scientist / Data Analyst has a proven track record of gaining insights from analysing company data, adept at using large data sets to find opportunities for process improvement and process optimisation and using models to test the effectiveness of different courses of action. With strong experience in using a variety of data mining/data analysis methods, using a variety of data tools, building, and implementing models, using/creating algorithms and creating/running simulations. With proven ability in driving business results with their data-based insights, comfortable working with a wide range of stakeholders and functional teams. With a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.\\n\\nWhat well offer you\\n\\nAs part of our flexible scheme, here are just some of the benefits that youll enjoy\\nBest in class leave policy\\nGender neutral parental leaves\\n100% reimbursement under child care assistance benefit (gender neutral)\\nFlexible working arrangements\\nSponsorship for Industry relevant certifications and education\\nEmployee Assistance Program for you and your family members\\nComprehensive Hospitalization Insurance for you and your dependents\\nAccident and Term life Insurance\\nComplementary Health screening for 35 yrs. and above\\n\\nYour skills and experience\\n\\nStrong problem-solving skills with an emphasis on process analysis and improvement\\nExperience using statistical computer languages SQL (expert level), Python etc. to manipulate data and draw insights from large data sets.\\nLean Six Sigma black belt, experienced in working in a regulated transactional environment\\nCelonis analytical skills for process mining is desirable\\nExperience working with and creating data architectures.\\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.\\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experienced with applications of these techniques.\\nExcellent written and verbal communication skills for coordinating across teams.\\nA drive to learn and master new technologies and techniques.\\nExperienced in using SQL to assist in Data analysis\\nOverall work experience of 10-15 years',\n",
       " 'Work closely with business stakeholders to understand their goals determine how Data analytics can be used to achieve those goals.\\nShould be able to design data modelling processes, create algorithms develop predictive models to help analyze the data\\nDevelop prediction tools which can help Caterpillar customers to gain efficiency and plan the forecast better\\nDevelop innovative solutions which enables machine learning and prediction. Also , Work with Cross - functional team to come up with innovative solutions\\nAct as a Technical lead in the field of Analytics and Machine Learning.\\nRequired skills :\\nStrong knowledge in python, SQL\\nFully skilled in Visualization tools such as Power Bi , Tableau with ability to utilize it as Automation tool\\nShould be able to develop Data Analysis Algorithm\\nShould have experience in developing prediction formulas for non-linear data.\\nSelf - starter passionate for Data analysis\\nCan make use of statistical tools for data prediction. Regression analysis (Linear/Non-linear)\\nShould be able to present the data in a meaningful form\\nPassion for Innovation and Technology\\nSelf-Starter, Should be able to present Ideas to stakeholders\\nDesired Skill\\nGood understanding of Six Sigma tools\\nCan develop Matlab models on the deep learning toolbox\\nShould be able to translate Data requirement into analytics\\nPositive outlook, strong work ethic, willingness to learn and grow\\nExcellent communication skills',\n",
       " 'Roles and Responsibilities\\nThe primary purpose of this role is to provide advanced analytical capabilities to support data science initiatives. This position gains experience in various areas including, but not limited to: predictive modeling; personalization and recommendation algorithms; natural language processing and text mining; search recall, precision, ranking, and related problems; optimization and mathematical programming with applications in labor scheduling, inventory and capacity planning, network flows, and supply chain optimization.\\n\\nA Lead Data Scientist solves highly complex problems that do not yet have solutions in the research and span much of the Data Science portfolio. Therefore, the individual in this role must be an expert in his/her field and confident in interacting and proposing solutions to the business. This role influences the thought process and methodology for other team members.\\n\\nQualifications:\\nMinimum Qualifications\\n10+ years of experience in data science or advanced analytics in industry\\nData Science experience in Retail is mandatory\\nData Science experience in Pricing function in Retail will be a plus\\n5+ years of experience in data science or advanced analytics in industry\\nKnowledge of SQL and various statistical modeling or machine learning techniques\\nMandatory experience in Python, Pyspark, Spark.\\nProgramming experience (Python, Java, Scala, Rust, etc.)\\nExperience using multiple data systems and sources (such as Hadoop, Spark, Aster, Teradata, etc.)',\n",
       " \"We are looking for an expert in machine learning to help us extract value from our data. You will lead all the processes from data collection, cleaning, and preprocessing, to training models and deploying them to production.\\n\\nThe ideal candidate will be passionate about artificial intelligence and stay up-to-date with the latest developments in the field.\\n\\nMain Responsibilities :\\n\\nYour responsibilities will include :\\n\\n- Working with the data science team to research, develop, evaluate and optimize various computer vision and deep learning models for different problems.\\n\\n- Explore and analyze unstructured data like images through image processing.\\n\\n- Take ownership to drive computer vision solutions and meet customer requirements.\\n\\n- Deploying developed computer vision models on edge devices after optimization to meet\\n\\ncustomer requirements and maintain them to later improve to address additional customer requirements in future.\\n\\nKey Requirements :\\n\\n- MSc or PhD in Computer Science, Data Science, Machine Learning or in related fields preferred but candidates with Bachelor's degree in computer science are also welcome to apply provided they have a strong technical knowledge and experience in computer vision.\\n\\n- Understanding about depth and breadth of computer vision and deep learning algorithms.\\n\\n- At least 2 years of experience in computer vision andor deep learning for object detection and tracking along with semantic or instance segmentation either in academic or industrial domain.\\n\\n- Experience with any machinedeep learning frameworks like Tensorflow, Keras, Scikit-Learn and PyTorch.\\n\\n- Experience in training models through GPU computing using NVIDIA CUDA or on cloud.\\n\\n- Ability to transform research articles into the working solutions to solve real-world problems.\\n\\n- Strong experience in using both basic and advanced image processing algorithms for feature engineering.\\n\\n- Proficiency in Python and related packages like numpy, scikit-image, PIL, opencv, matplotlib, seaborn, etc.\\n\\n- Excellent written and verbal communication skills for effectively communicating with the team and ability to presenting information to varied technical and non-technical audience.\\n\\n- Must be able to produce solutions independently in an organized manner and also be able to work in a team when required.\\n\\n- Must have good Object-Oriented Programing & logical analysis skills in Python.\\n\\nPreferred Skills (Not Mandatory) :\\n\\n- Strong foundation in data structures and algorithms in Python or C++\\n\\n- Advanced knowledge in performance, scalability, numerical accuracy and best practices for implementing various solutions.\\n\\n- Proficiency with edge computing principles and architecture preferably for NVIDIA Jetson devices and Raspberry Pi.\\n\\n- Experience in different model optimization techniques apart from hyperparameter tuning to reduce memory usage without hindering the performance for deploying on edge devices.\\n\\n- Proficiency with AWS or Azure cloud computing environments.\\n\\n- Exposure to IoT technology.\\n\\n- Experience in Agile Application Development and Scrum methodologies to develop efficient, maintainable, readable and production-ready pipelines.\\n\\n- Must have curiosity, eagerness and motivation to be involved in Data Science and Image Processing.\",\n",
       " 'Research and Develop Innovative Use Cases, Solutions and Quantitative Models in Video and Image Recognition and Signal Processing for Hitachi s cross-industry business (e.g., Energy, Industry, Mobility, Smart Life and Financial Services).\\nDesign, Implement and Demonstrate Proof-of-Concept and Working Proto-types for Hitachi and its clients.\\nProvide R&D support to Hitachi business units and group companies to productize research prototypes.\\nExplore emerging tools, techniques, and technologies, and work with academia for cutting-edge solutions.\\nCollaborate with cross-functional teams and eco-system partners for mutual business benefit.\\nGenerate eminence via patents, publications, thought leadership, invited speakerships and conspicuous presence in forums of repute.\\nAdd value to self through continuous learning and knowledge acquisition.\\nGive back learnings to colleagues and communities.\\nMentor colleagues for growth and success.\\nMandatory Requirements:\\nAcademic Qualification:\\nBachelor s degree with STEM background (Science, Technology, Engineering and Management) with strong quantitative flavour.\\nStrong Fundamentals:\\nThe candidate is required to build quantitative models from first principles and hence need to have excellent understanding of basics in mathematics and statistics (e.g., differential equations, linear algebra, matrix, combinatorics, probability, Bayesian statistics, eigen vectors, Markov models, Fourier analysis).\\nCore Expertise:\\nThe candidate is expected to specialize in Video and Image Signal Processing. Solid understanding of Video and Image Recognition is a pre-requisite. It is good to have hands-on model implementation skills for any one of the below areas.\\n1. Image segmentation:\\nThresholding, edge detection and linking, region growing.\\nDeformable shapes, active contours, etc.\\nMorphology.\\n2. Image representation:\\nBasic descriptors: area, minor axis length, major axis length, normalized axis ratio, eccentricity, Fourier descriptors, shape numbers, etc.\\nBinary descriptors: ORB, BRISK, LBP, etc.\\nAdvanced descriptors: HoG, SIFT/SURF, GLOH, etc.\\n3. Object detection/classification (using classical image processing & Deep Learning)\\n4. Image/background modeling, change/anomaly detection using image processing.\\n5. Technology stack (e.g., OpenCV, Pillow, torchvision, TensorFlow, Caffe, Python/Keras).\\nEmerging Trends - Artificial Intelligence and Machine Learning:\\nGood Understanding of working principles of neural networks and underlying algorithms (e.g., convolutional-CNN), recurrent-RNN-LSTM-GRU, Generative Adversarial Network (GAN), back-propagation, loss function, gradient descent)\\nData Wrangling and Model Lifecycle Maintenance:\\nExperience in data cleaning, ETL, pipeline building and model-maintenance, using commonly used methodology (e.g., Airflow, MLflow)\\nCommunication : Ability to articulate key messages concisely and precisely\\nCollaboration: Excellent interpersonal and teaming skills\\nAdvanced Qualification: PhD in any Quantitative Discipline',\n",
       " 'As a Data Scientist at IBM, you will help transform our clients data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.\\n\\nYour Role and Responsibilities\\nAs a Data Scientist at IBM, you will help transform our clients data into tangible business value by analyzing information, communicating outcomes and collaborating on product development.\\nWork with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.\\nResponsible for successful delivery of data science solutions and services in a client consulting environment; experience in predictive modeling, optimization, and/or machine learning analytics techniques using tools/programming languages including SPSS, SAS, R, Python, Spark, etc. within a Hadoop-enabled environment.\\nDefine key business problems to be solved; formulate mathematical approaches and gather data to solve those problems, develop, analyze/draw conclusions, test solutions and present to client. Enhance performance by applying advanced mathematical modeling, data analytics, optimization, and machine learning techniques.\\nRequired Technical and Professional Expertise\\nMinimum of 8 years of related experience required\\nMust have hands on experience on multiple cloud-based solutions like Red HAT, Azure, AWS and IBM Cloud for data storage, database connections and using technical architecture for end to end analytics\\nExpertise in Deep data and analytics\\nTime series analysis and forecasting\\nStatistical modelling skills (significance testing, conditional probability)\\nLinear Programming/ Optimization Techniques\\nAgent based modeling and simulation\\nConversational Skills for ChatBots\\nKnowledge of Agent Development Frameworks (e.g. SPADE, JADE, etc),\\nKnowledge Graphs and Ontologies.\\nDemonstrated proficiency in multiple programming languages with a strong foundation in a statistical platform such as Python, Java, or MATLAB\\nSQL coding experience, Strong Python 3.5 skills.\\nExperience in using containerized workloads and services such as Kubernetes, Docker, etc.\\nData integration processes such as Kettle\\nExperience building AI models (ML, NLP, Neural Networks).\\nStrong business acumen to solve client problems by understanding, preparing, and analyzing data to predict emerging trends and provide recommendations to optimize business results.\\nPreferred Technical and Professional Expertise\\nProficient in English, Good collaboration skills for interacting with clients\\nAbility to manage and make decisions about competing priorities and resources.\\nAbility to delegate where appropriate.\\nMust be a strong team player/leader.\\nAbility to lead Data Science projects with multiple junior data analysts\\nStrong oral written and interpersonal skills for interacting and throughout all levels of the organization.\\nAbility to clearly communicate complex business problems and technical solutions.',\n",
       " \"You will collaborate with case teams to gather requirements, specify, design, develop, deliver and support analytic solutions serving client needs. You will provide technical support through deeper understanding of relevant data analytics solutions and processes to build high quality and efficient analytic solutions. You will work with global stakeholders such as Project/Case teams and clients and support by working in AP/EU/AM time zones during weekdays or weekends.\\n\\n\\nYOU'RE GOOD AT\\n\\nWorking with case (and proposal) teams\\nAcquiring deep expertise in at least one analytics topic & understanding of all analytics capabilities\\nDefining and explaining expected analytics outcome; defining approach selection\\nSupporting and enhancing original analysis and insights to case teams, typically owning the maintenance of all or part of an analytics module post implementation.\\nEstablishing credibility by thought partnering with case teams on analytics topics; drawing conclusions on a range of external and internal issues related to their module\\nCommunicating analytical insights through sophisticated synthesis and packaging of results (including PPT slides and charts) with consultants, collects, synthesizes, analyses case team learning & inputs into new best practices and methodologies\\nAssuring proper sign off before uploading materials into internal repository for reference; sanitizing confidential client content in marketing documents\\nAble to generate project work for self via connects\\n\\nTeam requirements:\\nGuides juniors on analytical methodologies and platforms, and helps in quality checks\\nContributes to team s content & IP development\\nImparts technical trainings to team members, consulting cohort and (If/when required) Client IT team.\\n\\nFunctional Skills:\\nSelecting and integrating any Big Data tools and frameworks required to provide requested capabilities\\nEnhancing/optimizing and maintaining ETL process(s) across on-premise and cloud architectures\\nMonitoring performance and advising any necessary infrastructure changes\\n\\nCommunicating with confidence and ease:\\nYou will be a clear and confident communicator, able to deliver messages in a concise manner with strong and effective written and verbal communication.\\n\\nThinking Analytically:\\nYou should be strong in analytical solutioning with hands on experience in advanced analytics delivery, through the entire life cycle of analytics. Strong analytics skills with the ability to develop and codify knowledge and provide analytical advice where required.\\n\\n\\nYOU BRING (EXPERIENCE & QUALIFICATIONS)\\n\\nBachelor's / Master's degree in computer science engineering/technology\\nAt least 5 years within relevant domain of Data Engineering across industries and work experience providing analytics solutions in a commercial setting\\nConsulting experience will be considered a plus\\nProficient understanding of distributed computing principles\\nManagement of Spark clusters, with all included services - various implementations of Spark preferred\\nAbility to solve ongoing issues with operating the cluster and optimize for efficiency\\nUnderstanding of prevalent cloud ecosystems and its associated services AWS, Azure, Google Cloud, IBM Cloud. Expertise in at least one.\\nExperience with building stream-processing systems, using solutions such as Storm or Spark-Streaming\\nGood knowledge of Big Data querying tools, such as Pig, Hive, and Impala\\nExperience with integration of data from multiple data sources\\nExperience with NoSQL databases, such as HBase, Cassandra, MongoDB\\nKnowledge of various ETL techniques and frameworks, such as Flume\\nExperience with various messaging systems, such as Kafka or RabbitMQ\\nExperience with Big Data ML toolkits, such as Mahout, SparkML, or H2O\\nGood understanding of Lambda Architecture, along with its advantages and drawbacks\\n\\n\\nYOU'LL WORK WITH\\n\\nYou will work with the case team and/or client technical and border Gamma Team.\\n\\nADDITIONAL INFORMATION\\n\\nBCG GAMMA combines innovative skills in computer science, artificial intelligence, statistics, and machine learning with deep industry expertise. The BCG GAMMA R team is comprised of data engineers, data scientists and business consultants who specialize in the use of advanced analytics to get high-impact business results. Our teams own the full analytics value-chain end to end: framing the business problem, building the data, designing innovative algorithms, creating scale through designing tools and apps, and training colleagues and clients in new solutions.\\n   Here at BCG GAMMA R, you ll have the chance to work with clients in every BCG region and every industry area. We are also a core member of a rapidly growing Digital enterprise at BCG - a constellation of teams focused on driving practical results for BCG clients by applying leading edge analytics approaches, data, and technology\",\n",
       " \"As a part of BCG s GAMMA team, you will work closely with consulting teams on a diverse range of advanced analytics topics. You will have the opportunity to leverage analytical methodologies to deliver value to BCG's Consulting (case) teams and Practice Areas (domain) through providing analytics subject matter expertise, and accelerated execution support.\\nYou will collaborate with case teams to gather requirements, specify, design, develop, deliver, and support analytic solutions serving client needs. You will provide technical support through deeper understanding of relevant data analytics solutions and processes to build high quality and efficient analytic solutions.\\n\\n\\nYOU'RE GOOD AT\\n\\nDelivering original analysis and insights to case teams, typically owning all or part of an analytics module whilst integrating with a case team\\nBuilding data systems and end to end data pipelines for Ingesting structured and semi-structured data from different sources, building data model, and running complex data analysis.\\nUse tools provided by the analytics platform and write production-level code, adhering to and promoting best practices in the Data Engineering discipline of Infrastructure Engineering & Cloud Operations\\nBuilding CI/CD pipelines, orchestration, and containerization tools.\\nDeveloping broad expertise in at least one Cloud platform like AWS/GCP/Azure.\\nCommunicating analytical insights through sophisticated synthesis and packaging of results (including PPT slides and charts) with consultants, collecting, synthesizing, analysing case team learning & inputs into new best practices and methodologies\\nEnsuring proper sign off before uploading materials into internal repository for reference; sanitizing confidential client content in marketing documents\\n\\nWorking with case (and proposal) teams\\nHelp streamline our data science workflows, adding value to our product offerings and building out the customer lifecycle and retention models\\nSelecting and integrating any Big Data tools and frameworks required to provide requested capabilities\\nImplementing ETL process(s) across on-premises and cloud architectures\\nMonitoring performance and advising any necessary infrastructure changes\\nFunctional Skills:\\nCommunicating with confidence and ease\\nYou will be a clear and confident communicator, able to deliver messages in a concise manner with strong and effective written and verbal communication\\nThinking Analytically\\nYou should be strong in analytical solutioning with hands on experience in advanced analytics delivery, through the entire life cycle of analytics. Strong analytics skills with the ability to develop and codify knowledge and provide analytical advice where required.\\n\\n\\nYOU BRING (EXPERIENCE & QUALIFICATIONS)\\n\\nYOU BRING\\nBachelor's / Master's degree in computer science engineering/technology\\nAt least 4-6 years within relevant domain of Data Engineering across industries and work experience providing analytics solutions in a commercial setting\\nConsulting experience will be considered a plus\\nProficient understanding of distributed computing principles including management of Spark clusters, with all included services - various implementations of Spark preferred\\nGood knowledge of Big Data querying and analysis tools, such as Hive, Snowflake and Databricks.\\nExpert of prevalent cloud ecosystems and its associated services AWS, Azure, Google Cloud, IBM Cloud. Expertise in at least one.\\nExtensive hands-on experience with Data engineering tasks like productizing data pipelines, building CI/CD pipeline, code orchestration using tools like Airflow, DevOps etc Good to have: -\\nExperience with NoSQL databases, such as HBase, Cassandra, MongoDB\\nKnowledge of various ETL techniques and frameworks, such as Flume\\nExperience with various messaging systems, such as Kafka or RabbitMQ\\nExperience with Big Data ML toolkits, such as Mahout, SparkML, or H2O\\nGood understanding of Lambda Architecture, along with its advantages and drawbacks.\",\n",
       " \"As a part of BCG s GAMMA team you will work closely with consulting teams on a diverse range of advanced analytics topics. You will have the opportunity to leverage analytical methodologies to deliver value to BCG's Consulting (case) teams and Practice Areas (domain) through providing analytical and engineering subject matter expertise including but not limited to architecture designing, data Ingestion, data modelling and pipeline deployment.\\nYou will collaborate with case teams to gather requirements, specify, design, develop, deliver, and support analytic solutions serving client needs. You will provide technical support through deeper understanding of relevant data analytics solutions and processes to build high quality and efficient analytic solutions.\\n\\n\\nYOU'RE GOOD AT\\n\\nDelivering original analysis and insights to case teams, typically owning all or part of an analytics module whilst integrating with a case team.\\nBuilding data systems and end to end data pipelines for Ingesting structured and semi-structured data from different sources, building data model, and running complex data analysis.\\nBuilding data-intensive solutions that are highly available, scalable, reliable, secure, and cost-effective.\\nBasic understanding of CI/CD pipelines, orchestration, and containerization tools.\\nDeveloping broad expertise in at least one Cloud platform like AWS/GCP/Azure.\\nCommunicating analytical insights through sophisticated synthesis and packaging of results (including PPT slides and charts) with consultants, collecting, synthesizing, analysing case team learning & inputs into new best practices and methodologies\\nEnsuring proper sign off before uploading materials into internal repository for reference; sanitizing confidential client content in marketing documents\\n\\nWorking with case (and proposal) teams\\nHelp streamline our data science workflows, adding value to our product offerings and building out the customer lifecycle and retention models.\\nSelecting and integrating any Big Data tools and frameworks required to provide requested capabilities\\nImplementing ETL process(s) across on-premises and cloud architectures\\nMonitoring performance and advising any necessary infrastructure changes\\nFunctional Skills:\\nCommunicating with confidence and ease\\nYou will be a clear and confident communicator, able to deliver messages in a concise manner with strong and effective written and verbal communication\\nThinking Analytically\\nYou should be strong in analytical solutioning with hands on experience in advanced analytics delivery, through the entire life cycle of analytics. Strong analytics skills with the ability to develop and codify knowledge and provide analytical advice where required.\\n\\n\\nYOU BRING (EXPERIENCE & QUALIFICATIONS)\\n\\nBachelor's / Master's degree in computer science engineering/technology\\nAt least 2-4 years within relevant domain of Data Engineering across industries and work experience providing analytics solutions in a commercial setting\\nConsulting experience will be considered a plus\\nProficient understanding of distributed computing principles including management of Spark clusters, with all included services - various implementations of Spark preferred\\nAbility to solve ongoing issues with operating the cluster and optimize for efficiency\\nExpert of prevalent cloud ecosystems and its associated services AWS, Azure, Google Cloud, IBM Cloud. Expertise in at least one.\\nBasic hands-on experience with Data engineering tasks like productizing data pipelines, building CI/CD pipeline, code orchestration using tools like Airflow, DevOps etc\\nGood knowledge of Big Data querying and analysis tools, such as Hive, Snowflake and Databricks\\n\\nGood to have: -\\nExperience with integration of data from multiple data sources\\nExperience with NoSQL databases, such as HBase, Cassandra, MongoDB\\nKnowledge on web development technologies.\\nExperience with various messaging systems, such as Kafka or RabbitMQ\\nExperience with Big Data ML toolkits, such as Mahout, SparkML, or H2O\\nGood understanding of Lambda Architecture, along with its advantages and drawbacks.\\n\\n\\nYOU'LL WORK WITH\\n\\nOur data analytics and artifical intelligence professionals mix deep domain expertise with advanced analytical methods and techniques to develop innovative solutions that help our clients tackle their most pressing issues. We design algorithms and build complex models out of large amounts of data.\\n\\nADDITIONAL INFORMATION\\n\\nBCG GAMMA combines innovative skills in computer science, artificial intelligence, statistics, and machine learning with deep industry expertise. The BCG GAMMA team is comprised of data engineers, data scientists and business consultants who specialize in the use of advanced analytics to get high-impact business results.\\nOur teams own the full analytics value-chain end to end: framing the business problem, building the data, designing innovative algorithms, creating scale through designing tools and apps, and training colleagues and clients in new solutions. Here at BCG GAMMA, you ll have the chance to work with clients in every BCG region and every industry area. We are also a core member of a rapidly growing Digital enterprise at BCG - a constellation of teams focused on driving practical results for BCG clients by applying leading edge analytics approaches, data, and technology.\",\n",
       " 'Having meetings with team members regarding projects.\\nCollecting and interpreting data.\\nAutomating and integrating processes.\\nResearching solutions to overcome data analytics challenges.\\nDeveloping complex mathematical models that integrate business rules and requirements.\\nCreating machine learning models.\\nCommunicating and meeting with engineers, IT teams, and other interested parties.\\nSharing complex ideas verbally and visually in an understandable manner with non-technical stakeholders.\\nExperience in technologies like Python, Jupyter, Machine Learning Algorithms, SQL, Data Visualization, Statistical or Mathematical software']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3d668580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping Job locations\n",
    "loc=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/div[1]/main/div[2]/div[2]/section[1]/div[1]/div[2]/div[3]/span')\n",
    "        loc.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        loc.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e71eec3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6d517311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kolkata, Mumbai, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru',\n",
       " 'New Delhi, Hyderabad/Secunderabad, Pune, Ahmedabad, Chennai, Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)',\n",
       " 'New Delhi, Bangalore/Bengaluru',\n",
       " '-',\n",
       " 'India, Bangalore/Bengaluru, Mumbai (All Areas)',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Kolkata, Mumbai, Hyderabad/Secunderabad, Pune, Chennai, Ahmedabad, Delhi / NCR, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Hyderabad/Secunderabad, Bangalore/Bengaluru']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c4a5201a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Location</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>BfsiConsultingMachine learningOpen sourcePython</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "      <td>This role will be a part of Survey Solutions a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tata Nexarc</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>data miningMachine LearningSQLPython</td>\n",
       "      <td>New Delhi, Hyderabad/Secunderabad, Pune, Ahmed...</td>\n",
       "      <td>Roles and Responsibilities\\n\\nIdentify valuabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paytm</td>\n",
       "      <td>Data Science - Engineering Manager</td>\n",
       "      <td>Computer scienceData analysisMachine learningE...</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>- We are looking for passionate and skilled da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bizongo</td>\n",
       "      <td>Data Scientist - II</td>\n",
       "      <td>Business AnalyticsRisk AnalyticsArtificial Int...</td>\n",
       "      <td>India, Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "      <td>Roles and Responsibilities\\nWhat you will work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>Python Programming Language Data Science Pract...</td>\n",
       "      <td>ConsultingMachine learningJavascriptData proce...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Key Responsibilities : DM-369- Understanding o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>ACN - Applied Intelligence - C4DI - Sustainabi...</td>\n",
       "      <td>Business processData managementConsultingOracl...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>We are looking for a highly motivated and pass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cargill</td>\n",
       "      <td>Data Scientist - Supply Chain</td>\n",
       "      <td>Supply Chain</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>The Supply Chain Data Scientist will work coll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cargill</td>\n",
       "      <td>Data Science Lead - Forecasting</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job Purpose and Impact\\nIntelligent Supply Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cargill</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>data science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job Purpose and Impact\\nIntelligent Supply Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Six Sigma Black BeltData AnalyticsSQL</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Job Title: Data Scientist / Data Analyst - AVP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Caterpillar Inc</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>deep learningAutomationData analysisMachine le...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Work closely with business stakeholders to und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lowe's</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>optimizationpricingretail</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Roles and Responsibilities\\nThe primary purpos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Vicara</td>\n",
       "      <td>Artificial Intelligence/Computer Vision Engine...</td>\n",
       "      <td>Python</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "      <td>We are looking for an expert in machine learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hitachi Ltd.</td>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Computer visiondeep learningImage processingNe...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Research and Develop Innovative Use Cases, Sol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>IBM</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ConsultingPredictive modelingSPSSForecastingAn...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>As a Data Scientist at IBM, you will help tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>advanced analyticssparkAnalyticalConsultingMac...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>You will collaborate with case teams to gather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>advanced analyticsData analysisTechnology mana...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>As a part of BCG s GAMMA team, you will work c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>advanced analyticsData analysisAnalyticalConsu...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>As a part of BCG s GAMMA team you will work cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Accolite Software India Pvt Ltd</td>\n",
       "      <td>Sr. Data Scientist</td>\n",
       "      <td>Machine learningbusiness rulesData analyticsda...</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru</td>\n",
       "      <td>Having meetings with team members regarding pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Company  \\\n",
       "0                         Accenture   \n",
       "1                       Tata Nexarc   \n",
       "2                             Paytm   \n",
       "3                                 -   \n",
       "4                           Bizongo   \n",
       "5                         Accenture   \n",
       "6                         Accenture   \n",
       "7                           Cargill   \n",
       "8                           Cargill   \n",
       "9                           Cargill   \n",
       "10                    Deutsche Bank   \n",
       "11                  Caterpillar Inc   \n",
       "12                           Lowe's   \n",
       "13                           Vicara   \n",
       "14                     Hitachi Ltd.   \n",
       "15                              IBM   \n",
       "16          Boston Consulting Group   \n",
       "17          Boston Consulting Group   \n",
       "18          Boston Consulting Group   \n",
       "19  Accolite Software India Pvt Ltd   \n",
       "\n",
       "                                          Designation  \\\n",
       "0                    Analystics & Modeling Specialist   \n",
       "1                                      Data Scientist   \n",
       "2                  Data Science - Engineering Manager   \n",
       "3                                                   -   \n",
       "4                                 Data Scientist - II   \n",
       "5   Python Programming Language Data Science Pract...   \n",
       "6   ACN - Applied Intelligence - C4DI - Sustainabi...   \n",
       "7                       Data Scientist - Supply Chain   \n",
       "8                     Data Science Lead - Forecasting   \n",
       "9                               Senior Data Scientist   \n",
       "10                                     Data Scientist   \n",
       "11                                Lead Data Scientist   \n",
       "12                                Lead Data Scientist   \n",
       "13  Artificial Intelligence/Computer Vision Engine...   \n",
       "14                              Junior Data Scientist   \n",
       "15                                     Data Scientist   \n",
       "16                               Senior Data Engineer   \n",
       "17                               Senior Data Engineer   \n",
       "18                                      Data Engineer   \n",
       "19                                 Sr. Data Scientist   \n",
       "\n",
       "                                               Skills  \\\n",
       "0     BfsiConsultingMachine learningOpen sourcePython   \n",
       "1                data miningMachine LearningSQLPython   \n",
       "2   Computer scienceData analysisMachine learningE...   \n",
       "3                                                   -   \n",
       "4   Business AnalyticsRisk AnalyticsArtificial Int...   \n",
       "5   ConsultingMachine learningJavascriptData proce...   \n",
       "6   Business processData managementConsultingOracl...   \n",
       "7                                        Supply Chain   \n",
       "8                                        Data Science   \n",
       "9                                        data science   \n",
       "10              Six Sigma Black BeltData AnalyticsSQL   \n",
       "11  deep learningAutomationData analysisMachine le...   \n",
       "12                          optimizationpricingretail   \n",
       "13                                             Python   \n",
       "14  Computer visiondeep learningImage processingNe...   \n",
       "15  ConsultingPredictive modelingSPSSForecastingAn...   \n",
       "16  advanced analyticssparkAnalyticalConsultingMac...   \n",
       "17  advanced analyticsData analysisTechnology mana...   \n",
       "18  advanced analyticsData analysisAnalyticalConsu...   \n",
       "19  Machine learningbusiness rulesData analyticsda...   \n",
       "\n",
       "                                             Location  \\\n",
       "0   Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...   \n",
       "1   New Delhi, Hyderabad/Secunderabad, Pune, Ahmed...   \n",
       "2                      New Delhi, Bangalore/Bengaluru   \n",
       "3                                                   -   \n",
       "4      India, Bangalore/Bengaluru, Mumbai (All Areas)   \n",
       "5                                 Bangalore/Bengaluru   \n",
       "6                                 Bangalore/Bengaluru   \n",
       "7                                 Bangalore/Bengaluru   \n",
       "8                                 Bangalore/Bengaluru   \n",
       "9                                 Bangalore/Bengaluru   \n",
       "10                                Bangalore/Bengaluru   \n",
       "11                                Bangalore/Bengaluru   \n",
       "12                                Bangalore/Bengaluru   \n",
       "13  Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...   \n",
       "14                                Bangalore/Bengaluru   \n",
       "15                                Bangalore/Bengaluru   \n",
       "16                                Bangalore/Bengaluru   \n",
       "17                                Bangalore/Bengaluru   \n",
       "18                                Bangalore/Bengaluru   \n",
       "19        Hyderabad/Secunderabad, Bangalore/Bengaluru   \n",
       "\n",
       "                                          Description  \n",
       "0   This role will be a part of Survey Solutions a...  \n",
       "1   Roles and Responsibilities\\n\\nIdentify valuabl...  \n",
       "2   - We are looking for passionate and skilled da...  \n",
       "3                                                   -  \n",
       "4   Roles and Responsibilities\\nWhat you will work...  \n",
       "5   Key Responsibilities : DM-369- Understanding o...  \n",
       "6   We are looking for a highly motivated and pass...  \n",
       "7   The Supply Chain Data Scientist will work coll...  \n",
       "8   Job Purpose and Impact\\nIntelligent Supply Cha...  \n",
       "9   Job Purpose and Impact\\nIntelligent Supply Cha...  \n",
       "10  Job Title: Data Scientist / Data Analyst - AVP...  \n",
       "11  Work closely with business stakeholders to und...  \n",
       "12  Roles and Responsibilities\\nThe primary purpos...  \n",
       "13  We are looking for an expert in machine learni...  \n",
       "14  Research and Develop Innovative Use Cases, Sol...  \n",
       "15  As a Data Scientist at IBM, you will help tran...  \n",
       "16  You will collaborate with case teams to gather...  \n",
       "17  As a part of BCG s GAMMA team, you will work c...  \n",
       "18  As a part of BCG s GAMMA team you will work cl...  \n",
       "19  Having meetings with team members regarding pr...  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe \n",
    "\n",
    "DF=pd.DataFrame({'Company':comp,'Designation':name,'Skills':skill,'Location':loc,'Description':desc})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e89639fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568518ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bb6e283",
   "metadata": {},
   "source": [
    "# 8. Scrape the details of Highest selling novels.\n",
    "* Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey- compare/\n",
    "    *  You have to find the following details:\n",
    "        *  A) Book name\n",
    "        *  B) Author name\n",
    "        *  C) Volumes sold\n",
    "        *  D) Publisher\n",
    "        *  E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1ff5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import re\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307b906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')\n",
    "\n",
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c57748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the link\n",
    "url='https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b980d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '50',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '70',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '80',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '90',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '100']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping Rank\n",
    "rank=[]\n",
    "names = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]//tbody/tr/td[1]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    rank.append(name.text)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff286bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Da Vinci Code,The',\n",
       " 'Harry Potter and the Deathly Hallows',\n",
       " \"Harry Potter and the Philosopher's Stone\",\n",
       " 'Harry Potter and the Order of the Phoenix',\n",
       " 'Fifty Shades of Grey',\n",
       " 'Harry Potter and the Goblet of Fire',\n",
       " 'Harry Potter and the Chamber of Secrets',\n",
       " 'Harry Potter and the Prisoner of Azkaban',\n",
       " 'Angels and Demons',\n",
       " \"Harry Potter and the Half-blood Prince:Children's Edition\",\n",
       " 'Fifty Shades Darker',\n",
       " 'Twilight',\n",
       " 'Girl with the Dragon Tattoo,The:Millennium Trilogy',\n",
       " 'Fifty Shades Freed',\n",
       " 'Lost Symbol,The',\n",
       " 'New Moon',\n",
       " 'Deception Point',\n",
       " 'Eclipse',\n",
       " 'Lovely Bones,The',\n",
       " 'Curious Incident of the Dog in the Night-time,The',\n",
       " 'Digital Fortress',\n",
       " 'Short History of Nearly Everything,A',\n",
       " 'Girl Who Played with Fire,The:Millennium Trilogy',\n",
       " 'Breaking Dawn',\n",
       " 'Very Hungry Caterpillar,The:The Very Hungry Caterpillar',\n",
       " 'Gruffalo,The',\n",
       " \"Jamie's 30-Minute Meals\",\n",
       " 'Kite Runner,The',\n",
       " 'One Day',\n",
       " 'Thousand Splendid Suns,A',\n",
       " \"Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\",\n",
       " \"Time Traveler's Wife,The\",\n",
       " 'Atonement',\n",
       " \"Bridget Jones's Diary:A Novel\",\n",
       " 'World According to Clarkson,The',\n",
       " \"Captain Corelli's Mandolin\",\n",
       " 'Sound of Laughter,The',\n",
       " 'Life of Pi',\n",
       " 'Billy Connolly',\n",
       " 'Child Called It,A',\n",
       " \"Gruffalo's Child,The\",\n",
       " \"Angela's Ashes:A Memoir of a Childhood\",\n",
       " 'Birdsong',\n",
       " 'Northern Lights:His Dark Materials S.',\n",
       " 'Labyrinth',\n",
       " 'Harry Potter and the Half-blood Prince',\n",
       " 'Help,The',\n",
       " 'Man and Boy',\n",
       " 'Memoirs of a Geisha',\n",
       " \"No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\",\n",
       " 'Island,The',\n",
       " 'PS, I Love You',\n",
       " 'You are What You Eat:The Plan That Will Change Your Life',\n",
       " 'Shadow of the Wind,The',\n",
       " 'Tales of Beedle the Bard,The',\n",
       " 'Broker,The',\n",
       " \"Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\",\n",
       " 'Subtle Knife,The:His Dark Materials S.',\n",
       " 'Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation',\n",
       " \"Delia's How to Cook:(Bk.1)\",\n",
       " 'Chocolat',\n",
       " 'Boy in the Striped Pyjamas,The',\n",
       " \"My Sister's Keeper\",\n",
       " 'Amber Spyglass,The:His Dark Materials S.',\n",
       " 'To Kill a Mockingbird',\n",
       " 'Men are from Mars, Women are from Venus:A Practical Guide for Improvin',\n",
       " 'Dear Fatty',\n",
       " 'Short History of Tractors in Ukrainian,A',\n",
       " 'Hannibal',\n",
       " 'Lord of the Rings,The',\n",
       " 'Stupid White Men:...and Other Sorry Excuses for the State of the Natio',\n",
       " 'Interpretation of Murder,The',\n",
       " 'Sharon Osbourne Extreme:My Autobiography',\n",
       " 'Alchemist,The:A Fable About Following Your Dream',\n",
       " \"At My Mother's Knee ...:and Other Low Joints\",\n",
       " 'Notes from a Small Island',\n",
       " 'Return of the Naked Chef,The',\n",
       " 'Bridget Jones: The Edge of Reason',\n",
       " \"Jamie's Italy\",\n",
       " 'I Can Make You Thin',\n",
       " 'Down Under',\n",
       " 'Summons,The',\n",
       " 'Small Island',\n",
       " 'Nigella Express',\n",
       " 'Brick Lane',\n",
       " \"Memory Keeper's Daughter,The\",\n",
       " 'Room on the Broom',\n",
       " 'About a Boy',\n",
       " 'My Booky Wook',\n",
       " 'God Delusion,The',\n",
       " '\"Beano\" Annual,The',\n",
       " 'White Teeth',\n",
       " 'House at Riverton,The',\n",
       " 'Book Thief,The',\n",
       " 'Nights of Rain and Stars',\n",
       " 'Ghost,The',\n",
       " 'Happy Days with the Naked Chef',\n",
       " 'Hunger Games,The:Hunger Games Trilogy',\n",
       " \"Lost Boy,The:A Foster Child's Search for the Love of a Family\",\n",
       " \"Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping  Book Name\n",
    "book=[]\n",
    "names = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]//tbody/tr/td[2]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    book.append(name.text)\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff17255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brown, Dan',\n",
       " 'Rowling, J.K.',\n",
       " 'Rowling, J.K.',\n",
       " 'Rowling, J.K.',\n",
       " 'James, E. L.',\n",
       " 'Rowling, J.K.',\n",
       " 'Rowling, J.K.',\n",
       " 'Rowling, J.K.',\n",
       " 'Brown, Dan',\n",
       " 'Rowling, J.K.',\n",
       " 'James, E. L.',\n",
       " 'Meyer, Stephenie',\n",
       " 'Larsson, Stieg',\n",
       " 'James, E. L.',\n",
       " 'Brown, Dan',\n",
       " 'Meyer, Stephenie',\n",
       " 'Brown, Dan',\n",
       " 'Meyer, Stephenie',\n",
       " 'Sebold, Alice',\n",
       " 'Haddon, Mark',\n",
       " 'Brown, Dan',\n",
       " 'Bryson, Bill',\n",
       " 'Larsson, Stieg',\n",
       " 'Meyer, Stephenie',\n",
       " 'Carle, Eric',\n",
       " 'Donaldson, Julia',\n",
       " 'Oliver, Jamie',\n",
       " 'Hosseini, Khaled',\n",
       " 'Nicholls, David',\n",
       " 'Hosseini, Khaled',\n",
       " 'Larsson, Stieg',\n",
       " 'Niffenegger, Audrey',\n",
       " 'McEwan, Ian',\n",
       " 'Fielding, Helen',\n",
       " 'Clarkson, Jeremy',\n",
       " 'Bernieres, Louis de',\n",
       " 'Kay, Peter',\n",
       " 'Martel, Yann',\n",
       " 'Stephenson, Pamela',\n",
       " 'Pelzer, Dave',\n",
       " 'Donaldson, Julia',\n",
       " 'McCourt, Frank',\n",
       " 'Faulks, Sebastian',\n",
       " 'Pullman, Philip',\n",
       " 'Mosse, Kate',\n",
       " 'Rowling, J.K.',\n",
       " 'Stockett, Kathryn',\n",
       " 'Parsons, Tony',\n",
       " 'Golden, Arthur',\n",
       " 'McCall Smith, Alexander',\n",
       " 'Hislop, Victoria',\n",
       " 'Ahern, Cecelia',\n",
       " 'McKeith, Gillian',\n",
       " 'Zafon, Carlos Ruiz',\n",
       " 'Rowling, J.K.',\n",
       " 'Grisham, John',\n",
       " 'Atkins, Robert C.',\n",
       " 'Pullman, Philip',\n",
       " 'Truss, Lynne',\n",
       " 'Smith, Delia',\n",
       " 'Harris, Joanne',\n",
       " 'Boyne, John',\n",
       " 'Picoult, Jodi',\n",
       " 'Pullman, Philip',\n",
       " 'Lee, Harper',\n",
       " 'Gray, John',\n",
       " 'French, Dawn',\n",
       " 'Lewycka, Marina',\n",
       " 'Harris, Thomas',\n",
       " 'Tolkien, J. R. R.',\n",
       " 'Moore, Michael',\n",
       " 'Rubenfeld, Jed',\n",
       " 'Osbourne, Sharon',\n",
       " 'Coelho, Paulo',\n",
       " \"O'Grady, Paul\",\n",
       " 'Bryson, Bill',\n",
       " 'Oliver, Jamie',\n",
       " 'Fielding, Helen',\n",
       " 'Oliver, Jamie',\n",
       " 'McKenna, Paul',\n",
       " 'Bryson, Bill',\n",
       " 'Grisham, John',\n",
       " 'Levy, Andrea',\n",
       " 'Lawson, Nigella',\n",
       " 'Ali, Monica',\n",
       " 'Edwards, Kim',\n",
       " 'Donaldson, Julia',\n",
       " 'Hornby, Nick',\n",
       " 'Brand, Russell',\n",
       " 'Dawkins, Richard',\n",
       " '0',\n",
       " 'Smith, Zadie',\n",
       " 'Morton, Kate',\n",
       " 'Zusak, Markus',\n",
       " 'Binchy, Maeve',\n",
       " 'Harris, Robert',\n",
       " 'Oliver, Jamie',\n",
       " 'Collins, Suzanne',\n",
       " 'Pelzer, Dave',\n",
       " 'Oliver, Jamie']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping Author Name\n",
    "Author=[]\n",
    "names = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]//tbody/tr/td[3]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    Author.append(name.text)\n",
    "Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0aa247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5,094,805',\n",
       " '4,475,152',\n",
       " '4,200,654',\n",
       " '4,179,479',\n",
       " '3,758,936',\n",
       " '3,583,215',\n",
       " '3,484,047',\n",
       " '3,377,906',\n",
       " '3,193,946',\n",
       " '2,950,264',\n",
       " '2,479,784',\n",
       " '2,315,405',\n",
       " '2,233,570',\n",
       " '2,193,928',\n",
       " '2,183,031',\n",
       " '2,152,737',\n",
       " '2,062,145',\n",
       " '2,052,876',\n",
       " '2,005,598',\n",
       " '1,979,552',\n",
       " '1,928,900',\n",
       " '1,852,919',\n",
       " '1,814,784',\n",
       " '1,787,118',\n",
       " '1,783,535',\n",
       " '1,781,269',\n",
       " '1,743,266',\n",
       " '1,629,119',\n",
       " '1,616,068',\n",
       " '1,583,992',\n",
       " '1,555,135',\n",
       " '1,546,886',\n",
       " '1,539,428',\n",
       " '1,508,205',\n",
       " '1,489,403',\n",
       " '1,352,318',\n",
       " '1,310,207',\n",
       " '1,310,176',\n",
       " '1,231,957',\n",
       " '1,217,712',\n",
       " '1,208,711',\n",
       " '1,204,058',\n",
       " '1,184,967',\n",
       " '1,181,503',\n",
       " '1,181,093',\n",
       " '1,153,181',\n",
       " '1,132,336',\n",
       " '1,130,802',\n",
       " '1,126,337',\n",
       " '1,115,549',\n",
       " '1,108,328',\n",
       " '1,107,379',\n",
       " '1,104,403',\n",
       " '1,092,349',\n",
       " '1,090,847',\n",
       " '1,087,262',\n",
       " '1,054,196',\n",
       " '1,037,160',\n",
       " '1,023,688',\n",
       " '1,015,956',\n",
       " '1,009,873',\n",
       " '1,004,414',\n",
       " '1,003,780',\n",
       " '1,002,314',\n",
       " '998,213',\n",
       " '992,846',\n",
       " '986,753',\n",
       " '986,115',\n",
       " '970,509',\n",
       " '967,466',\n",
       " '963,353',\n",
       " '962,515',\n",
       " '959,496',\n",
       " '956,114',\n",
       " '945,640',\n",
       " '931,312',\n",
       " '925,425',\n",
       " '924,695',\n",
       " '906,968',\n",
       " '905,086',\n",
       " '890,847',\n",
       " '869,671',\n",
       " '869,659',\n",
       " '862,602',\n",
       " '856,540',\n",
       " '845,858',\n",
       " '842,535',\n",
       " '828,215',\n",
       " '820,563',\n",
       " '816,907',\n",
       " '816,585',\n",
       " '815,586',\n",
       " '814,370',\n",
       " '809,641',\n",
       " '808,900',\n",
       " '807,311',\n",
       " '794,201',\n",
       " '792,187',\n",
       " '791,507',\n",
       " '791,095']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping Volume sold\n",
    "volume=[]\n",
    "names = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]//tbody/tr/td[4]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    volume.append(name.text)\n",
    "volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eecbdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transworld',\n",
       " 'Bloomsbury',\n",
       " 'Bloomsbury',\n",
       " 'Bloomsbury',\n",
       " 'Random House',\n",
       " 'Bloomsbury',\n",
       " 'Bloomsbury',\n",
       " 'Bloomsbury',\n",
       " 'Transworld',\n",
       " 'Bloomsbury',\n",
       " 'Random House',\n",
       " 'Little, Brown Book',\n",
       " 'Quercus',\n",
       " 'Random House',\n",
       " 'Transworld',\n",
       " 'Little, Brown Book',\n",
       " 'Transworld',\n",
       " 'Little, Brown Book',\n",
       " 'Pan Macmillan',\n",
       " 'Random House',\n",
       " 'Transworld',\n",
       " 'Transworld',\n",
       " 'Quercus',\n",
       " 'Little, Brown Book',\n",
       " 'Penguin',\n",
       " 'Pan Macmillan',\n",
       " 'Penguin',\n",
       " 'Bloomsbury',\n",
       " 'Hodder & Stoughton',\n",
       " 'Bloomsbury',\n",
       " 'Quercus',\n",
       " 'Random House',\n",
       " 'Random House',\n",
       " 'Pan Macmillan',\n",
       " 'Penguin',\n",
       " 'Random House',\n",
       " 'Random House',\n",
       " 'Canongate',\n",
       " 'HarperCollins',\n",
       " 'Orion',\n",
       " 'Pan Macmillan',\n",
       " 'HarperCollins',\n",
       " 'Random House',\n",
       " 'Scholastic Ltd.',\n",
       " 'Orion',\n",
       " 'Bloomsbury',\n",
       " 'Penguin',\n",
       " 'HarperCollins',\n",
       " 'Random House',\n",
       " 'Little, Brown Book',\n",
       " 'Headline',\n",
       " 'HarperCollins',\n",
       " 'Penguin',\n",
       " 'Orion',\n",
       " 'Bloomsbury',\n",
       " 'Random House',\n",
       " 'Random House',\n",
       " 'Scholastic Ltd.',\n",
       " 'Profile Books Group',\n",
       " 'Random House',\n",
       " 'Transworld',\n",
       " 'Random House Childrens Books G',\n",
       " 'Hodder & Stoughton',\n",
       " 'Scholastic Ltd.',\n",
       " 'Random House',\n",
       " 'HarperCollins',\n",
       " 'Random House',\n",
       " 'Penguin',\n",
       " 'Random House',\n",
       " 'HarperCollins',\n",
       " 'Penguin',\n",
       " 'Headline',\n",
       " 'Little, Brown Book',\n",
       " 'HarperCollins',\n",
       " 'Transworld',\n",
       " 'Transworld',\n",
       " 'Penguin',\n",
       " 'Pan Macmillan',\n",
       " 'Penguin',\n",
       " 'Transworld',\n",
       " 'Transworld',\n",
       " 'Random House',\n",
       " 'Headline',\n",
       " 'Random House',\n",
       " 'Transworld',\n",
       " 'Penguin',\n",
       " 'Pan Macmillan',\n",
       " 'Penguin',\n",
       " 'Hodder & Stoughton',\n",
       " 'Transworld',\n",
       " 'D.C. Thomson',\n",
       " 'Penguin',\n",
       " 'Pan Macmillan',\n",
       " 'Transworld',\n",
       " 'Orion',\n",
       " 'Random House',\n",
       " 'Penguin',\n",
       " 'Scholastic Ltd.',\n",
       " 'Orion',\n",
       " 'Penguin']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping Publisher\n",
    "publisher=[]\n",
    "names = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]//tbody/tr/td[5]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    publisher.append(name.text)\n",
    "publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f88487e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Crime, Thriller & Adventure',\n",
       " \"Children's Fiction\",\n",
       " \"Children's Fiction\",\n",
       " \"Children's Fiction\",\n",
       " 'Romance & Sagas',\n",
       " \"Children's Fiction\",\n",
       " \"Children's Fiction\",\n",
       " \"Children's Fiction\",\n",
       " 'Crime, Thriller & Adventure',\n",
       " \"Children's Fiction\",\n",
       " 'Romance & Sagas',\n",
       " 'Young Adult Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Romance & Sagas',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Young Adult Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Young Adult Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Popular Science',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Young Adult Fiction',\n",
       " 'Picture Books',\n",
       " 'Picture Books',\n",
       " 'Food & Drink: General',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Humour: Collections & General',\n",
       " 'General & Literary Fiction',\n",
       " 'Autobiography: General',\n",
       " 'General & Literary Fiction',\n",
       " 'Biography: The Arts',\n",
       " 'Autobiography: General',\n",
       " 'Picture Books',\n",
       " 'Autobiography: General',\n",
       " 'General & Literary Fiction',\n",
       " 'Young Adult Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Science Fiction & Fantasy',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Fitness & Diet',\n",
       " 'General & Literary Fiction',\n",
       " \"Children's Fiction\",\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Fitness & Diet',\n",
       " 'Young Adult Fiction',\n",
       " 'Usage & Writing Guides',\n",
       " 'Food & Drink: General',\n",
       " 'General & Literary Fiction',\n",
       " 'Young Adult Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Young Adult Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Popular Culture & Media: General Interest',\n",
       " 'Autobiography: The Arts',\n",
       " 'General & Literary Fiction',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Science Fiction & Fantasy',\n",
       " 'Current Affairs & Issues',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'Autobiography: The Arts',\n",
       " 'General & Literary Fiction',\n",
       " 'Autobiography: The Arts',\n",
       " 'Travel Writing',\n",
       " 'Food & Drink: General',\n",
       " 'General & Literary Fiction',\n",
       " 'National & Regional Cuisine',\n",
       " 'Fitness & Diet',\n",
       " 'Travel Writing',\n",
       " 'Crime, Thriller & Adventure',\n",
       " 'General & Literary Fiction',\n",
       " 'Food & Drink: General',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Picture Books',\n",
       " 'General & Literary Fiction',\n",
       " 'Autobiography: The Arts',\n",
       " 'Popular Science',\n",
       " \"Children's Annuals\",\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'General & Literary Fiction',\n",
       " 'Food & Drink: General',\n",
       " 'Young Adult Fiction',\n",
       " 'Biography: General',\n",
       " 'Food & Drink: General']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrapping Genre\n",
    "genre=[]\n",
    "names = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]//tbody/tr/td[6]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    genre.append(name.text)\n",
    "genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8c1568e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Volume Sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                          Book Name       Author Name  \\\n",
       "0     1                                  Da Vinci Code,The        Brown, Dan   \n",
       "1     2               Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2     3           Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3     4          Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4     5                               Fifty Shades of Grey      James, E. L.   \n",
       "..  ...                                                ...               ...   \n",
       "95   96                                          Ghost,The    Harris, Robert   \n",
       "96   97                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97   98              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98   99  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  100  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volume Sold        Publisher                        Genre  \n",
       "0    5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1    4,475,152       Bloomsbury           Children's Fiction  \n",
       "2    4,200,654       Bloomsbury           Children's Fiction  \n",
       "3    4,179,479       Bloomsbury           Children's Fiction  \n",
       "4    3,758,936     Random House              Romance & Sagas  \n",
       "..         ...              ...                          ...  \n",
       "95     807,311     Random House   General & Literary Fiction  \n",
       "96     794,201          Penguin        Food & Drink: General  \n",
       "97     792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98     791,507            Orion           Biography: General  \n",
       "99     791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe \n",
    "\n",
    "DF=pd.DataFrame({'Rank':rank,'Book Name':book,'Author Name':Author,'Volume Sold':volume,'Publisher':publisher,'Genre':genre})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6542a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9697c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e02c9060",
   "metadata": {},
   "source": [
    "# 9. Scrape the details most watched tv series of all time from imdb.com. Url = https://www.imdb.com/list/ls095964455/\n",
    "* You have to find the following details:\n",
    "    *  A) Name\n",
    "    *  B) Year span\n",
    "    *  C) Genre\n",
    "    *  D) Run time\n",
    "    *  E) Ratings\n",
    "    *  F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "18c03fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import re\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ef97a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')\n",
    "\n",
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3ceaccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the link\n",
    "url='https://www.imdb.com/list/ls095964455/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5bf2f433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Game of Thrones',\n",
       " 'Stranger Things',\n",
       " 'The Walking Dead',\n",
       " '13 Reasons Why',\n",
       " 'The 100',\n",
       " 'Orange Is the New Black',\n",
       " 'Riverdale',\n",
       " \"Grey's Anatomy\",\n",
       " 'The Flash',\n",
       " 'Arrow',\n",
       " 'Money Heist',\n",
       " 'The Big Bang Theory',\n",
       " 'Black Mirror',\n",
       " 'Sherlock',\n",
       " 'Vikings',\n",
       " 'Pretty Little Liars',\n",
       " 'The Vampire Diaries',\n",
       " 'American Horror Story',\n",
       " 'Breaking Bad',\n",
       " 'Lucifer',\n",
       " 'Supernatural',\n",
       " 'Prison Break',\n",
       " 'How to Get Away with Murder',\n",
       " 'Teen Wolf',\n",
       " 'The Simpsons',\n",
       " 'Once Upon a Time',\n",
       " 'Narcos',\n",
       " 'Daredevil',\n",
       " 'Friends',\n",
       " 'How I Met Your Mother',\n",
       " 'Suits',\n",
       " 'Mr. Robot',\n",
       " 'The Originals',\n",
       " 'Supergirl',\n",
       " 'Gossip Girl',\n",
       " 'Sense8',\n",
       " 'Gotham',\n",
       " 'Westworld',\n",
       " 'Jessica Jones',\n",
       " 'Modern Family',\n",
       " 'Rick and Morty',\n",
       " 'Shadowhunters',\n",
       " 'The End of the F***ing World',\n",
       " 'House of Cards',\n",
       " 'Dark',\n",
       " 'Elite',\n",
       " 'Sex Education',\n",
       " 'Shameless',\n",
       " 'New Girl',\n",
       " 'Agents of S.H.I.E.L.D.',\n",
       " 'You',\n",
       " 'Dexter',\n",
       " 'Fear the Walking Dead',\n",
       " 'Family Guy',\n",
       " 'The Blacklist',\n",
       " 'Lost',\n",
       " 'Peaky Blinders',\n",
       " 'House',\n",
       " 'Quantico',\n",
       " 'Orphan Black',\n",
       " 'Homeland',\n",
       " 'Blindspot',\n",
       " \"DC's Legends of Tomorrow\",\n",
       " \"The Handmaid's Tale\",\n",
       " 'Chilling Adventures of Sabrina',\n",
       " 'The Good Doctor',\n",
       " 'Jane the Virgin',\n",
       " 'Glee',\n",
       " 'South Park',\n",
       " 'Brooklyn Nine-Nine',\n",
       " 'Under the Dome',\n",
       " 'The Umbrella Academy',\n",
       " 'True Detective',\n",
       " 'The OA',\n",
       " 'Desperate Housewives',\n",
       " 'Better Call Saul',\n",
       " 'Bates Motel',\n",
       " 'The Punisher',\n",
       " 'Atypical',\n",
       " 'Dynasty',\n",
       " 'This Is Us',\n",
       " 'The Good Place',\n",
       " 'Iron Fist',\n",
       " 'The Rain',\n",
       " 'Mindhunter',\n",
       " 'Revenge',\n",
       " 'Luke Cage',\n",
       " 'Scandal',\n",
       " 'The Defenders',\n",
       " 'Big Little Lies',\n",
       " 'Insatiable',\n",
       " 'The Mentalist',\n",
       " 'The Crown',\n",
       " 'Chernobyl',\n",
       " 'iZombie',\n",
       " 'Reign',\n",
       " 'A Series of Unfortunate Events',\n",
       " 'Criminal Minds',\n",
       " 'Scream: The TV Series',\n",
       " 'The Haunting of Hill House']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Series Names\n",
    "ser=[]\n",
    "names = driver.find_elements(By.XPATH,'//h3[@class=\"lister-item-header\"]/a')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    ser.append(name.text)\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fd5876ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2011–2019)',\n",
       " '(2016– )',\n",
       " '(2010–2022)',\n",
       " '(2017–2020)',\n",
       " '(2014–2020)',\n",
       " '(2013–2019)',\n",
       " '(2017– )',\n",
       " '(2005– )',\n",
       " '(2014–2023)',\n",
       " '(2012–2020)',\n",
       " '(2017–2021)',\n",
       " '(2007–2019)',\n",
       " '(2011–2019)',\n",
       " '(2010–2017)',\n",
       " '(2013–2020)',\n",
       " '(2010–2017)',\n",
       " '(2009–2017)',\n",
       " '(2011– )',\n",
       " '(2008–2013)',\n",
       " '(2016–2021)',\n",
       " '(2005–2020)',\n",
       " '(2005–2017)',\n",
       " '(2014–2020)',\n",
       " '(2011–2017)',\n",
       " '(1989– )',\n",
       " '(2011–2018)',\n",
       " '(2015–2017)',\n",
       " '(2015–2018)',\n",
       " '(1994–2004)',\n",
       " '(2005–2014)',\n",
       " '(2011–2019)',\n",
       " '(2015–2019)',\n",
       " '(2013–2018)',\n",
       " '(2015–2021)',\n",
       " '(2007–2012)',\n",
       " '(2015–2018)',\n",
       " '(2014–2019)',\n",
       " '(2016–2022)',\n",
       " '(2015–2019)',\n",
       " '(2009–2020)',\n",
       " '(2013– )',\n",
       " '(2016–2019)',\n",
       " '(2017–2019)',\n",
       " '(2013–2018)',\n",
       " '(2017–2020)',\n",
       " '(2018– )',\n",
       " '(2019– )',\n",
       " '(2011–2021)',\n",
       " '(2011–2018)',\n",
       " '(2013–2020)',\n",
       " '(2018– )',\n",
       " '(2006–2013)',\n",
       " '(2015– )',\n",
       " '(1999– )',\n",
       " '(2013– )',\n",
       " '(2004–2010)',\n",
       " '(2013–2022)',\n",
       " '(2004–2012)',\n",
       " '(2015–2018)',\n",
       " '(2013–2017)',\n",
       " '(2011–2020)',\n",
       " '(2015–2020)',\n",
       " '(2016–2022)',\n",
       " '(2017– )',\n",
       " '(2018–2020)',\n",
       " '(2017– )',\n",
       " '(2014–2019)',\n",
       " '(2009–2015)',\n",
       " '(1997– )',\n",
       " '(2013–2021)',\n",
       " '(2013–2015)',\n",
       " '(2019–2023)',\n",
       " '(2014–2019)',\n",
       " '(2016–2019)',\n",
       " '(2004–2012)',\n",
       " '(2015–2022)',\n",
       " '(2013–2017)',\n",
       " '(2017–2019)',\n",
       " '(2017–2021)',\n",
       " '(2017–2022)',\n",
       " '(2016–2022)',\n",
       " '(2016–2020)',\n",
       " '(2017–2018)',\n",
       " '(2018–2020)',\n",
       " '(2017–2019)',\n",
       " '(2011–2015)',\n",
       " '(2016–2018)',\n",
       " '(2012–2018)',\n",
       " '(2017)',\n",
       " '(2017–2019)',\n",
       " '(2018–2019)',\n",
       " '(2008–2015)',\n",
       " '(2016– )',\n",
       " '(2019)',\n",
       " '(2015–2019)',\n",
       " '(2013–2017)',\n",
       " '(2017–2019)',\n",
       " '(2005– )',\n",
       " '(2015–2019)',\n",
       " '(2018)']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Series Years\n",
    "year=[]\n",
    "names = driver.find_elements(By.XPATH,'//span[@class=\"lister-item-year text-muted unbold\"]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    year.append(name.text)\n",
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "41de95d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Action, Adventure, Drama',\n",
       " 'Drama, Fantasy, Horror',\n",
       " 'Drama, Horror, Thriller',\n",
       " 'Drama, Mystery, Thriller',\n",
       " 'Drama, Mystery, Sci-Fi',\n",
       " 'Comedy, Crime, Drama',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Drama, Romance',\n",
       " 'Action, Adventure, Drama',\n",
       " 'Action, Adventure, Crime',\n",
       " 'Action, Crime, Drama',\n",
       " 'Comedy, Romance',\n",
       " 'Drama, Mystery, Sci-Fi',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Action, Adventure, Drama',\n",
       " 'Drama, Mystery, Romance',\n",
       " 'Drama, Fantasy, Horror',\n",
       " 'Drama, Horror, Sci-Fi',\n",
       " 'Crime, Drama, Thriller',\n",
       " 'Crime, Drama, Fantasy',\n",
       " 'Drama, Fantasy, Horror',\n",
       " 'Action, Crime, Drama',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Action, Drama, Fantasy',\n",
       " 'Animation, Comedy',\n",
       " 'Adventure, Fantasy, Romance',\n",
       " 'Biography, Crime, Drama',\n",
       " 'Action, Crime, Drama',\n",
       " 'Comedy, Romance',\n",
       " 'Comedy, Romance',\n",
       " 'Comedy, Drama',\n",
       " 'Crime, Drama, Thriller',\n",
       " 'Drama, Fantasy, Horror',\n",
       " 'Action, Adventure, Drama',\n",
       " 'Drama, Romance',\n",
       " 'Drama, Mystery, Sci-Fi',\n",
       " 'Action, Crime, Drama',\n",
       " 'Drama, Mystery, Sci-Fi',\n",
       " 'Action, Crime, Drama',\n",
       " 'Comedy, Drama, Romance',\n",
       " 'Animation, Adventure, Comedy',\n",
       " 'Action, Drama, Fantasy',\n",
       " 'Adventure, Comedy, Crime',\n",
       " 'Drama',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Crime, Drama, Thriller',\n",
       " 'Comedy, Drama',\n",
       " 'Comedy, Drama',\n",
       " 'Comedy, Romance',\n",
       " 'Action, Adventure, Drama',\n",
       " 'Crime, Drama, Romance',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Drama, Horror, Sci-Fi',\n",
       " 'Animation, Comedy',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Adventure, Drama, Fantasy',\n",
       " 'Crime, Drama',\n",
       " 'Drama, Mystery',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Drama, Sci-Fi, Thriller',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Action, Crime, Drama',\n",
       " 'Action, Adventure, Drama',\n",
       " 'Drama, Sci-Fi, Thriller',\n",
       " 'Drama, Fantasy, Horror',\n",
       " 'Drama',\n",
       " 'Comedy',\n",
       " 'Comedy, Drama, Music',\n",
       " 'Animation, Comedy',\n",
       " 'Comedy, Crime',\n",
       " 'Drama, Mystery, Sci-Fi',\n",
       " 'Action, Adventure, Comedy',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Drama, Fantasy, Mystery',\n",
       " 'Comedy, Drama, Mystery',\n",
       " 'Crime, Drama',\n",
       " 'Drama, Horror, Mystery',\n",
       " 'Action, Crime, Drama',\n",
       " 'Comedy, Drama',\n",
       " 'Drama',\n",
       " 'Comedy, Drama, Romance',\n",
       " 'Comedy, Drama, Fantasy',\n",
       " 'Action, Adventure, Crime',\n",
       " 'Drama, Sci-Fi, Thriller',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Drama, Mystery, Thriller',\n",
       " 'Action, Crime, Drama',\n",
       " 'Drama, Thriller',\n",
       " 'Action, Adventure, Crime',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Comedy, Drama, Thriller',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Biography, Drama, History',\n",
       " 'Drama, History, Thriller',\n",
       " 'Comedy, Crime, Drama',\n",
       " 'Drama',\n",
       " 'Adventure, Comedy, Drama',\n",
       " 'Crime, Drama, Mystery',\n",
       " 'Comedy, Crime, Drama',\n",
       " 'Drama, Horror, Mystery']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Series genre\n",
    "gen=[]\n",
    "names = driver.find_elements(By.XPATH,'//span[@class=\"genre\"]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    gen.append(name.text)\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c270535f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['57 min',\n",
       " '51 min',\n",
       " '44 min',\n",
       " '60 min',\n",
       " '43 min',\n",
       " '59 min',\n",
       " '45 min',\n",
       " '41 min',\n",
       " '43 min',\n",
       " '42 min',\n",
       " '70 min',\n",
       " '22 min',\n",
       " '60 min',\n",
       " '88 min',\n",
       " '44 min',\n",
       " '44 min',\n",
       " '43 min',\n",
       " '60 min',\n",
       " '49 min',\n",
       " '42 min',\n",
       " '44 min',\n",
       " '44 min',\n",
       " '43 min',\n",
       " '41 min',\n",
       " '22 min',\n",
       " '60 min',\n",
       " '49 min',\n",
       " '54 min',\n",
       " '22 min',\n",
       " '22 min',\n",
       " '44 min',\n",
       " '49 min',\n",
       " '45 min',\n",
       " '43 min',\n",
       " '42 min',\n",
       " '60 min',\n",
       " '42 min',\n",
       " '62 min',\n",
       " '56 min',\n",
       " '22 min',\n",
       " '23 min',\n",
       " '42 min',\n",
       " '25 min',\n",
       " '51 min',\n",
       " '60 min',\n",
       " '60 min',\n",
       " '45 min',\n",
       " '46 min',\n",
       " '22 min',\n",
       " '45 min',\n",
       " '45 min',\n",
       " '53 min',\n",
       " '44 min',\n",
       " '22 min',\n",
       " '43 min',\n",
       " '44 min',\n",
       " '60 min',\n",
       " '44 min',\n",
       " '42 min',\n",
       " '44 min',\n",
       " '55 min',\n",
       " '42 min',\n",
       " '42 min',\n",
       " '60 min',\n",
       " '60 min',\n",
       " '41 min',\n",
       " '60 min',\n",
       " '44 min',\n",
       " '22 min',\n",
       " '22 min',\n",
       " '43 min',\n",
       " '60 min',\n",
       " '55 min',\n",
       " '60 min',\n",
       " '45 min',\n",
       " '46 min',\n",
       " '45 min',\n",
       " '53 min',\n",
       " '30 min',\n",
       " '42 min',\n",
       " '45 min',\n",
       " '22 min',\n",
       " '55 min',\n",
       " '45 min',\n",
       " '60 min',\n",
       " '44 min',\n",
       " '55 min',\n",
       " '43 min',\n",
       " '50 min',\n",
       " '60 min',\n",
       " '45 min',\n",
       " '43 min',\n",
       " '58 min',\n",
       " '330 min',\n",
       " '42 min',\n",
       " '42 min',\n",
       " '50 min',\n",
       " '42 min',\n",
       " '45 min',\n",
       " '572 min']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Series runtime\n",
    "watchtime=[]\n",
    "names = driver.find_elements(By.XPATH,'//span[@class=\"runtime\"]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    watchtime.append(name.text)\n",
    "watchtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ddfeec44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 2,088,924',\n",
       " ' 1,178,572',\n",
       " ' 988,183',\n",
       " ' 292,726',\n",
       " ' 251,866',\n",
       " ' 302,810',\n",
       " ' 145,458',\n",
       " ' 308,683',\n",
       " ' 346,249',\n",
       " ' 431,892',\n",
       " ' 475,773',\n",
       " ' 806,770',\n",
       " ' 545,252',\n",
       " ' 923,879',\n",
       " ' 531,414',\n",
       " ' 167,979',\n",
       " ' 322,132',\n",
       " ' 319,457',\n",
       " ' 1,871,188',\n",
       " ' 324,197',\n",
       " ' 445,402',\n",
       " ' 535,083',\n",
       " ' 152,203',\n",
       " ' 148,854',\n",
       " ' 407,340',\n",
       " ' 225,360',\n",
       " ' 421,737',\n",
       " ' 440,680',\n",
       " ' 992,154',\n",
       " ' 682,835',\n",
       " ' 410,661',\n",
       " ' 387,531',\n",
       " ' 136,530',\n",
       " ' 124,399',\n",
       " ' 175,374',\n",
       " ' 154,957',\n",
       " ' 230,892',\n",
       " ' 504,756',\n",
       " ' 215,122',\n",
       " ' 430,364',\n",
       " ' 518,889',\n",
       " ' 64,074',\n",
       " ' 190,662',\n",
       " ' 504,499',\n",
       " ' 384,170',\n",
       " ' 79,487',\n",
       " ' 280,246',\n",
       " ' 243,264',\n",
       " ' 224,941',\n",
       " ' 217,622',\n",
       " ' 239,993',\n",
       " ' 725,220',\n",
       " ' 130,339',\n",
       " ' 339,697',\n",
       " ' 249,602',\n",
       " ' 553,256',\n",
       " ' 545,779',\n",
       " ' 464,646',\n",
       " ' 61,267',\n",
       " ' 111,521',\n",
       " ' 341,920',\n",
       " ' 74,385',\n",
       " ' 105,221',\n",
       " ' 236,710',\n",
       " ' 95,676',\n",
       " ' 95,407',\n",
       " ' 51,093',\n",
       " ' 148,957',\n",
       " ' 369,459',\n",
       " ' 315,873',\n",
       " ' 107,016',\n",
       " ' 248,108',\n",
       " ' 570,052',\n",
       " ' 105,382',\n",
       " ' 129,577',\n",
       " ' 524,532',\n",
       " ' 108,684',\n",
       " ' 236,393',\n",
       " ' 89,370',\n",
       " ' 22,857',\n",
       " ' 144,319',\n",
       " ' 160,518',\n",
       " ' 131,681',\n",
       " ' 37,572',\n",
       " ' 288,462',\n",
       " ' 120,460',\n",
       " ' 131,784',\n",
       " ' 74,301',\n",
       " ' 108,448',\n",
       " ' 199,432',\n",
       " ' 29,093',\n",
       " ' 185,655',\n",
       " ' 216,070',\n",
       " ' 748,010',\n",
       " ' 68,649',\n",
       " ' 50,161',\n",
       " ' 61,407',\n",
       " ' 198,149',\n",
       " ' 41,563',\n",
       " ' 245,055']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Series Upvote\n",
    "vote=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"lister-item mode-detail\"]/div/p[4]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    vote.append(name.text.split(':')[1])\n",
    "vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d744f634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9.2',\n",
       " '8.7',\n",
       " '8.1',\n",
       " '7.5',\n",
       " '7.6',\n",
       " '8.1',\n",
       " '6.6',\n",
       " '7.6',\n",
       " '7.6',\n",
       " '7.5',\n",
       " '8.2',\n",
       " '8.2',\n",
       " '8.8',\n",
       " '9.1',\n",
       " '8.5',\n",
       " '7.4',\n",
       " '7.7',\n",
       " '8',\n",
       " '9.5',\n",
       " '8.1',\n",
       " '8.4',\n",
       " '8.3',\n",
       " '8.1',\n",
       " '7.7',\n",
       " '8.7',\n",
       " '7.7',\n",
       " '8.8',\n",
       " '8.6',\n",
       " '8.9',\n",
       " '8.3',\n",
       " '8.5',\n",
       " '8.6',\n",
       " '8.3',\n",
       " '6.2',\n",
       " '7.5',\n",
       " '8.2',\n",
       " '7.8',\n",
       " '8.5',\n",
       " '7.9',\n",
       " '8.5',\n",
       " '9.1',\n",
       " '6.5',\n",
       " '8.1',\n",
       " '8.7',\n",
       " '8.7',\n",
       " '7.3',\n",
       " '8.4',\n",
       " '8.6',\n",
       " '7.8',\n",
       " '7.5',\n",
       " '7.7',\n",
       " '8.7',\n",
       " '6.8',\n",
       " '8.2',\n",
       " '8',\n",
       " '8.3',\n",
       " '8.8',\n",
       " '8.7',\n",
       " '6.7',\n",
       " '8.3',\n",
       " '8.3',\n",
       " '7.3',\n",
       " '6.8',\n",
       " '8.4',\n",
       " '7.4',\n",
       " '8.1',\n",
       " '7.9',\n",
       " '6.8',\n",
       " '8.7',\n",
       " '8.4',\n",
       " '6.5',\n",
       " '7.9',\n",
       " '8.9',\n",
       " '7.8',\n",
       " '7.6',\n",
       " '8.9',\n",
       " '8.1',\n",
       " '8.5',\n",
       " '8.3',\n",
       " '7.2',\n",
       " '8.7',\n",
       " '8.2',\n",
       " '6.4',\n",
       " '6.3',\n",
       " '8.6',\n",
       " '7.9',\n",
       " '7.3',\n",
       " '7.7',\n",
       " '7.2',\n",
       " '8.5',\n",
       " '6.5',\n",
       " '8.1',\n",
       " '8.7',\n",
       " '9.4',\n",
       " '7.8',\n",
       " '7.4',\n",
       " '7.8',\n",
       " '8.1',\n",
       " '7.1',\n",
       " '8.6']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapping Series Ratings\n",
    "rating=[]\n",
    "names = driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/div/div/span[2]')\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    rating.append(name.text)\n",
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c94c3cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,088,924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016– )</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,178,572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>988,183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>292,726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>251,866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>50,161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>61,407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>198,149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7.1</td>\n",
       "      <td>41,563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>245,055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name    Year Span                     Genre  \\\n",
       "0                  Game of Thrones  (2011–2019)  Action, Adventure, Drama   \n",
       "1                  Stranger Things     (2016– )    Drama, Fantasy, Horror   \n",
       "2                 The Walking Dead  (2010–2022)   Drama, Horror, Thriller   \n",
       "3                   13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   \n",
       "4                          The 100  (2014–2020)    Drama, Mystery, Sci-Fi   \n",
       "..                             ...          ...                       ...   \n",
       "95                           Reign  (2013–2017)                     Drama   \n",
       "96  A Series of Unfortunate Events  (2017–2019)  Adventure, Comedy, Drama   \n",
       "97                  Criminal Minds     (2005– )     Crime, Drama, Mystery   \n",
       "98           Scream: The TV Series  (2015–2019)      Comedy, Crime, Drama   \n",
       "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
       "\n",
       "   Run Time Ratings       Votes  \n",
       "0    57 min     9.2   2,088,924  \n",
       "1    51 min     8.7   1,178,572  \n",
       "2    44 min     8.1     988,183  \n",
       "3    60 min     7.5     292,726  \n",
       "4    43 min     7.6     251,866  \n",
       "..      ...     ...         ...  \n",
       "95   42 min     7.4      50,161  \n",
       "96   50 min     7.8      61,407  \n",
       "97   42 min     8.1     198,149  \n",
       "98   45 min     7.1      41,563  \n",
       "99  572 min     8.6     245,055  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe \n",
    "\n",
    "DF=pd.DataFrame({'Name':ser,'Year Span':year,'Genre':gen,'Run Time':watchtime,'Ratings':rating,'Votes':vote})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c283a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd55a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e5b1a86",
   "metadata": {},
   "source": [
    "# 10. Details of Datasets from UCI machine learning repositories. Url = https://archive.ics.uci.edu/\n",
    "* You have to find the following details:\n",
    "    *  A) Dataset name\n",
    "    *  B) Data type\n",
    "    *  C) Task\n",
    "    *  D) Attribute type\n",
    "    *  E) No of instances\n",
    "    *  F) No of attribute\n",
    "    *  G) Year\n",
    "* Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "77addd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import nec library\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import re\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "59f8fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect webdriver\n",
    "driver=webdriver.Chrome('chromedriver')\n",
    "\n",
    "#maximize the automated window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "ccdd6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the link\n",
    "url='https://archive.ics.uci.edu/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "7ffab54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#URLs of the Datasets\n",
    "product_URL = []\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//td[@valign=\"middle\"]//table//tbody//tr/td//span//a')\n",
    "    for i in url:\n",
    "        product_URL.append(i.get_attribute('href'))\n",
    "len(product_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1db380c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://archive.ics.uci.edu/ml/datasets/Average+Localization+Error+%28ALE%29+in+sensor+node+localization+process+in+WSNs',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/9mers+from+cullpdb',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/TamilSentiMix',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Accelerometer',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Synchronous+Machine+Data+Set',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Synchronous+Machine+Data+Set',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Pedal+Me+Bicycle+Deliveries',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Wikipedia+Math+Essentials',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Wikipedia+Math+Essentials',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Turkish+Headlines+Dataset',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Secondary+Mushroom+Dataset',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Power+consumption+of+Tetouan+city',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Iris',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Adult',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Wine',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Heart+Disease',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Wine+Quality',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Rice+%28Cammeo+and+Osmancik%29',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Bank+Marketing',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Car+Evaluation',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Raisin+Dataset',\n",
       " 'https://archive.ics.uci.edu/ml/datasets/Abalone']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b0df369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping Dataset Names\n",
    "dataname=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/table[2]/tbody/tr/td/table[1]/tbody/tr/td[1]/p[1]/span[1]/b')\n",
    "        dataname.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        dataname.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "02bb7414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Average Localization Error (ALE) in sensor node localization process in WSNs Data Set',\n",
       " '9mers from cullpdb Data Set',\n",
       " 'TamilSentiMix Data Set',\n",
       " 'Accelerometer Data Set',\n",
       " 'Synchronous Machine Data Set Data Set',\n",
       " 'Synchronous Machine Data Set Data Set',\n",
       " 'Pedal Me Bicycle Deliveries Data Set',\n",
       " 'Wikipedia Math Essentials Data Set',\n",
       " 'Wikipedia Math Essentials Data Set',\n",
       " 'Turkish Headlines Dataset Data Set',\n",
       " 'Secondary Mushroom Dataset Data Set',\n",
       " 'Power consumption of Tetouan city Data Set',\n",
       " 'Iris Data Set',\n",
       " 'Adult Data Set',\n",
       " 'Dry Bean Dataset Data Set',\n",
       " 'Wine Data Set',\n",
       " 'Heart Disease Data Set',\n",
       " 'Wine Quality Data Set',\n",
       " 'Rice (Cammeo and Osmancik) Data Set',\n",
       " 'Bank Marketing Data Set',\n",
       " 'Breast Cancer Wisconsin (Diagnostic) Data Set',\n",
       " 'Car Evaluation Data Set',\n",
       " 'Raisin Dataset Data Set',\n",
       " 'Abalone Data Set']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a8282866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping Dataset datatypes\n",
    "datatype=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/table[2]/tbody/tr/td/table[2]/tbody/tr[1]/td[2]/p')\n",
    "        datatype.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        datatype.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "36e45e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Multivariate',\n",
       " 'Sequential',\n",
       " 'N/A',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Time-Series',\n",
       " 'Time-Series',\n",
       " 'Time-Series',\n",
       " 'Text',\n",
       " 'Univariate',\n",
       " 'Multivariate, Time-Series',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Multivariate',\n",
       " 'Multivariate']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "2dac1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scrapping Dataset task\n",
    "task=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/table[2]/tbody/tr/td/table[2]/tbody/tr[3]/td[2]/p')\n",
    "        task.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        task.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "855119ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regression',\n",
       " 'Classification, Regression',\n",
       " 'Classification',\n",
       " 'Classification, Regression',\n",
       " 'Regression',\n",
       " 'Regression',\n",
       " 'Regression',\n",
       " 'Regression',\n",
       " 'Regression',\n",
       " 'Classification, Clustering',\n",
       " 'Classification',\n",
       " 'Regression',\n",
       " 'Classification',\n",
       " 'Classification',\n",
       " 'Classification',\n",
       " 'Classification',\n",
       " 'Classification',\n",
       " 'Classification, Regression',\n",
       " 'Classification',\n",
       " 'Classification',\n",
       " 'Classification',\n",
       " 'Classification',\n",
       " 'Classification',\n",
       " 'Classification']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "02384935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scrapping Dataset Attribute type\n",
    "att=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/table[2]/tbody/tr/td/table[2]/tbody/tr[2]/td[2]/p')\n",
    "        att.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        att.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "535221c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Real',\n",
       " 'Real',\n",
       " 'N/A',\n",
       " 'Integer, Real',\n",
       " 'Real',\n",
       " 'Real',\n",
       " 'Real',\n",
       " 'Real',\n",
       " 'Real',\n",
       " 'N/A',\n",
       " 'Real',\n",
       " 'Integer, Real',\n",
       " 'Real',\n",
       " 'Categorical, Integer',\n",
       " 'Integer, Real',\n",
       " 'Integer, Real',\n",
       " 'Categorical, Integer, Real',\n",
       " 'Real',\n",
       " 'Real',\n",
       " 'Real',\n",
       " 'Real',\n",
       " 'Categorical',\n",
       " 'Integer, Real',\n",
       " 'Categorical, Integer, Real']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "5165bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping Dataset No of instances\n",
    "ins=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/table[2]/tbody/tr/td/table[2]/tbody/tr[1]/td[4]/p')\n",
    "        ins.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        ins.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5e0cad45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['107',\n",
       " '158716',\n",
       " '15744',\n",
       " '153000',\n",
       " '557',\n",
       " '557',\n",
       " '36',\n",
       " '731',\n",
       " '731',\n",
       " '4200',\n",
       " '61069',\n",
       " '52417',\n",
       " '150',\n",
       " '48842',\n",
       " '13611',\n",
       " '178',\n",
       " '303',\n",
       " '4898',\n",
       " '3810',\n",
       " '45211',\n",
       " '569',\n",
       " '1728',\n",
       " '900',\n",
       " '4177']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "4682ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping Dataset No of Attribute\n",
    "attno=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/table[2]/tbody/tr/td/table[2]/tbody/tr[2]/td[4]/p')\n",
    "        attno.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        attno.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d1c8bdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6',\n",
       " '4',\n",
       " 'N/A',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '15',\n",
       " '1068',\n",
       " '1068',\n",
       " '7',\n",
       " '21',\n",
       " '9',\n",
       " '4',\n",
       " '14',\n",
       " '17',\n",
       " '13',\n",
       " '75',\n",
       " '12',\n",
       " '8',\n",
       " '17',\n",
       " '32',\n",
       " '6',\n",
       " '8',\n",
       " '8']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "497130e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping Dataset date\n",
    "iyear=[]\n",
    "for i in product_URL:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'/html/body/table[2]/tbody/tr/td/table[2]/tbody/tr[2]/td[6]/p')\n",
    "        iyear.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        iyear.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "9f6ca1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-06-05',\n",
       " '2021-05-25',\n",
       " '2021-05-18',\n",
       " '2021-05-02',\n",
       " '2021-04-21',\n",
       " '2021-04-21',\n",
       " '2021-04-20',\n",
       " '2021-04-20',\n",
       " '2021-04-20',\n",
       " '2021-04-14',\n",
       " '2021-04-11',\n",
       " '2021-04-03',\n",
       " '1988-07-01',\n",
       " '1996-05-01',\n",
       " '2020-09-14',\n",
       " '1991-07-01',\n",
       " '1988-07-01',\n",
       " '2009-10-07',\n",
       " '2019-10-06',\n",
       " '2012-02-14',\n",
       " '1995-11-01',\n",
       " '1997-06-01',\n",
       " '2021-04-01',\n",
       " '1995-12-01']"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fa25af59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>No of Instance</th>\n",
       "      <th>No of Attribute</th>\n",
       "      <th>Year</th>\n",
       "      <th>Dataset URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average Localization Error (ALE) in sensor nod...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>107</td>\n",
       "      <td>6</td>\n",
       "      <td>2021-06-05</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Averag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9mers from cullpdb Data Set</td>\n",
       "      <td>Sequential</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>158716</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/9mers+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TamilSentiMix Data Set</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Classification</td>\n",
       "      <td>N/A</td>\n",
       "      <td>15744</td>\n",
       "      <td>N/A</td>\n",
       "      <td>2021-05-18</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/TamilS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accelerometer Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>153000</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-05-02</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Accele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Synchronous Machine Data Set Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>557</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-04-21</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Synchr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Synchronous Machine Data Set Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>557</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-04-21</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Synchr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pedal Me Bicycle Deliveries Data Set</td>\n",
       "      <td>Time-Series</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>2021-04-20</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Pedal+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wikipedia Math Essentials Data Set</td>\n",
       "      <td>Time-Series</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>731</td>\n",
       "      <td>1068</td>\n",
       "      <td>2021-04-20</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Wikipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia Math Essentials Data Set</td>\n",
       "      <td>Time-Series</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>731</td>\n",
       "      <td>1068</td>\n",
       "      <td>2021-04-20</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Wikipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Turkish Headlines Dataset Data Set</td>\n",
       "      <td>Text</td>\n",
       "      <td>Classification, Clustering</td>\n",
       "      <td>N/A</td>\n",
       "      <td>4200</td>\n",
       "      <td>7</td>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Turkis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Secondary Mushroom Dataset Data Set</td>\n",
       "      <td>Univariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>61069</td>\n",
       "      <td>21</td>\n",
       "      <td>2021-04-11</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Power consumption of Tetouan city Data Set</td>\n",
       "      <td>Multivariate, Time-Series</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>52417</td>\n",
       "      <td>9</td>\n",
       "      <td>2021-04-03</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Power+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Iris Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>1988-07-01</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Adult Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48842</td>\n",
       "      <td>14</td>\n",
       "      <td>1996-05-01</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dry Bean Dataset Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>13611</td>\n",
       "      <td>17</td>\n",
       "      <td>2020-09-14</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Dry+Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wine Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>178</td>\n",
       "      <td>13</td>\n",
       "      <td>1991-07-01</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Wine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Heart Disease Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>303</td>\n",
       "      <td>75</td>\n",
       "      <td>1988-07-01</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Heart+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Wine Quality Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>4898</td>\n",
       "      <td>12</td>\n",
       "      <td>2009-10-07</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Wine+Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Rice (Cammeo and Osmancik) Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>3810</td>\n",
       "      <td>8</td>\n",
       "      <td>2019-10-06</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Rice+%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bank Marketing Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>45211</td>\n",
       "      <td>17</td>\n",
       "      <td>2012-02-14</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Bank+M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Breast Cancer Wisconsin (Diagnostic) Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>569</td>\n",
       "      <td>32</td>\n",
       "      <td>1995-11-01</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Breast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Car Evaluation Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>1728</td>\n",
       "      <td>6</td>\n",
       "      <td>1997-06-01</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Car+Ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Raisin Dataset Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>900</td>\n",
       "      <td>8</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Raisin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Abalone Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>4177</td>\n",
       "      <td>8</td>\n",
       "      <td>1995-12-01</td>\n",
       "      <td>https://archive.ics.uci.edu/ml/datasets/Abalone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Dataset Name  \\\n",
       "0   Average Localization Error (ALE) in sensor nod...   \n",
       "1                         9mers from cullpdb Data Set   \n",
       "2                              TamilSentiMix Data Set   \n",
       "3                              Accelerometer Data Set   \n",
       "4               Synchronous Machine Data Set Data Set   \n",
       "5               Synchronous Machine Data Set Data Set   \n",
       "6                Pedal Me Bicycle Deliveries Data Set   \n",
       "7                  Wikipedia Math Essentials Data Set   \n",
       "8                  Wikipedia Math Essentials Data Set   \n",
       "9                  Turkish Headlines Dataset Data Set   \n",
       "10                Secondary Mushroom Dataset Data Set   \n",
       "11         Power consumption of Tetouan city Data Set   \n",
       "12                                      Iris Data Set   \n",
       "13                                     Adult Data Set   \n",
       "14                          Dry Bean Dataset Data Set   \n",
       "15                                      Wine Data Set   \n",
       "16                             Heart Disease Data Set   \n",
       "17                              Wine Quality Data Set   \n",
       "18                Rice (Cammeo and Osmancik) Data Set   \n",
       "19                            Bank Marketing Data Set   \n",
       "20      Breast Cancer Wisconsin (Diagnostic) Data Set   \n",
       "21                            Car Evaluation Data Set   \n",
       "22                            Raisin Dataset Data Set   \n",
       "23                                   Abalone Data Set   \n",
       "\n",
       "                    Data type                        Task  \\\n",
       "0                Multivariate                  Regression   \n",
       "1                  Sequential  Classification, Regression   \n",
       "2                         N/A              Classification   \n",
       "3                Multivariate  Classification, Regression   \n",
       "4                Multivariate                  Regression   \n",
       "5                Multivariate                  Regression   \n",
       "6                 Time-Series                  Regression   \n",
       "7                 Time-Series                  Regression   \n",
       "8                 Time-Series                  Regression   \n",
       "9                        Text  Classification, Clustering   \n",
       "10                 Univariate              Classification   \n",
       "11  Multivariate, Time-Series                  Regression   \n",
       "12               Multivariate              Classification   \n",
       "13               Multivariate              Classification   \n",
       "14               Multivariate              Classification   \n",
       "15               Multivariate              Classification   \n",
       "16               Multivariate              Classification   \n",
       "17               Multivariate  Classification, Regression   \n",
       "18               Multivariate              Classification   \n",
       "19               Multivariate              Classification   \n",
       "20               Multivariate              Classification   \n",
       "21               Multivariate              Classification   \n",
       "22               Multivariate              Classification   \n",
       "23               Multivariate              Classification   \n",
       "\n",
       "                Attribute Type No of Instance No of Attribute        Year  \\\n",
       "0                         Real            107               6  2021-06-05   \n",
       "1                         Real         158716               4  2021-05-25   \n",
       "2                          N/A          15744             N/A  2021-05-18   \n",
       "3                Integer, Real         153000               5  2021-05-02   \n",
       "4                         Real            557               5  2021-04-21   \n",
       "5                         Real            557               5  2021-04-21   \n",
       "6                         Real             36              15  2021-04-20   \n",
       "7                         Real            731            1068  2021-04-20   \n",
       "8                         Real            731            1068  2021-04-20   \n",
       "9                          N/A           4200               7  2021-04-14   \n",
       "10                        Real          61069              21  2021-04-11   \n",
       "11               Integer, Real          52417               9  2021-04-03   \n",
       "12                        Real            150               4  1988-07-01   \n",
       "13        Categorical, Integer          48842              14  1996-05-01   \n",
       "14               Integer, Real          13611              17  2020-09-14   \n",
       "15               Integer, Real            178              13  1991-07-01   \n",
       "16  Categorical, Integer, Real            303              75  1988-07-01   \n",
       "17                        Real           4898              12  2009-10-07   \n",
       "18                        Real           3810               8  2019-10-06   \n",
       "19                        Real          45211              17  2012-02-14   \n",
       "20                        Real            569              32  1995-11-01   \n",
       "21                 Categorical           1728               6  1997-06-01   \n",
       "22               Integer, Real            900               8  2021-04-01   \n",
       "23  Categorical, Integer, Real           4177               8  1995-12-01   \n",
       "\n",
       "                                          Dataset URL  \n",
       "0   https://archive.ics.uci.edu/ml/datasets/Averag...  \n",
       "1   https://archive.ics.uci.edu/ml/datasets/9mers+...  \n",
       "2   https://archive.ics.uci.edu/ml/datasets/TamilS...  \n",
       "3   https://archive.ics.uci.edu/ml/datasets/Accele...  \n",
       "4   https://archive.ics.uci.edu/ml/datasets/Synchr...  \n",
       "5   https://archive.ics.uci.edu/ml/datasets/Synchr...  \n",
       "6   https://archive.ics.uci.edu/ml/datasets/Pedal+...  \n",
       "7   https://archive.ics.uci.edu/ml/datasets/Wikipe...  \n",
       "8   https://archive.ics.uci.edu/ml/datasets/Wikipe...  \n",
       "9   https://archive.ics.uci.edu/ml/datasets/Turkis...  \n",
       "10  https://archive.ics.uci.edu/ml/datasets/Second...  \n",
       "11  https://archive.ics.uci.edu/ml/datasets/Power+...  \n",
       "12       https://archive.ics.uci.edu/ml/datasets/Iris  \n",
       "13      https://archive.ics.uci.edu/ml/datasets/Adult  \n",
       "14  https://archive.ics.uci.edu/ml/datasets/Dry+Be...  \n",
       "15       https://archive.ics.uci.edu/ml/datasets/Wine  \n",
       "16  https://archive.ics.uci.edu/ml/datasets/Heart+...  \n",
       "17  https://archive.ics.uci.edu/ml/datasets/Wine+Q...  \n",
       "18  https://archive.ics.uci.edu/ml/datasets/Rice+%...  \n",
       "19  https://archive.ics.uci.edu/ml/datasets/Bank+M...  \n",
       "20  https://archive.ics.uci.edu/ml/datasets/Breast...  \n",
       "21  https://archive.ics.uci.edu/ml/datasets/Car+Ev...  \n",
       "22  https://archive.ics.uci.edu/ml/datasets/Raisin...  \n",
       "23    https://archive.ics.uci.edu/ml/datasets/Abalone  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe \n",
    "\n",
    "DF=pd.DataFrame({'Dataset Name':dataname,'Data type':datatype,'Task':task,'Attribute Type':att,'No of Instance':ins,'No of Attribute':attno,'Year':iyear,'Dataset URL':product_URL})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d0770232",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86728ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be25ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48497976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
